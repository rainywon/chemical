# å¯¼å…¥å¿…è¦çš„åº“å’Œæ¨¡å—
import json
import logging  # æ—¥å¿—è®°å½•æ¨¡å—
from pathlib import Path  # è·¯å¾„å¤„ç†åº“
from typing import Generator, Optional, List, Tuple, Dict, Any  # ç±»å‹æç¤ºæ”¯æŒ
import warnings  # è­¦å‘Šå¤„ç†
import torch  # PyTorchæ·±åº¦å­¦ä¹ æ¡†æ¶
from langchain_community.vectorstores import FAISS  # FAISSå‘é‡æ•°æ®åº“é›†æˆ
from langchain_core.documents import Document  # æ–‡æ¡£å¯¹è±¡å®šä¹‰
from langchain_core.embeddings import Embeddings  # åµŒå…¥æ¨¡å‹æ¥å£
from langchain_ollama import OllamaLLM  # Ollamaè¯­è¨€æ¨¡å‹é›†æˆ
from rank_bm25 import BM25Okapi  # BM25æ£€ç´¢ç®—æ³•
from transformers import AutoModelForSequenceClassification, AutoTokenizer  # Transformeræ¨¡å‹
from config import Config  # è‡ªå®šä¹‰é…ç½®æ–‡ä»¶
from build_vector_store import VectorDBBuilder  # å‘é‡æ•°æ®åº“æ„å»ºå™¨
import numpy as np  # æ•°å€¼è®¡ç®—åº“
import pickle  # ç”¨äºåºåˆ—åŒ–å¯¹è±¡
import hashlib  # ç”¨äºç”Ÿæˆå“ˆå¸Œå€¼
import re  # ç”¨äºæ­£åˆ™è¡¨è¾¾å¼

# æå‰åˆå§‹åŒ–jiebaï¼ŒåŠ å¿«åç»­å¯åŠ¨é€Ÿåº¦
import os
import jieba  # ä¸­æ–‡åˆ†è¯åº“

# è®¾ç½®jiebaæ—¥å¿—çº§åˆ«ï¼Œå‡å°‘è¾“å‡º
jieba.setLogLevel(logging.INFO)

# é¢„åŠ è½½jiebaåˆ†è¯å™¨
jieba.initialize()

# ç¦ç”¨ä¸å¿…è¦çš„è­¦å‘Š
warnings.filterwarnings("ignore", category=UserWarning)

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)  # è·å–å½“å‰æ¨¡å—çš„æ—¥å¿—è®°å½•å™¨


class RAGSystem:
    """RAGé—®ç­”ç³»ç»Ÿï¼Œæ”¯æŒæ–‡æ¡£æ£€ç´¢å’Œç”Ÿæˆå¼é—®ç­”

    ç‰¹æ€§ï¼š
    - è‡ªåŠ¨ç®¡ç†å‘é‡æ•°æ®åº“ç”Ÿå‘½å‘¨æœŸ
    - æ”¯æŒæµå¼ç”Ÿæˆå’ŒåŒæ­¥ç”Ÿæˆ
    - å¯é…ç½®çš„æ£€ç´¢ç­–ç•¥
    - å®Œå–„çš„é”™è¯¯å¤„ç†
    """

    def __init__(self, config: Config):
        """åˆå§‹åŒ–RAGç³»ç»Ÿ

        :param config: åŒ…å«æ‰€æœ‰é…ç½®å‚æ•°çš„Configå¯¹è±¡
        """
        self.config = config  # ä¿å­˜é…ç½®å¯¹è±¡
        self.vector_store: Optional[FAISS] = None  # FAISSå‘é‡æ•°æ®åº“å®ä¾‹
        self.llm: Optional[OllamaLLM] = None  # Ollamaè¯­è¨€æ¨¡å‹å®ä¾‹
        self.embeddings: Optional[Embeddings] = None  # åµŒå…¥æ¨¡å‹å®ä¾‹
        self.rerank_model = None  # é‡æ’åºæ¨¡å‹
        self.vector_db_build = VectorDBBuilder(config)  # å‘é‡æ•°æ®åº“æ„å»ºå™¨å®ä¾‹
        self._tokenize_cache = {}  # æ·»åŠ åˆ†è¯ç¼“å­˜å­—å…¸

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self._init_logging()  # åˆå§‹åŒ–æ—¥å¿—é…ç½®
        self._init_embeddings()  # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        self._init_vector_store()  # åˆå§‹åŒ–å‘é‡æ•°æ®åº“
        self._init_bm25_retriever()  # åˆå§‹åŒ–BM25æ£€ç´¢å™¨
        self._init_llm()  # åˆå§‹åŒ–å¤§è¯­è¨€æ¨¡å‹
        self._init_rerank_model()  # åˆå§‹åŒ–é‡æ’åºæ¨¡å‹

    def _tokenize(self, text: str) -> List[str]:
        """ä¸“ä¸šä¸­æ–‡åˆ†è¯å¤„ç†ï¼Œä½¿ç”¨ç¼“å­˜æé«˜æ€§èƒ½
        :param text: å¾…åˆ†è¯çš„æ–‡æœ¬
        :return: åˆ†è¯åçš„è¯é¡¹åˆ—è¡¨
        """
        # æ£€æŸ¥ç¼“å­˜ä¸­æ˜¯å¦å·²æœ‰ç»“æœ
        if text in self._tokenize_cache:
            return self._tokenize_cache[text]
        
        # å¦‚æœæ–‡æœ¬è¿‡é•¿ï¼Œåªç¼“å­˜å‰2000ä¸ªå­—ç¬¦çš„åˆ†è¯ç»“æœ
        cache_key = text[:2000] if len(text) > 2000 else text
        
        # åˆ†è¯å¤„ç†
        result = [word for word in jieba.cut(text) if word.strip()]
        
        # åªåœ¨ç¼“å­˜ä¸è¶…è¿‡10000ä¸ªæ¡ç›®æ—¶è¿›è¡Œç¼“å­˜
        if len(self._tokenize_cache) < 10000:
            self._tokenize_cache[cache_key] = result
            
        return result

    def _init_logging(self):
        """åˆå§‹åŒ–æ—¥å¿—é…ç½®"""
        logging.basicConfig(
            level=logging.INFO,  # æ—¥å¿—çº§åˆ«è®¾ä¸ºINFO
            format="%(asctime)s - %(levelname)s - %(message)s",  # æ—¥å¿—æ ¼å¼
            handlers=[logging.StreamHandler()]  # è¾“å‡ºåˆ°æ§åˆ¶å°
        )

    def _init_embeddings(self):
        """åˆå§‹åŒ–åµŒå…¥æ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
            # é€šè¿‡æ„å»ºå™¨åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹
            self.embeddings = self.vector_db_build.create_embeddings()
            logger.info("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error("âŒ åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–åµŒå…¥æ¨¡å‹: {str(e)}")

    def _init_vector_store(self):
        """åˆå§‹åŒ–å‘é‡æ•°æ®åº“"""
        try:
            vector_path = Path(self.config.vector_db_path)  # è·å–å‘é‡åº“è·¯å¾„

            # æ£€æŸ¥ç°æœ‰å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            if vector_path.exists():
                logger.info("ğŸ” æ­£åœ¨åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
                if not self.embeddings:
                    raise ValueError("åµŒå…¥æ¨¡å‹æœªåˆå§‹åŒ–")

                # åŠ è½½æœ¬åœ°FAISSæ•°æ®åº“
                self.vector_store = FAISS.load_local(
                    folder_path=str(vector_path),
                    embeddings=self.embeddings,
                    allow_dangerous_deserialization=True  # å…è®¸åŠ è½½æ—§ç‰ˆæœ¬åºåˆ—åŒ–æ•°æ®
                )
                logger.info(f"âœ… å·²åŠ è½½å‘é‡æ•°æ®åº“ï¼š{vector_path}")
            else:
                # æ„å»ºæ–°å‘é‡æ•°æ®åº“
                logger.warning("âš ï¸ æœªæ‰¾åˆ°ç°æœ‰å‘é‡æ•°æ®åº“ï¼Œæ­£åœ¨æ„å»ºæ–°æ•°æ®åº“...")
                self.vector_store = self.vector_db_build.build_vector_store()
                logger.info(f"âœ… æ–°å»ºå‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ï¼š{vector_path}")
        except Exception as e:
            logger.error("âŒ å‘é‡æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–å‘é‡æ•°æ®åº“: {str(e)}")

    def _init_rerank_model(self):
        """åˆå§‹åŒ–é‡æ’åºæ¨¡å‹"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–rerankæ¨¡å‹...")
            # ä»HuggingFaceåŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œåˆ†è¯å™¨
            self.rerank_model = AutoModelForSequenceClassification.from_pretrained(
                self.config.rerank_model_path
            )
            self.rerank_tokenizer = AutoTokenizer.from_pretrained(self.config.rerank_model_path)
            logger.info("âœ… rerankæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ rerankæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–rerankæ¨¡å‹: {str(e)}")

    def _init_llm(self):
        """åˆå§‹åŒ–Ollamaå¤§è¯­è¨€æ¨¡å‹"""
        try:
            logger.info("ğŸš€ æ­£åœ¨åˆå§‹åŒ–Ollamaæ¨¡å‹...")
            # åˆ›å»ºOllamaLLMå®ä¾‹
            self.llm = OllamaLLM(
                model="deepseek_8B:latest",  # æ¨¡å‹åç§°
                #deepseek_8B:latest   1513b8b198dc    8.5 GB    59 seconds ago
                # deepseek-r1:8b             2deepseek_8B:latest GB    46 minutes ago
                # deepseek-r1:14b            ea35dfe18182    9.0 GB    29 hours ago
                base_url=self.config.ollama_base_url,  # OllamaæœåŠ¡åœ°å€
                temperature=self.config.llm_temperature,  # æ¸©åº¦å‚æ•°æ§åˆ¶éšæœºæ€§
                num_predict=self.config.llm_max_tokens,  # æœ€å¤§ç”Ÿæˆtokenæ•°
                stop=["<|im_end|>"]
            )

            # æµ‹è¯•æ¨¡å‹è¿æ¥
            logger.info("âœ… Ollamaæ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ Ollamaæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"æ— æ³•åˆå§‹åŒ–Ollamaæ¨¡å‹: {str(e)}")

    def _init_bm25_retriever(self):
        """åˆå§‹åŒ–BM25æ£€ç´¢å™¨ï¼ˆæŒä¹…åŒ–ç¼“å­˜ç‰ˆï¼‰"""
        try:
            logger.info("ğŸ”§ æ­£åœ¨åˆå§‹åŒ–BM25æ£€ç´¢å™¨...")

            # éªŒè¯å‘é‡åº“æ˜¯å¦åŒ…å«æ–‡æ¡£
            if not self.vector_store.docstore._dict:
                raise ValueError("å‘é‡åº“ä¸­æ— å¯ç”¨æ–‡æ¡£")

            # ä»å‘é‡åº“åŠ è½½æ‰€æœ‰æ–‡æ¡£å†…å®¹
            all_docs = list(self.vector_store.docstore._dict.values())
            self.bm25_docs = [doc.page_content for doc in all_docs]
            self.doc_metadata = [doc.metadata for doc in all_docs]
            
            # è®¡ç®—æ–‡æ¡£é›†åˆçš„å“ˆå¸Œå€¼ï¼Œç”¨äºç¼“å­˜æ ‡è¯†
            docs_hash = hashlib.md5(str([d[:100] for d in self.bm25_docs]).encode()).hexdigest()
            cache_path = Path(self.config.vector_db_path).parent / f"bm25_tokenized_cache_{docs_hash}.pkl"
            
            # å°è¯•åŠ è½½ç¼“å­˜çš„åˆ†è¯ç»“æœ
            if cache_path.exists():
                try:
                    logger.info(f"å‘ç°BM25åˆ†è¯ç¼“å­˜ï¼Œæ­£åœ¨åŠ è½½: {cache_path}")
                    with open(cache_path, 'rb') as f:
                        cached_data = pickle.load(f)
                        tokenized_docs = cached_data.get('tokenized_docs')
                        
                    if tokenized_docs and len(tokenized_docs) == len(self.bm25_docs):
                        logger.info(f"æˆåŠŸåŠ è½½ç¼“å­˜çš„åˆ†è¯ç»“æœï¼Œå…± {len(tokenized_docs)} ç¯‡æ–‡æ¡£")
                    else:
                        logger.warning("ç¼“å­˜æ•°æ®ä¸åŒ¹é…ï¼Œå°†é‡æ–°å¤„ç†åˆ†è¯")
                        tokenized_docs = None
                except Exception as e:
                    logger.warning(f"åŠ è½½ç¼“å­˜å¤±è´¥: {str(e)}ï¼Œå°†é‡æ–°å¤„ç†åˆ†è¯")
                    tokenized_docs = None
            else:
                tokenized_docs = None
            
            # å¦‚æœæ²¡æœ‰æœ‰æ•ˆçš„ç¼“å­˜ï¼Œé‡æ–°åˆ†è¯å¤„ç†
            if tokenized_docs is None:
                logger.info(f"å¼€å§‹å¤„ç† {len(self.bm25_docs)} ç¯‡æ–‡æ¡£è¿›è¡ŒBM25ç´¢å¼•...")
                
                # æ‰¹å¤„ç†åˆ†è¯ä»¥å‡å°‘å†…å­˜å‹åŠ›
                batch_size = 100  # æ¯æ‰¹å¤„ç†çš„æ–‡æ¡£æ•°
                tokenized_docs = []
                
                for i in range(0, len(self.bm25_docs), batch_size):
                    batch = self.bm25_docs[i:i+batch_size]
                    batch_tokenized = [self._tokenize(doc) for doc in batch]
                    tokenized_docs.extend(batch_tokenized)
                    
                    if (i + batch_size) % 500 == 0 or (i + batch_size) >= len(self.bm25_docs):
                        logger.info(f"å·²å¤„ç† {min(i + batch_size, len(self.bm25_docs))}/{len(self.bm25_docs)} ç¯‡æ–‡æ¡£")
                
                # ä¿å­˜åˆ†è¯ç»“æœåˆ°ç¼“å­˜
                try:
                    logger.info(f"ä¿å­˜åˆ†è¯ç»“æœåˆ°ç¼“å­˜: {cache_path}")
                    with open(cache_path, 'wb') as f:
                        pickle.dump({'tokenized_docs': tokenized_docs}, f)
                except Exception as e:
                    logger.warning(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {str(e)}")

            # éªŒè¯åˆ†è¯ç»“æœæœ‰æ•ˆæ€§
            if len(tokenized_docs) == 0 or all(len(d) == 0 for d in tokenized_docs):
                raise ValueError("æ–‡æ¡£åˆ†è¯åä¸ºç©ºï¼Œè¯·æ£€æŸ¥åˆ†è¯é€»è¾‘")

            # åˆå§‹åŒ–BM25æ¨¡å‹
            logger.info("å¼€å§‹æ„å»ºBM25ç´¢å¼•...")
            self.bm25 = BM25Okapi(tokenized_docs)

            logger.info(f"âœ… BM25åˆå§‹åŒ–å®Œæˆï¼Œæ–‡æ¡£æ•°ï¼š{len(self.bm25_docs)}")
        except Exception as e:
            logger.error(f"âŒ BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise RuntimeError(f"BM25åˆå§‹åŒ–å¤±è´¥: {str(e)}")

    def _hybrid_retrieve(self, question: str) -> List[Dict[str, Any]]:
        """æ··åˆæ£€ç´¢æµç¨‹ï¼ˆå‘é‡+BM25ï¼‰ï¼Œæ ¹æ®åŠ¨æ€æƒé‡èåˆç»“æœ

        :param question: ç”¨æˆ·é—®é¢˜
        :return: åŒ…å«æ–‡æ¡£å’Œæ£€ç´¢ä¿¡æ¯çš„å­—å…¸åˆ—è¡¨
        """
        results = []
        
        # åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡
        vector_weight, bm25_weight = self._determine_retrieval_weights(question)
        logger.info(f"æŸ¥è¯¢æƒé‡ - å‘é‡æ£€ç´¢: {vector_weight:.2f}, BM25æ£€ç´¢: {bm25_weight:.2f}")

        # å‘é‡æ£€ç´¢éƒ¨åˆ†
        vector_results = self.vector_store.similarity_search_with_score(
            question, k=self.config.vector_top_k  # è·å–top kç»“æœ
        )
        
        # å¤„ç†å‘é‡æ£€ç´¢ç»“æœ
        filtered_vector_results = []
        for doc, score in vector_results:
            # è½¬æ¢ä¸ºæ ‡å‡†ä½™å¼¦å€¼ï¼ˆ0~1èŒƒå›´ï¼‰
            norm_score = (score + 1) / 2
            
            if norm_score >= self.config.vector_similarity_threshold:  # ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                filtered_vector_results.append({
                    "doc": doc,
                    "score": norm_score,  # åŸå§‹åˆ†æ•°
                    "weighted_score": norm_score * vector_weight,  # åº”ç”¨æƒé‡åçš„åˆ†æ•°
                    "raw_score": norm_score,
                    "type": "vector",
                    "source": doc.metadata.get("source", "unknown")
                })

        # BM25æ£€ç´¢éƒ¨åˆ†
        tokenized_query = self._tokenize(question)  # é—®é¢˜åˆ†è¯
        bm25_scores = self.bm25.get_scores(tokenized_query)  # è®¡ç®—BM25åˆ†æ•°
        
        # è·å–top kçš„ç´¢å¼•ï¼ˆå€’åºæ’åˆ—ï¼‰
        top_bm25_indices = np.argsort(bm25_scores)[-self.config.bm25_top_k:][::-1]
        
        # å¯¹BM25åˆ†æ•°è¿›è¡Œå½’ä¸€åŒ–å¤„ç†
        selected_bm25_scores = [bm25_scores[idx] for idx in top_bm25_indices]
        if selected_bm25_scores:  # ç¡®ä¿æœ‰åˆ†æ•°å¯ä»¥å½’ä¸€åŒ–
            # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
            mean_score = np.mean(selected_bm25_scores)
            std_score = np.std(selected_bm25_scores) + 1e-9  # é¿å…é™¤ä»¥0
            
            # ä½¿ç”¨Logisticå½’ä¸€åŒ–
            normalized_bm25_scores = []
            for score in selected_bm25_scores:
                # å…ˆè¿›è¡ŒZ-scoreæ ‡å‡†åŒ–
                z_score = (score - mean_score) / std_score
                # ç„¶ååº”ç”¨Sigmoidå‡½æ•°
                logistic_score = 1 / (1 + np.exp(-z_score))
                normalized_bm25_scores.append(logistic_score)
        else:
            normalized_bm25_scores = []

        # å¯¹BM25æ£€ç´¢ç»“æœè¿›è¡Œé˜ˆå€¼è¿‡æ»¤
        filtered_bm25_results = []
        for idx, norm_score in zip(top_bm25_indices, normalized_bm25_scores):
            if norm_score >= self.config.bm25_similarity_threshold:  # ä½¿ç”¨ç›¸ä¼¼åº¦é˜ˆå€¼è¿‡æ»¤
                doc = Document(
                    page_content=self.bm25_docs[idx],
                    metadata=self.doc_metadata[idx]
                )
                filtered_bm25_results.append({
                    "doc": doc,
                    "score": norm_score,  # åŸå§‹åˆ†æ•°
                    "weighted_score": norm_score * bm25_weight,  # åº”ç”¨æƒé‡åçš„åˆ†æ•°
                    "raw_score": norm_score,
                    "type": "bm25",
                    "source": doc.metadata.get("source", "unknown")
                })

        # åˆå¹¶è¿‡æ»¤åçš„ç»“æœ
        results = filtered_vector_results + filtered_bm25_results
        
        # æ ¹æ®åŠ æƒåçš„åˆ†æ•°è¿›è¡Œæ’åº
        results = sorted(results, key=lambda x: x["weighted_score"], reverse=True)
        
        # æ–‡æ¡£å»é‡ï¼ˆå¯èƒ½åŒä¸€æ–‡æ¡£åŒæ—¶è¢«å‘é‡å’ŒBM25æ£€ç´¢åˆ°ï¼‰
        seen_docs = {}
        unique_results = []
        
        for res in results:
            doc_id = res["source"] + str(hash(res["doc"].page_content[:100]))
            
            if doc_id not in seen_docs:
                # ç¬¬ä¸€æ¬¡çœ‹åˆ°è¿™ä¸ªæ–‡æ¡£ï¼Œç›´æ¥æ·»åŠ 
                unique_results.append(res)
                seen_docs[doc_id] = len(unique_results) - 1
            else:
                # æ–‡æ¡£å·²å­˜åœ¨ï¼Œä¿ç•™å¾—åˆ†æ›´é«˜çš„ç‰ˆæœ¬
                existing_idx = seen_docs[doc_id]
                if res["weighted_score"] > unique_results[existing_idx]["weighted_score"]:
                    # æ›´æ–°ä¸ºæ›´é«˜åˆ†çš„ç‰ˆæœ¬
                    unique_results[existing_idx] = res

        logger.info(f"ğŸ“š æ··åˆæ£€ç´¢å¾—åˆ°{len(unique_results)}ç¯‡æ–‡æ¡£ï¼Œåº”ç”¨æƒé‡ [å‘é‡:{vector_weight:.2f}, BM25:{bm25_weight:.2f}]")
        return unique_results
    
    def _determine_retrieval_weights(self, question: str) -> Tuple[float, float]:
        """åŠ¨æ€ç¡®å®šæ£€ç´¢ç­–ç•¥æƒé‡ï¼Œè‡ªé€‚åº”ä¼˜åŒ–ä¸åŒç±»å‹çš„æŸ¥è¯¢
        
        :param question: ç”¨æˆ·é—®é¢˜
        :return: (å‘é‡æ£€ç´¢æƒé‡, BM25æ£€ç´¢æƒé‡)
        """
        # é»˜è®¤æƒé‡
        default_vector = 0.5
        default_bm25 = 0.5
        
        try:
            # 1. åŸºç¡€ç‰¹å¾è¯è¯†åˆ«
            
            # äº‹å®å‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘BM25ï¼‰
            factual_indicators = [
                'ä»€ä¹ˆæ˜¯', 'å®šä¹‰', 'å¦‚ä½•', 'æ€ä¹ˆ', 'å“ªäº›', 'è°', 'ä½•æ—¶', 'ä¸ºä»€ä¹ˆ', 
                'å¤šå°‘', 'æ•°æ®', 'æ ‡å‡†æ˜¯', 'è¦æ±‚æ˜¯', 'è§„å®š', 'æ¡ä¾‹', 'æ­¥éª¤',
                'æ–¹æ³•', 'æ“ä½œè§„ç¨‹', 'é™å€¼', 'ç±»å‹', 'ç§ç±»', 'åˆ†ç±»', 'æœ‰å“ªäº›',
                'åˆ—å‡º', 'æšä¸¾', 'æ ‡å‡†å€¼', 'å‚æ•°æ˜¯', 'æ•°å€¼', 'å…¬å¼', 'è®¡ç®—',
                'å¦‚ä½•åš', 'æ€æ ·åš', 'æ“ä½œæ–¹å¼', 'ä½¿ç”¨æ–¹æ³•', 'ä½¿ç”¨æ­¥éª¤', 'å¦‚ä½•å¤„ç†',
                'éœ€è¦ä»€ä¹ˆ', 'åŒ…æ‹¬å“ªäº›', 'ç»„æˆéƒ¨åˆ†', 'æ‰§è¡Œæ ‡å‡†', 'æ³•è§„è¦æ±‚', 'æŠ€æœ¯è§„èŒƒ',
                'åº”è¯¥æ€ä¹ˆ', 'éœ€è¦æ³¨æ„', 'æ³¨æ„äº‹é¡¹', 'æ£€æŸ¥é¡¹ç›®', 'ä¿å­˜æ¡ä»¶', 'å‚¨å­˜è¦æ±‚',
                'æœ‰æ•ˆæœŸ', 'å¤±æ•ˆæ—¥æœŸ', 'é€‚ç”¨èŒƒå›´', 'ä½¿ç”¨èŒƒå›´', 'å¸¸è§é—®é¢˜', 'æ•…éšœåŸå› '
            ]
            
            # æ¦‚å¿µå‹é—®é¢˜ç‰¹å¾è¯ï¼ˆåå‘å‘é‡æ£€ç´¢ï¼‰
            conceptual_indicators = [
                'è§£é‡Š', 'åˆ†æ', 'è¯„ä»·', 'æ¯”è¾ƒ', 'åŒºåˆ«', 'å…³ç³»', 'å½±å“', 'åŸç†', 
                'æœºåˆ¶', 'æ€è€ƒ', 'å¯èƒ½', 'å»ºè®®', 'é¢„æµ‹', 'æ¨æµ‹', 'ç»¼åˆ', 'æ€»ç»“',
                'è”ç³»', 'è¯¦ç»†æè¿°', 'æ·±å…¥', 'å¤æ‚', 'å…¨é¢', 'ä¸ºä»€ä¹ˆä¼š', 'å¦‚ä½•ç†è§£',
                'è®ºè¿°', 'é˜è¿°', 'æ¢è®¨', 'ç ”ç©¶', 'è§‚ç‚¹', 'çœ‹æ³•', 'ç†è®º', 'å­¦è¯´',
                'æ¨æ–­', 'å‡è®¾', 'çŒœæƒ³', 'å‰æ™¯', 'è¶‹åŠ¿', 'å‘å±•æ–¹å‘', 'æœªæ¥å¯èƒ½',
                'åˆ©å¼Š', 'ä¼˜ç¼ºç‚¹', 'åˆç†æ€§', 'å¯è¡Œæ€§', 'æœ‰æ•ˆæ€§', 'ç§‘å­¦ä¾æ®',
                'æ·±å±‚åŸå› ', 'æœ¬è´¨', 'å®è´¨', 'æ ¸å¿ƒé—®é¢˜', 'å…³é”®å› ç´ ', 'é‡è¦æ€§',
                'ç³»ç»Ÿæ€§', 'æ•´ä½“æ€§', 'ç»“æ„æ€§', 'è¾©è¯å…³ç³»', 'äº’åŠ¨æœºåˆ¶', 'ååŒæ•ˆåº”',
                'ç†è®ºåŸºç¡€', 'å“²å­¦æ€è€ƒ', 'åˆ›æ–°æ€è·¯', 'çªç ´ç‚¹', 'è§£å†³æ€è·¯'
            ]
            
            # 2. åŒ–å·¥ç‰¹å®šé¢†åŸŸç‰¹å¾è¯
            chemical_specific_terms = {
                # åŒ–å­¦ååº”ç±»ï¼ˆç²¾ç¡®åŒ¹é…é‡è¦ï¼ŒBM25ä¼˜åŠ¿ï¼‰
                'bm25_favor': [
                    'ååº”æ¡ä»¶', 'ååº”ç‰©', 'äº§ç‰©', 'å‚¬åŒ–å‰‚', 'åŒ–å­¦å¼', 'phå€¼', 'æ‘©å°”æ¯”',
                    'æ¸©åº¦èŒƒå›´', 'å‹åŠ›è¦æ±‚', 'ååº”æ—¶é—´', 'äº§ç‡', 'è½¬åŒ–ç‡', 'é€‰æ‹©æ€§',
                    'MSDS', 'å±é™©å“ç¼–å·', 'CASå·', 'ç†”ç‚¹', 'æ²¸ç‚¹', 'é—ªç‚¹', 'å¯†åº¦',
                    'æº¶è§£åº¦', 'æŒ¥å‘æ€§', 'ç²˜åº¦', 'æ¯”é‡', 'æŠ˜å°„ç‡', 'åˆ†å­é‡', 'åˆ†å­å¼',
                    'ç»“æ„å¼', 'å¼‚æ„ä½“', 'åŒåˆ†å¼‚æ„', 'æ™¶ä½“ç»“æ„', 'å…‰å­¦æ´»æ€§', 'æ—‹å…‰åº¦',
                    'é…¸å€¼', 'ç¢±å€¼', 'æ°§åŒ–è¿˜åŸç”µä½', 'ç¦»è§£å¸¸æ•°', 'ç”µå¯¼ç‡', 'çƒ­å¯¼ç‡',
                    'æ¯”çƒ­å®¹', 'çƒ­è†¨èƒ€ç³»æ•°', 'è’¸æ±½å‹', 'ä¸´ç•Œæ¸©åº¦', 'ä¸´ç•Œå‹åŠ›', 'ä¸´ç•Œä½“ç§¯',
                    'çˆ†ç‚¸æé™', 'è‡ªç‡ƒç‚¹', 'å¼•ç‡ƒæ¸©åº¦', 'ç‡ƒçƒ§çƒ­', 'ç‡ƒç‚¹', 'ç€ç«ç‚¹',
                    'ç¦å¿Œç‰©', 'èšåˆå±å®³', 'æ¯’æ€§åˆ†çº§', 'LD50', 'LC50', 'è‡´ç™Œæ€§',
                    'è…èš€æ€§', 'åˆºæ¿€æ€§', 'è‡´æ•æ€§', 'ç”Ÿç‰©åŠè¡°æœŸ', 'è“„ç§¯æ€§', 'é™è§£æ€§',
                    'å±é™©è´§ç‰©ç¼–å·', 'è”åˆå›½ç¼–å·', 'å±è§„å·', 'EINECSå·', 'RTECSå·',
                    # åŒ–å·¥å·¥è‰ºå‚æ•°
                    'å·¥è‰ºå‚æ•°', 'å·¥è‰ºæµç¨‹', 'å•å…ƒæ“ä½œ', 'è£…ç½®æ„æˆ', 'è®¾å¤‡å‚æ•°', 'ç®¡é“è§„æ ¼',
                    'é˜€é—¨ç±»å‹', 'ä»ªè¡¨å‹å·', 'æ§åˆ¶å‚æ•°', 'è¿›æ–™é€Ÿç‡', 'å‡ºæ–™é€Ÿç‡', 'å¾ªç¯æ¯”',
                    'åœç•™æ—¶é—´', 'ç©ºé€Ÿ', 'æ¶²ä½é«˜åº¦', 'é›·è¯ºæ•°', 'æ™®æœ—ç‰¹æ•°', 'ä¼ çƒ­ç³»æ•°',
                    'ä¼ è´¨ç³»æ•°', 'æµä½“é˜»åŠ›', 'æ…æ‹ŒåŠŸç‡', 'æ··åˆåº¦', 'åˆ†ç¦»åº¦', 'æçº¯åº¦',
                    # åŒ–å­¦å“å®‰å…¨å‚æ•°
                    'å±é™©åŒ–å­¦å“ç›®å½•', 'é‡å¤§å±é™©æº', 'ä¸´ç•Œé‡', 'å±é™©ç­‰çº§', 'å±å®³ç¨‹åº¦',
                    'å±å®³è¯†åˆ«ç ', 'GHSæ ‡è¯†', 'å±é™©æ€§è¯´æ˜', 'é¢„é˜²æªæ–½è¯´æ˜', 'è±¡å½¢å›¾',
                    'ä¿¡å·è¯', 'æ¯’æ€§çº§åˆ«', 'æ€¥æ€§æ¯’æ€§', 'æ…¢æ€§æ¯’æ€§', 'ç‰¹å®šé¶å™¨å®˜æ¯’æ€§',
                    'æ°´å±å®³ç­‰çº§', 'åœŸå£¤å±å®³', 'å¤§æ°”å±å®³', 'ç”Ÿç‰©ç´¯ç§¯æ€§', 'æŒä¹…æ€§',
                    # åŒ–å·¥è®¾å¤‡å®‰å…¨
                    'è®¾å¤‡å®‰å…¨é—´è·', 'é˜²çˆ†ç­‰çº§', 'é˜²ç«ç­‰çº§', 'é˜²è…ç­‰çº§', 'ä¿æŠ¤ç­‰çº§',
                    'è¿‡å‹ä¿æŠ¤', 'è¶…æ¸©ä¿æŠ¤', 'æ³„å‹è£…ç½®', 'å®‰å…¨é˜€', 'çˆ†ç ´ç‰‡',
                    'ç´§æ€¥åˆ‡æ–­é˜€', 'é˜»ç«å™¨', 'é˜²é›·è®¾æ–½', 'æ¥åœ°è£…ç½®', 'é™ç”µæ¶ˆé™¤',
                    'å®‰å…¨è”é”', 'è¿é”ä¿æŠ¤', 'åŒé‡ä¿é™©', 'å®‰å…¨è”é”', 'å¤±æ•ˆä¿æŠ¤',
                    # æ£€æµ‹æ£€éªŒ
                    'æ£€æµ‹æ–¹æ³•', 'æ£€æµ‹æ ‡å‡†', 'æ£€æµ‹å‘¨æœŸ', 'æ£€éªŒé¡¹ç›®', 'æ£€éªŒæ ‡å‡†',
                    'å–æ ·ç‚¹ä½', 'å–æ ·æ–¹æ³•', 'æ ·å“ä¿å­˜', 'åˆ†ææ–¹æ³•', 'ä»ªå™¨ç²¾åº¦',
                    'æ£€æµ‹é™', 'å®šé‡é™', 'æµ‹é‡ä¸ç¡®å®šåº¦', 'æ ¡å‡†å‘¨æœŸ', 'æ ‡æ ·æµ“åº¦',
                    'æ ‡å®šæ›²çº¿', 'æº¯æºæ€§', 'æ ¡éªŒå‘¨æœŸ', 'æ£€æµ‹å‘¨æœŸ', 'æ£€éªŒæŠ¥å‘Š'
                ],
                # å®‰å…¨ç†è®ºç±»ï¼ˆè¯­ä¹‰ç†è§£é‡è¦ï¼Œå‘é‡ä¼˜åŠ¿ï¼‰
                'vector_favor': [
                    'å®‰å…¨ç®¡ç†', 'é£é™©è¯„ä¼°', 'é¢„é˜²æªæ–½', 'åº”æ€¥é¢„æ¡ˆ', 'äº‹æ•…åˆ†æ',
                    'å®‰å…¨æ–‡åŒ–', 'æœ¬è´¨å®‰å…¨', 'å®‰å…¨ç³»ç»Ÿ', 'å±å®³è¯†åˆ«', 'é£é™©æ§åˆ¶',
                    'è¿é”ååº”', 'æ‰©æ•£æ¨¡å‹', 'ä¸´ç•Œç‚¹', 'ç¨³å®šæ€§', 'ç›¸å®¹æ€§',
                    'å®‰å…¨ç”Ÿäº§', 'èŒä¸šå¥åº·', 'ä½œä¸šç¯å¢ƒ', 'å®‰å…¨è´£ä»»åˆ¶', 'å®‰å…¨æ•™è‚²',
                    'å®‰å…¨æ£€æŸ¥', 'éšæ‚£æ’æŸ¥', 'å±é™©æºè¾¨è¯†', 'é£é™©åˆ†çº§', 'å®‰å…¨å®¡æ ¸',
                    'åŒé‡é¢„é˜²', 'å®‰å…¨æŠ•å…¥', 'å®‰å…¨æ ‡å‡†åŒ–', 'å®‰å…¨ç»©æ•ˆ', 'å®‰å…¨ç›®æ ‡',
                    'å®‰å…¨æŠ€æœ¯', 'å®‰å…¨è¯„ä»·', 'å®‰å…¨é˜²æŠ¤', 'å®‰å…¨ç›‘æµ‹', 'å®‰å…¨ä¿¡æ¯',
                    'äº‹æ•…è°ƒæŸ¥', 'äº‹æ•…è´£ä»»', 'å®‰å…¨æ”¹è¿›', 'å®‰å…¨æ‰¿è¯º', 'å®‰å…¨æ„¿æ™¯',
                    'å®‰å…¨é¢†å¯¼åŠ›', 'å®‰å…¨å‚ä¸', 'åˆè§„ç®¡ç†', 'åº”æ€¥å“åº”', 'åº”æ€¥å¤„ç½®',
                    'åº”æ€¥æ•‘æ´', 'ç–æ•£ç¨‹åº', 'æ•‘æ´è£…å¤‡', 'è­¦æˆ’åŒºåŸŸ', 'å®‰å…¨ç–æ•£',
                    'ä¼¤å‘˜æ•‘æŠ¤', 'å±é™©æºæ§åˆ¶', 'æ³„æ¼å¤„ç†', 'ç«ç¾æ‰‘æ•‘', 'çˆ†ç‚¸é˜²æŠ¤',
                    'äº‹æ•…æ•™è®­', 'ç»éªŒæ€»ç»“', 'æ”¹è¿›æªæ–½', 'ç³»ç»Ÿä¼˜åŒ–', 'è¿‡ç¨‹å®‰å…¨',
                    # å®‰å…¨ç®¡ç†ä½“ç³»
                    'å®‰å…¨ç”Ÿäº§æ³•', 'æ³•å¾‹æ³•è§„', 'å›½å®¶æ ‡å‡†', 'è¡Œä¸šæ ‡å‡†', 'ä¼ä¸šæ ‡å‡†',
                    'å®‰å…¨æ–¹é’ˆ', 'å®‰å…¨æ„¿æ™¯', 'å®‰å…¨æˆ˜ç•¥', 'å®‰å…¨è§„åˆ’', 'èŒè´£åˆ’åˆ†',
                    'PDCAå¾ªç¯', 'æŒç»­æ”¹è¿›', 'é—­ç¯ç®¡ç†', 'ä½“ç³»å®¡æ ¸', 'ç¬¦åˆæ€§è¯„ä»·',
                    'ç®¡ç†è¯„å®¡', 'è‡ªæˆ‘è¯„ä»·', 'å®‰å…¨è®¤è¯', 'ä½“ç³»å»ºè®¾', 'ç»„ç»‡æœºæ„',
                    'å®‰å…¨å§”å‘˜ä¼š', 'å®‰å…¨ç®¡ç†éƒ¨é—¨', 'å®‰å…¨æ€»ç›‘', 'å®‰å…¨å±¥èŒ', 'å®‰å…¨é—®è´£',
                    # æ–°é™©ç®¡ç†ä¸æ§åˆ¶
                    'å®‰å…¨é£é™©', 'é£é™©æº', 'é£é™©çŸ©é˜µ', 'é£é™©æ¥å—åº¦', 'é£é™©å†³ç­–',
                    'é£é™©æ²Ÿé€š', 'è„†å¼±æ€§åˆ†æ', 'å¤±æ•ˆæ¨¡å¼', 'ä½œä¸šå±å®³åˆ†æ', 'å±å®³ä¸å¯æ“ä½œæ€§åˆ†æ',
                    'æ•…éšœæ ‘åˆ†æ', 'äº‹ä»¶æ ‘åˆ†æ', 'å®‰å…¨å®Œæ•´æ€§ç­‰çº§', 'åŠŸèƒ½å®‰å…¨', 'å±‚æ¬¡ä¿æŠ¤',
                    'æœ¬è´¨å®‰å…¨è®¾è®¡', 'éä¾µå…¥å¼å®‰å…¨', 'å›ºæœ‰å®‰å…¨', 'å®‰å…¨è£•åº¦', 'å®¹é”™è®¾è®¡',
                    'é˜²è¯¯æ“ä½œ', 'äººå› å·¥ç¨‹', 'å†—ä½™è®¾è®¡', 'å¤šæ ·æ€§è®¾è®¡', 'çºµæ·±é˜²å¾¡',
                    # åº”æ€¥ç®¡ç†
                    'åº”æ€¥ç®¡ç†ä½“ç³»', 'åº”æ€¥é¢„æ¡ˆä½“ç³»', 'åº”æ€¥èƒ½åŠ›è¯„ä¼°', 'åº”æ€¥æ¼”ç»ƒ', 'åº”æ€¥åŸ¹è®­',
                    'åº”æ€¥æŒ‡æŒ¥', 'åº”æ€¥å†³ç­–', 'åº”æ€¥æ²Ÿé€š', 'åº”æ€¥åè°ƒ', 'åŒºåŸŸè”åŠ¨',
                    'ä¸“å®¶ç»„', 'åº”æ€¥èµ„æº', 'åº”æ€¥ç‰©èµ„', 'è­¦æŠ¥ç³»ç»Ÿ', 'æŠ¥è­¦è”åŠ¨',
                    'æƒ…æ™¯æ„å»º', 'æƒ…æ™¯æ¨¡æ‹Ÿ', 'æƒ…æ™¯åº”å¯¹', 'äº‹ä»¶å‡çº§', 'äº‹ä»¶é™çº§',
                    'æ¢å¤é‡å»º', 'äº‹åè¯„ä¼°', 'å¿ƒç†ç–å¯¼', 'ç¤¾ä¼šç¨³å®š', 'ç¯å¢ƒä¿®å¤',
                    # å®‰å…¨æ–‡åŒ–
                    'å®‰å…¨æ°›å›´', 'å®‰å…¨æ„è¯†', 'å®‰å…¨è¡Œä¸º', 'å®‰å…¨æ€åº¦', 'å®‰å…¨ä¹ æƒ¯',
                    'å®‰å…¨å¿ƒç†', 'å®‰å…¨æ„ŸçŸ¥', 'å®‰å…¨è®¤çŸ¥', 'å®‰å…¨å†³ç­–', 'å®‰å…¨ä»·å€¼è§‚',
                    'å®‰å…¨ä¿¡å¿µ', 'å®‰å…¨åŠ¨æœº', 'å®‰å…¨æ‰¿è¯º', 'ä¸»åŠ¨å®‰å…¨', 'è¢«åŠ¨å®‰å…¨',
                    'å®‰å…¨æ¿€åŠ±', 'å®‰å…¨æ²Ÿé€š', 'å®‰å…¨å¯¹è¯', 'å®‰å…¨å­¦ä¹ ', 'æ ‡æ†ç®¡ç†',
                    'æœ€ä½³å®è·µ', 'ç»éªŒåˆ†äº«', 'å®‰å…¨è­¦ç¤º', 'å®‰å…¨è­¦å¥', 'å®‰å…¨å®£ä¼ '
                ]
            }
                               
            # è®¡ç®—å„ç±»ç‰¹å¾å‡ºç°æ¬¡æ•°ï¼ˆæƒé‡è®¡æ•°ï¼‰
            factual_count = sum(1 for term in factual_indicators if term in question)
            conceptual_count = sum(1 for term in conceptual_indicators if term in question)
            
            # åŒ–å·¥ç‰¹å®šæœ¯è¯­æƒé‡ï¼ˆé¢å¤–åŠ æƒï¼‰
            chemical_bm25_count = sum(1.5 for term in chemical_specific_terms['bm25_favor'] if term in question)
            chemical_vector_count = sum(1.5 for term in chemical_specific_terms['vector_favor'] if term in question)
            
            # ç´¯åŠ é¢†åŸŸç‰¹å¾æƒé‡
            factual_count += chemical_bm25_count
            conceptual_count += chemical_vector_count
            
            # 3. æ•°å€¼å‹æŸ¥è¯¢ç‰¹å¾è¯†åˆ«ï¼ˆæ•°å€¼æŸ¥è¯¢é€šå¸¸æ˜¯ç²¾ç¡®åŒ¹é…ï¼Œåå‘BM25ï¼‰
            number_pattern = r'\d+\.?\d*'
            unit_pattern = r'(åº¦|å…‹|åƒå…‹|å¨|å‡|æ¯«å‡|ppm|mg|kg|â„ƒ|mol|Pa|MPa|atm)'
            
            # åˆ¤æ–­æ˜¯å¦åŒ…å«æ•°å­—+å•ä½ç»„åˆ
            has_numeric_query = bool(re.search(number_pattern + r'.*?' + unit_pattern, question) or 
                                     re.search(unit_pattern + r'.*?' + number_pattern, question))
            
            if has_numeric_query:
                factual_count += 2  # æ•°å€¼ç±»æŸ¥è¯¢æ˜¾è‘—å¢åŠ BM25æƒé‡
            
            # 4. ä¸“æœ‰åè¯è¯†åˆ«ï¼ˆåŒ–å­¦å“åç§°ã€è®¾å¤‡åç§°ç­‰ä¸“æœ‰åè¯åå‘BM25ç²¾ç¡®åŒ¹é…ï¼‰
            # ç®€å•å¯å‘å¼ï¼šè¿ç»­çš„éå¸¸è§è¯å¯èƒ½æ˜¯ä¸“æœ‰åè¯
            words = self._tokenize(question)
            for i in range(len(words)-1):
                if len(words[i]) >= 2 and len(words[i+1]) >= 2:  # è¿ç»­ä¸¤ä¸ªé•¿è¯
                    # å‡è®¾è¿™å¯èƒ½æ˜¯ä¸“æœ‰åè¯
                    factual_count += 0.5
            
            # 5. è€ƒè™‘é—®é¢˜é•¿åº¦å’Œå¤æ‚åº¦
            query_length = len(question)
            length_factor = min(1.0, query_length / 50)  # æ ‡å‡†åŒ–é•¿åº¦å› ç´ 
            
            # å¥å­å¤æ‚åº¦ï¼ˆä»¥é€—å·ã€å¥å·ç­‰æ ‡ç‚¹ç¬¦å·æ•°é‡ä¸ºå‚è€ƒï¼‰
            punctuation_count = len(re.findall(r'[ï¼Œã€‚ï¼Ÿï¼ï¼›ï¼šã€]', question))
            complexity_factor = min(1.0, punctuation_count / 3)
            
            # 6. è®¡ç®—åå‘ç³»æ•°
            # ç‰¹å¾è¯æ¯”ä¾‹ï¼Œå†³å®šåŸºç¡€åå‘æ–¹å‘
            feature_bias = 0
            if factual_count > 0 or conceptual_count > 0:
                feature_bias = (factual_count - conceptual_count) / (factual_count + conceptual_count)
                # feature_biasèŒƒå›´ä¸º[-1, 1]ï¼Œæ­£å€¼åå‘BM25ï¼Œè´Ÿå€¼åå‘å‘é‡
            
            # 7. ç¡®å®šæœ€ç»ˆæƒé‡
            if feature_bias > 0.1:  # æ˜æ˜¾åå‘äº‹å®å‹/BM25
                # äº‹å®å‹é—®é¢˜ï¼šå¢åŠ BM25æƒé‡
                base_bm25 = 0.6 + 0.2 * min(abs(feature_bias), 0.4)  # æœ€é«˜åˆ°0.8
                # æ•°å€¼æŸ¥è¯¢é¢å¤–åŠ æƒ
                if has_numeric_query:
                    base_bm25 = min(0.85, base_bm25 + 0.1)
                bm25_weight = base_bm25
                vector_weight = 1.0 - bm25_weight
            elif feature_bias < -0.1:  # æ˜æ˜¾åå‘æ¦‚å¿µå‹/å‘é‡
                # æ¦‚å¿µå‹é—®é¢˜ï¼šå¢åŠ å‘é‡æƒé‡
                base_vector = 0.6 + 0.2 * min(abs(feature_bias), 0.4)  # æœ€é«˜åˆ°0.8
                # é•¿å¥å’Œå¤æ‚å¥å­åŠ æƒ
                vector_weight = base_vector + 0.1 * (length_factor + complexity_factor) / 2
                vector_weight = min(0.85, vector_weight)  # é™åˆ¶æœ€å¤§å€¼
                bm25_weight = 1.0 - vector_weight
            else:  # æ··åˆç±»å‹ï¼ˆ-0.1åˆ°0.1ä¹‹é—´ï¼‰
                # æ··åˆç±»å‹ï¼šä¿æŒå¹³è¡¡ï¼Œå¾®è°ƒ
                vector_weight = default_vector + 0.1 * length_factor
                bm25_weight = 1.0 - vector_weight
            
            # 8. ç»“æœæ—¥å¿—è®°å½•ï¼ˆä¾¿äºè°ƒè¯•å’Œæ”¹è¿›ï¼‰
            logger.debug(f"æŸ¥è¯¢æƒé‡åˆ†æ - é—®é¢˜: {question[:30]}...")
            logger.debug(f"  â€¢ äº‹å®ç‰¹å¾å¾—åˆ†: {factual_count:.2f}, æ¦‚å¿µç‰¹å¾å¾—åˆ†: {conceptual_count:.2f}")
            logger.debug(f"  â€¢ åå‘ç³»æ•°: {feature_bias:.2f}, é•¿åº¦å› å­: {length_factor:.2f}")
            logger.debug(f"  â€¢ æœ€ç»ˆæƒé‡ - å‘é‡: {vector_weight:.2f}, BM25: {bm25_weight:.2f}")
                
            # ç¡®ä¿æƒé‡ç›¸åŠ ä¸º1
            total = vector_weight + bm25_weight
            return vector_weight/total, bm25_weight/total
            
        except Exception as e:
            logger.warning(f"âš ï¸ åŠ¨æ€æƒé‡è®¡ç®—å¤±è´¥: {str(e)}ï¼Œä½¿ç”¨é»˜è®¤æƒé‡")
            return default_vector, default_bm25

    def _rerank_documents(self, results: List[Dict], question: str) -> List[Dict]:
        """ä½¿ç”¨é‡æ’åºæ¨¡å‹ä¼˜åŒ–æ£€ç´¢ç»“æœ

        :param results: æ£€ç´¢ç»“æœåˆ—è¡¨
        :param question: åŸå§‹é—®é¢˜
        :return: é‡æ’åºåçš„ç»“æœåˆ—è¡¨
        """
        try:
            if not results:
                return results

            # æ‰¹å¤„ç†é€»è¾‘ï¼Œæ¯æ¬¡å¤„ç†å°‘é‡æ–‡æ¡£
            batch_size = 8  # å‡å°æ‰¹å¤„ç†å¤§å°ä»¥é¿å…å¼ é‡ç»´åº¦ä¸åŒ¹é…
            batched_rerank_scores = []
            
            # é™åˆ¶æ–‡æ¡£é•¿åº¦ï¼Œé¿å…è¿‡é•¿æ–‡æ¡£
            max_doc_length = 5000  # è®¾ç½®æœ€å¤§æ–‡æ¡£é•¿åº¦
            for res in results:
                if len(res["doc"].page_content) > max_doc_length:
                    res["doc"].page_content = res["doc"].page_content[:max_doc_length]
            
            # åˆ†æ‰¹å¤„ç†æ–‡æ¡£
            for i in range(0, len(results), batch_size):
                batch_results = results[i:i+batch_size]
                batch_pairs = [(question, res["doc"].page_content) for res in batch_results]
                
                try:
                    # å¯¹è¾“å…¥è¿›è¡Œtokenizeå’Œæ‰¹å¤„ç†
                    batch_inputs = self.rerank_tokenizer(
                        batch_pairs,
                        padding=True,
                        truncation=True,
                        max_length=512,  # é™åˆ¶ç»Ÿä¸€çš„æœ€å¤§é•¿åº¦
                        return_tensors="pt"
                    )
                    
                    # æ¨¡å‹æ¨ç†
                    with torch.no_grad():
                        batch_outputs = self.rerank_model(**batch_inputs)
                        # ä½¿ç”¨sigmoidè½¬æ¢åˆ†æ•°
                        batch_scores = torch.sigmoid(batch_outputs.logits).squeeze().tolist()
                        
                        # ç¡®ä¿batch_scoresæ˜¯åˆ—è¡¨
                        if not isinstance(batch_scores, list):
                            batch_scores = [batch_scores]
                        
                        batched_rerank_scores.extend(batch_scores)
                except Exception as e:
                    # æ‰¹å¤„ç†å¤±è´¥æ—¶ï¼Œä½¿ç”¨åŸå§‹åˆ†æ•°
                    logger.warning(f"æ–‡æ¡£æ‰¹æ¬¡ {i//batch_size+1} é‡æ’åºå¤±è´¥: {str(e)}")
                    for res in batch_results:
                        batched_rerank_scores.append(res["score"])

            # æ›´æ–°ç»“æœåˆ†æ•°
            for res, rerank_score in zip(results, batched_rerank_scores):
                # ç›´æ¥ä½¿ç”¨é‡æ’åºåˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                res.update({
                    "original_score": res["score"],  # ä¿å­˜åŸå§‹æ£€ç´¢åˆ†æ•°
                    "rerank_score": rerank_score,
                    "final_score": rerank_score  # ç›´æ¥ä½¿ç”¨é‡æ’åºåˆ†æ•°ä½œä¸ºæœ€ç»ˆåˆ†æ•°
                })
                
                # è®°å½•æ—¥å¿—
                logger.debug(f"æ–‡æ¡£é‡æ’åº: {res['source']} - åŸå§‹åˆ†æ•°: {res['original_score']:.4f} - é‡æ’åºåˆ†æ•°: {rerank_score:.4f}")

            # æŒ‰æœ€ç»ˆåˆ†æ•°é™åºæ’åˆ—
            sorted_results = sorted(results, key=lambda x: x["final_score"], reverse=True)
            
            # åº”ç”¨å¤šæ ·æ€§å¢å¼ºç­–ç•¥
            return self._diversify_results(sorted_results)
            
        except Exception as e:
            logger.error(f"é‡æ’åºæ•´ä½“å¤±è´¥: {str(e)}")
            # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å¿…è¦çš„å­—æ®µ
            for res in results:
                if "final_score" not in res:
                    res["final_score"] = res["score"]
                if "rerank_score" not in res:
                    res["rerank_score"] = res["score"]
                if "original_score" not in res:
                    res["original_score"] = res["score"]
            
            # è¿”å›åŸå§‹æ’åºçš„ç»“æœ
            return sorted(results, key=lambda x: x["score"], reverse=True)
    
    def _diversify_results(self, ranked_results: List[Dict]) -> List[Dict]:
        """å¢å¼ºæ£€ç´¢ç»“æœçš„å¤šæ ·æ€§
        
        ä½¿ç”¨MMR(Maximum Marginal Relevance)ç®—æ³•å¹³è¡¡ç›¸å…³æ€§å’Œå¤šæ ·æ€§
        
        :param ranked_results: æŒ‰åˆ†æ•°æ’åºçš„æ£€ç´¢ç»“æœ
        :return: å¤šæ ·æ€§å¢å¼ºåçš„ç»“æœ
        """
        if len(ranked_results) <= 2:
            return ranked_results  # ç»“æœå¤ªå°‘ä¸éœ€è¦å¤šæ ·æ€§ä¼˜åŒ–
        
        try:
            # MMRå‚æ•°
            lambda_param = 0.7  # æ§åˆ¶ç›¸å…³æ€§vså¤šæ ·æ€§çš„å¹³è¡¡ï¼Œè¶Šå¤§è¶Šåå‘ç›¸å…³æ€§
            
            # åˆå§‹åŒ–å·²é€‰æ‹©å’Œå€™é€‰æ–‡æ¡£
            selected = [ranked_results[0]]  # æœ€é«˜åˆ†æ–‡æ¡£ç›´æ¥é€‰å…¥
            candidates = ranked_results[1:]
            
            # å¤„ç†top 20æ–‡æ¡£
            while len(selected) < min(len(ranked_results), self.config.final_top_k):
                # è®¡ç®—æ¯ä¸ªå€™é€‰æ–‡æ¡£çš„MMRåˆ†æ•°
                mmr_scores = []
                
                for candidate in candidates:
                    # è®¡ç®—ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆç›¸å…³æ€§éƒ¨åˆ†ï¼‰
                    relevance = candidate["final_score"]
                    
                    # è®¡ç®—ä¸å·²é€‰æ–‡æ¡£çš„æœ€å¤§ç›¸ä¼¼åº¦ï¼ˆå¤šæ ·æ€§éƒ¨åˆ†ï¼‰
                    max_sim = 0
                    for selected_doc in selected:
                        # ä½¿ç”¨æ–‡æœ¬å†…å®¹çš„è¯é‡å è®¡ç®—ç›¸ä¼¼åº¦
                        sim = self._compute_document_similarity(
                            candidate["doc"].page_content,
                            selected_doc["doc"].page_content
                        )
                        max_sim = max(max_sim, sim)
                    
                    # è®¡ç®—MMRåˆ†æ•°
                    mmr = lambda_param * relevance - (1 - lambda_param) * max_sim
                    mmr_scores.append(mmr)
                
                # é€‰æ‹©MMRåˆ†æ•°æœ€é«˜çš„æ–‡æ¡£
                best_idx = mmr_scores.index(max(mmr_scores))
                selected.append(candidates.pop(best_idx))
            
            # è¿”å›å¤šæ ·æ€§å¢å¼ºåçš„æ–‡æ¡£
            return selected
            
        except Exception as e:
            logger.error(f"å¤šæ ·æ€§å¢å¼ºå¤±è´¥: {str(e)}")
            # å¤±è´¥æ—¶è¿”å›åŸå§‹æ’åºçš„å‰20ä¸ªæ–‡æ¡£
            return ranked_results[:self.config.final_top_k]
    
    def _compute_document_similarity(self, doc1: str, doc2: str) -> float:
        """è®¡ç®—ä¸¤ä¸ªæ–‡æ¡£ä¹‹é—´çš„ç›¸ä¼¼åº¦
        
        :param doc1: ç¬¬ä¸€ä¸ªæ–‡æ¡£å†…å®¹
        :param doc2: ç¬¬äºŒä¸ªæ–‡æ¡£å†…å®¹
        :return: ç›¸ä¼¼åº¦åˆ†æ•°ï¼ˆ0-1ï¼‰
        """
        try:
            # ä½¿ç”¨åŸºäºè¯é›†åˆçš„Jaccardç›¸ä¼¼åº¦
            tokens1 = set(self._tokenize(doc1))
            tokens2 = set(self._tokenize(doc2))
            
            # è®¡ç®—Jaccardç³»æ•°
            if not tokens1 or not tokens2:
                return 0.0
                
            intersection = tokens1.intersection(tokens2)
            union = tokens1.union(tokens2)
            
            # å¦‚æœæ–‡æ¡£é•¿åº¦ç›¸å·®å¤ªå¤§ï¼Œç»™äºˆæƒ©ç½š
            len_ratio = min(len(doc1), len(doc2)) / max(len(doc1), len(doc2))
            
            # åŠ æƒç›¸ä¼¼åº¦
            return (len(intersection) / len(union)) * len_ratio
            
        except Exception as e:
            logger.warning(f"æ–‡æ¡£ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥: {str(e)}")
            return 0.0

    def _retrieve_documents(self, question: str) -> Tuple[List[Document], List[Dict]]:
        """å®Œæ•´æ£€ç´¢æµç¨‹

        :param question: ç”¨æˆ·é—®é¢˜
        :return: (æ–‡æ¡£åˆ—è¡¨, åˆ†æ•°ä¿¡æ¯åˆ—è¡¨)
        """
        try:
            # æ··åˆæ£€ç´¢
            raw_results = self._hybrid_retrieve(question)
            if not raw_results:
                logger.warning("æ··åˆæ£€ç´¢æœªè¿”å›ä»»ä½•ç»“æœ")
                return [], []

            # ç›´æ¥é‡æ’åº
            try:
                reranked = self._rerank_documents(raw_results, question)
            except Exception as e:
                logger.error(f"é‡æ’åºå®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹ç»“æœ: {str(e)}")
                # ç¡®ä¿æ¯ä¸ªç»“æœéƒ½æœ‰å¿…è¦çš„å­—æ®µ
                for res in raw_results:
                    if "final_score" not in res:
                        res["final_score"] = res["score"]
                    if "rerank_score" not in res:
                        res["rerank_score"] = res["score"]
                reranked = sorted(raw_results, key=lambda x: x["score"], reverse=True)

            # æ ¹æ®é˜ˆå€¼è¿‡æ»¤ç»“æœ
            try:
                final_results = [
                    res for res in reranked
                    if res["final_score"] >= self.config.similarity_threshold
                    and len(res["doc"].page_content.strip()) >= 12  # æ·»åŠ é•¿åº¦æ£€æŸ¥
                ]
                final_results = sorted(
                    final_results,
                    key=lambda x: x["final_score"],
                    reverse=True
                )[:self.config.final_top_k]  # é™åˆ¶è¿”å›æ•°é‡
            except Exception as e:
                logger.error(f"ç»“æœè¿‡æ»¤å¤±è´¥ï¼Œä½¿ç”¨å‰Nä¸ªç»“æœ: {str(e)}")
                final_results = reranked[:min(len(reranked), self.config.final_top_k)]

            # è¾“å‡ºæœ€ç»ˆåˆ†æ•°ä¿¡æ¯
            logger.info(f"ğŸ“Š æœ€ç»ˆæ–‡æ¡£æ•°ç›®:{len(final_results)}ç¯‡")

            # æå–æ–‡æ¡£å’Œåˆ†æ•°ä¿¡æ¯
            docs = []
            score_info = []
            
            for res in final_results:
                try:
                    doc = res["doc"]
                    info = {
                        "source": res["source"],
                        "type": res.get("type", "unknown"),
                        "vector_score": res.get("score", 0),
                        "bm25_score": res.get("score", 0),
                        "rerank_score": res.get("rerank_score", res.get("score", 0)),
                        "final_score": res.get("final_score", res.get("score", 0))
                    }
                    docs.append(doc)
                    score_info.append(info)
                except Exception as e:
                    logger.warning(f"å¤„ç†å•ä¸ªç»“æœæ—¶å‡ºé”™ï¼Œå·²è·³è¿‡: {str(e)}")
                    continue

            return docs, score_info
        except Exception as e:
            logger.error(f"æ–‡æ¡£æ£€ç´¢ä¸¥é‡å¤±è´¥: {str(e)}", exc_info=True)
            # ç´§æ€¥æƒ…å†µä¸‹è¿”å›ç©ºç»“æœè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸
            return [], []

    def _build_prompt(self, question: str, context: str) -> str:
        """æ„å»ºæç¤ºè¯æ¨¡æ¿"""
        # ç³»ç»Ÿè§’è‰²å®šä¹‰
        system_role = (
            "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œå…·æœ‰æ·±åšçš„ä¸“ä¸šçŸ¥è¯†å’Œå®è·µç»éªŒã€‚"
            "ä½ éœ€è¦åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™ï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"
        )
        
        # æ€è€ƒè¿‡ç¨‹æŒ‡ä»¤
        reasoning_instruction = (
            "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š\n"
            "1. ä»”ç»†é˜…è¯»å¹¶ç†è§£æä¾›çš„å‚è€ƒèµ„æ–™\n"
            "2. åˆ†æé—®é¢˜ä¸­çš„å…³é”®ä¿¡æ¯å’Œè¦æ±‚\n"
            "3. ä»å‚è€ƒèµ„æ–™ä¸­æå–ç›¸å…³ä¿¡æ¯\n"
            "4. ç»™å‡ºè¯¦ç»†çš„æ¨ç†è¿‡ç¨‹\n"
            "5. æ€»ç»“å¹¶ç»™å‡ºæœ€ç»ˆç­”æ¡ˆ\n\n"
            "å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·ç›´æ¥è¯´æ˜æ— æ³•å›ç­”ã€‚"
        )
        
        if context:
            return (
                "<|im_start|>system\n"
                f"{system_role}\n"
                f"{reasoning_instruction}\n"
                "å‚è€ƒèµ„æ–™ï¼š\n{context}\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question, context=context[:self.config.max_context_length])
        else:
            return (
                "<|im_start|>system\n"
                f"ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œ{cot_instruction}\n"
                "<|im_end|>\n"
                "<|im_start|>user\n"
                "{question}\n"
                "<|im_end|>\n"
                "<|im_start|>assistant\n"
            ).format(question=question)

    def _build_chat_prompt(self, current_question: str, chat_history: List[Dict], context: str = "") -> str:
        """æ„å»ºå¤šè½®å¯¹è¯çš„æç¤ºè¯æ¨¡æ¿"""
        # ç³»ç»Ÿè§’è‰²å®šä¹‰
        system_role = (
            "ä½ æ˜¯ä¸€ä½æ‹¥æœ‰20å¹´ç»éªŒçš„åŒ–å·¥å®‰å…¨é¢†åŸŸæƒå¨ä¸“å®¶ï¼Œç²¾é€šå±é™©åŒ–å­¦å“ç®¡ç†ã€å®‰å…¨ç”Ÿäº§ã€å·¥è‰ºå®‰å…¨ã€åº”æ€¥å“åº”å’Œé£é™©è¯„ä¼°ã€‚"
            "ä½ æŒæ¡å›½å†…å¤–åŒ–å·¥å®‰å…¨æ³•è§„æ ‡å‡†ï¼Œç†Ÿæ‚‰HAZOPã€LOPAã€JSAç­‰å®‰å…¨åˆ†ææ–¹æ³•ï¼Œäº†è§£æœ€æ–°çš„å®‰å…¨æŠ€æœ¯å’Œç®¡ç†å®è·µã€‚"
            "ä½ éœ€è¦åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å’ŒèŠå¤©å†å²ï¼Œç»™å‡ºå‡†ç¡®ã€ä¸“ä¸šä¸”æ˜“äºç†è§£çš„å›ç­”ã€‚"
            "ä½ å§‹ç»ˆåšæŒ'å®‰å…¨ç¬¬ä¸€'åŸåˆ™ï¼Œåœ¨å›ç­”ä¸­ä¼˜å…ˆè€ƒè™‘äººå‘˜å®‰å…¨å’Œç¯å¢ƒä¿æŠ¤ã€‚"
            "ä½ çš„å›ç­”åº”ä¿æŒè¿è´¯æ€§å’Œä¸€è‡´æ€§ï¼Œè€ƒè™‘ä¹‹å‰çš„å¯¹è¯å†…å®¹ï¼Œå¹¶é’ˆå¯¹åŒ–å·¥å®‰å…¨é¢†åŸŸç‰¹å®šæƒ…å¢ƒæä¾›å®ç”¨å»ºè®®ã€‚"
            
            "\nåœ¨å›ç­”æ¶‰åŠæ³•è§„æ ‡å‡†æ—¶ï¼Œè¯·æ˜ç¡®å¼•ç”¨ç›¸å…³ä¾æ®ã€‚"
            "\nåœ¨å›ç­”é£é™©è¯„ä¼°é—®é¢˜æ—¶ï¼Œè¯·è¿ç”¨ç³»ç»Ÿæ€§æ€ç»´ï¼Œè€ƒè™‘å¤šç§å±å®³å› ç´ ã€‚"
            "\nåœ¨å›ç­”å·¥è‰ºå®‰å…¨é—®é¢˜æ—¶ï¼Œè¯·ç»“åˆåŒ–å­¦ååº”æœºç†å’Œå·¥ç¨‹æ§åˆ¶æªæ–½ã€‚"
            "\nåœ¨å›ç­”è®¾å¤‡å®‰å…¨é—®é¢˜æ—¶ï¼Œè¯·ç»“åˆææ–™ç§‘å­¦å’Œæœºæ¢°å®Œæ•´æ€§åŸåˆ™ã€‚"
            "\nåœ¨å›ç­”åº”æ€¥å“åº”é—®é¢˜æ—¶ï¼Œè¯·æä¾›æ¸…æ™°çš„ç¨‹åºæ­¥éª¤å’Œæ³¨æ„äº‹é¡¹ã€‚"
            
            "\nè¯·ç¡®ä¿ç”¨æ˜“äºç†è§£çš„æ–¹å¼è¡¨è¾¾ä¸“ä¸šå†…å®¹ï¼Œé¿å…è¿‡åº¦ä½¿ç”¨æœ¯è¯­è€Œä¸è§£é‡Šã€‚"
            "\nå¦‚é‡ç´§æ€¥æƒ…å†µç±»é—®é¢˜ï¼Œè¯·å¼ºè°ƒç«‹å³é‡‡å–è¡ŒåŠ¨çš„é‡è¦æ€§å¹¶æä¾›å…·ä½“æŒ‡å¯¼ã€‚"
        )
        
        # å›ç­”æ­¥éª¤æŒ‡å¯¼
        reasoning_steps = (
            "è¯·æŒ‰ç…§ä»¥ä¸‹æ­¥éª¤å›ç­”é—®é¢˜ï¼š\n"
            "1. ä»”ç»†åˆ†æé—®é¢˜çš„å…³é”®ç‚¹å’ŒåŒ–å·¥å®‰å…¨é¢†åŸŸèƒŒæ™¯\n"
            "2. å…¨é¢å®¡è§†æä¾›çš„å‚è€ƒèµ„æ–™ï¼Œæ‰¾å‡ºç›¸å…³ä¿¡æ¯\n"
            "3. æ ¹æ®åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“ä¸šçŸ¥è¯†è¯„ä¼°ä¿¡æ¯çš„é€‚ç”¨æ€§\n"
            "4. ä»å¤šè§’åº¦(å·¥è‰ºã€è®¾å¤‡ã€äººå‘˜ã€ç®¡ç†)è€ƒè™‘é—®é¢˜\n"
            "5. æ„å»ºæ¸…æ™°çš„æŠ€æœ¯åˆ†æå’Œæ¨ç†è¿‡ç¨‹\n"
            "6. ç¡®ä¿å›ç­”å®ç”¨ã€å¯æ“ä½œä¸”ç¬¦åˆå®‰å…¨è§„èŒƒ\n"
            "7. æ€»ç»“æ ¸å¿ƒè§‚ç‚¹å¹¶ç»™å‡ºæ˜ç¡®å»ºè®®\n\n"
            "å¦‚æœå‚è€ƒèµ„æ–™ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·æ˜ç¡®æŒ‡å‡ºä¿¡æ¯çš„å±€é™æ€§ï¼Œå¹¶åŸºäºåŒ–å·¥å®‰å…¨åŸç†æä¾›ä¸€èˆ¬æ€§æŒ‡å¯¼ã€‚"
            "å¦‚æœé—®é¢˜æ¶‰åŠç´§æ€¥å±é™©æƒ…å†µï¼Œä¼˜å…ˆå¼ºè°ƒäººå‘˜å®‰å…¨å’Œåº”æ€¥æªæ–½ã€‚"
        )
        
        # æ„å»ºç³»ç»Ÿæç¤ºéƒ¨åˆ†
        prompt = "<|im_start|>system\n" + system_role + "\n\n" + reasoning_steps + "\n"
        
        # æ·»åŠ å‚è€ƒèµ„æ–™ï¼ˆå¦‚æœæœ‰ï¼‰
        if context:
            prompt += "\nå‚è€ƒèµ„æ–™ï¼š\n" + context[:self.config.max_context_length] + "\n"
        
        prompt += "<|im_end|>\n"
        
        # æ·»åŠ èŠå¤©å†å²
        for message in chat_history:
            role = "user" if message["message_type"] == "user" else "assistant"
            content = message.get("content", "")
            if content:  # ç¡®ä¿æ¶ˆæ¯å†…å®¹ä¸ä¸ºç©º
                prompt += f"<|im_start|>{role}\n{content}\n<|im_end|>\n"
        
        # æ·»åŠ å½“å‰é—®é¢˜å’ŒåŠ©æ‰‹è§’è‰²
        prompt += f"<|im_start|>user\n{current_question}\n<|im_end|>\n"
        prompt += "<|im_start|>assistant\n"
        
        return prompt
        
    def _format_references(self, docs: List[Document], score_info: List[Dict]) -> List[Dict]:
        """æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯"""
        return [
            {
                "file": str(Path(info["source"]).name),  # æ–‡ä»¶å
                "content": doc.page_content,  # æˆªå–å‰500å­—ç¬¦
                "score": info["final_score"],  # ç»¼åˆè¯„åˆ†
                "type": info["type"],  # æ£€ç´¢ç±»å‹
                "full_path": info["source"]  # å®Œæ•´æ–‡ä»¶è·¯å¾„
            }
            for doc, info in zip(docs, score_info)
        ]


    def stream_query_with_history(self, session_id: str, current_question: str, 
                               chat_history: List[Dict] = None) -> Generator[str, None, None]:
        """å¸¦èŠå¤©å†å²çš„æµå¼RAGæŸ¥è¯¢
        
        :param session_id: ä¼šè¯ID
        :param current_question: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: èŠå¤©å†å²åˆ—è¡¨
        :return: ç”Ÿæˆå™¨ï¼Œæµå¼è¾“å‡ºç»“æœ
        """
        logger.info(f"ğŸ”„ å¤šè½®å¯¹è¯å¤„ç† | ä¼šè¯ID: {session_id} | é—®é¢˜: {current_question[:50]}...")
        
        if not current_question.strip():
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            }) + "\n"
            return
        
        # åˆå§‹åŒ–èŠå¤©å†å²
        if chat_history is None:
            chat_history = []
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(current_question)
                if not docs:
                    logger.warning(f"æŸ¥è¯¢ '{current_question[:50]}...' æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    # å½“æ²¡æœ‰æ–‡æ¡£æ—¶ï¼Œä»ç„¶ä½¿ç”¨å†å²è®°å½•ï¼Œä½†æ— ä¸Šä¸‹æ–‡
                    context = ""
                else:
                    # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯å¹¶å‘é€
                    references = self._format_references(docs, score_info)
                    yield json.dumps({
                        "type": "references",
                        "data": references
                    }) + "\n"
                    
                    # æ„å»ºä¸Šä¸‹æ–‡
                    context = "\n\n".join([
                        f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                        f"- æ¥æº: {Path(info['source']).name}\n"
                        f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                        for i, (doc, info) in enumerate(zip(docs, score_info))
                    ])
            except Exception as e:
                logger.error(f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
                # æ£€ç´¢å¤±è´¥æ—¶ä½¿ç”¨ç©ºä¸Šä¸‹æ–‡
                context = ""
                yield json.dumps({
                    "type": "error", 
                    "data": "âš ï¸ æ–‡æ¡£æ£€ç´¢æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œå°†ä½¿ç”¨èŠå¤©å†å²å›ç­”..."
                }) + "\n"
            
            # é˜¶æ®µ2ï¼šæ„å»ºå¤šè½®å¯¹è¯æç¤º
            prompt = self._build_chat_prompt(current_question, chat_history, context)
            
            # é˜¶æ®µ3ï¼šæµå¼ç”Ÿæˆ
            try:
                import time  # æ–°å¢æ—¶é—´æ¨¡å—å¯¼å…¥
                token_count = 0  # åˆå§‹åŒ–tokenè®¡æ•°å™¨
                start_time = time.time()  # è®°å½•ç”Ÿæˆå¼€å§‹æ—¶é—´
                last_chunk_time = start_time  # è®°å½•ä¸Šä¸€ä¸ªchunkæ—¶é—´
                for chunk in self.llm.stream(prompt):
                    current_time = time.time()
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # è®¡ç®—tokenæ•°é‡ï¼ˆå‡è®¾æœ‰tokenizerå±æ€§ï¼‰
                        chunk_tokens = len(self.llm.tokenizer.encode(
                            cleaned_chunk, 
                            add_special_tokens=False,
                            return_tensors=None
                        ))
                        token_count += chunk_tokens
                        
                        # è®¡ç®—é€Ÿåº¦æŒ‡æ ‡
                        elapsed_total = current_time - start_time
                        avg_speed = token_count / elapsed_total  # å¹³å‡é€Ÿåº¦
                        elapsed_chunk = current_time - last_chunk_time
                        instant_speed = chunk_tokens / elapsed_chunk  # ç¬æ—¶é€Ÿåº¦
                        
                        # æ‰“å°é€Ÿåº¦æ—¥å¿—ï¼ˆä¿ç•™2ä½å°æ•°ï¼‰
                        logger.info(
                            f"ğŸš„ Tokenç”Ÿæˆé€Ÿåº¦ | ä¼šè¯ID: {session_id} | "
                            f"å¹³å‡é€Ÿåº¦: {avg_speed:.2f}tok/s | "
                            f"ç¬æ—¶é€Ÿåº¦: {instant_speed:.2f}tok/s | "
                            f"ç´¯è®¡Token: {token_count}"
                        )
                        
                        last_chunk_time = current_time  # æ›´æ–°æœ€åchunkæ—¶é—´


                        # å‘é€ç”Ÿæˆå†…å®¹
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"
            except Exception as e:
                logger.error(f"æµå¼ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•"
                }) + "\n"
                
        except Exception as e:
            logger.exception(f"å¤šè½®å¯¹è¯å¤„ç†é”™è¯¯: {str(e)}")
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }) + "\n"
            
    def stream_query_model_with_history(self, session_id: str, current_question: str, 
                                 chat_history: List[Dict] = None) -> Generator[str, None, None]:
        """ç›´æ¥å¤§æ¨¡å‹çš„å¤šè½®å¯¹è¯æµå¼ç”Ÿæˆï¼ˆä¸ä½¿ç”¨çŸ¥è¯†åº“ï¼‰
        
        :param session_id: ä¼šè¯ID
        :param current_question: å½“å‰ç”¨æˆ·é—®é¢˜
        :param chat_history: èŠå¤©å†å²åˆ—è¡¨
        :return: ç”Ÿæˆå™¨ï¼Œæµå¼è¾“å‡ºç»“æœ
        """
        logger.info(f"ğŸ”„ ç›´æ¥å¤šè½®å¯¹è¯ | ä¼šè¯ID: {session_id} | é—®é¢˜: {current_question[:50]}...")
        
        if not current_question.strip():
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜"
            }) + "\n"
            return
        
        # åˆå§‹åŒ–èŠå¤©å†å²
        if chat_history is None:
            chat_history = []
        
        try:
            # æ„å»ºå¤šè½®å¯¹è¯æç¤ºï¼ˆæ— çŸ¥è¯†åº“ä¸Šä¸‹æ–‡ï¼‰
            prompt = self._build_chat_prompt(current_question, chat_history)
            
            # æµå¼ç”Ÿæˆ
            try:
                for chunk in self.llm.stream(prompt):
                    cleaned_chunk = chunk.replace("<|im_end|>", "")
                    if cleaned_chunk:
                        # å‘é€ç”Ÿæˆå†…å®¹
                        yield json.dumps({
                            "type": "content",
                            "data": cleaned_chunk
                        }) + "\n"
            except Exception as e:
                logger.error(f"ç›´æ¥å¤šè½®å¯¹è¯ç”Ÿæˆä¸­æ–­: {str(e)}")
                yield json.dumps({
                    "type": "error",
                    "data": "\nâš ï¸ ç”Ÿæˆè¿‡ç¨‹å‘ç”Ÿæ„å¤–ä¸­æ–­ï¼Œè¯·åˆ·æ–°é¡µé¢é‡è¯•"
                }) + "\n"
                
        except Exception as e:
            logger.exception(f"ç›´æ¥å¤šè½®å¯¹è¯å¤„ç†é”™è¯¯: {str(e)}")
            yield json.dumps({
                "type": "error",
                "data": "âš ï¸ ç³»ç»Ÿå¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿä¸¥é‡é”™è¯¯ï¼Œè¯·è”ç³»ç®¡ç†å‘˜"
            }) + "\n"

    def answer_query(self, question: str) -> Tuple[str, List[Dict], Dict]:
        """éæµå¼RAGç”Ÿæˆï¼Œé€‚ç”¨äºè¯„ä¼°æ¨¡å—
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            
        Returns:
            Tuple(ç”Ÿæˆçš„å›ç­”, æ£€ç´¢çš„æ–‡æ¡£åˆ—è¡¨, å…ƒæ•°æ®)
        """
        logger.info(f"ğŸ” éæµå¼å¤„ç†æŸ¥è¯¢(ç”¨äºè¯„ä¼°): {question[:50]}...")
        
        try:
            # é˜¶æ®µ1ï¼šæ–‡æ¡£æ£€ç´¢
            try:
                docs, score_info = self._retrieve_documents(question)
                if not docs:
                    logger.warning(f"è¯„ä¼°æŸ¥è¯¢ '{question[:50]}...' æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£")
                    return "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œæ— æ³•å›ç­”è¯¥é—®é¢˜ã€‚", [], {"status": "no_docs"}
            except Exception as e:
                logger.error(f"è¯„ä¼°æ¨¡å¼ä¸‹æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", exc_info=True)
                return f"æ–‡æ¡£æ£€ç´¢å¤±è´¥: {str(e)}", [], {"status": "retrieval_error", "error": str(e)}
            
            # æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£ä¿¡æ¯
            try:
                references = self._format_references(docs, score_info)
            except Exception as e:
                logger.error(f"æ ¼å¼åŒ–å‚è€ƒæ–‡æ¡£å¤±è´¥: {str(e)}")
                # åˆ›å»ºç®€åŒ–ç‰ˆå‚è€ƒä¿¡æ¯
                references = [{"file": f"æ–‡æ¡£{i+1}", "content": doc.page_content[:200] + "..."} 
                             for i, doc in enumerate(docs)]
            
            # é˜¶æ®µ2ï¼šæ„å»ºä¸Šä¸‹æ–‡
            try:
                context = "\n\n".join([
                    f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}\n"
                    f"- æ¥æº: {Path(info['source']).name}\n"
                    f"- ç»¼åˆç½®ä¿¡åº¦: {info['final_score'] * 100:.1f}%"
                    for i, (doc, info) in enumerate(zip(docs, score_info))
                ])
            except Exception as e:
                logger.error(f"æ„å»ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")
                # å¦‚æœæ„å»ºä¸Šä¸‹æ–‡å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆæœ¬
                context = "\n\n".join([f"ã€å‚è€ƒæ–‡æ¡£{i + 1}ã€‘{doc.page_content}" 
                                     for i, doc in enumerate(docs)])
            
            # é˜¶æ®µ3ï¼šæ„å»ºæç¤ºæ¨¡æ¿
            prompt = self._build_prompt(question, context)
            
            # é˜¶æ®µ4ï¼šä¸€æ¬¡æ€§ç”Ÿæˆï¼ˆéæµå¼ï¼‰
            try:
                answer = self.llm.invoke(prompt)
                cleaned_answer = answer.replace("<|im_end|>", "").strip()
                
                return cleaned_answer, references, {"status": "success"}
            except Exception as e:
                logger.error(f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}")
                # å°è¯•ä½¿ç”¨ç®€åŒ–æç¤º
                try:
                    simple_prompt = (
                        "<|im_start|>system\n"
                        "ä½ æ˜¯ä¸€ä½ç»éªŒä¸°å¯Œçš„åŒ–å·¥å®‰å…¨é¢†åŸŸä¸“å®¶ï¼Œè¯·å°½é‡å›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
                        "<|im_end|>\n"
                        "<|im_start|>user\n"
                        f"{question}\n"
                        "<|im_end|>\n"
                        "<|im_start|>assistant\n"
                    )
                    fallback_answer = self.llm.invoke(simple_prompt)
                    cleaned_fallback = fallback_answer.replace("<|im_end|>", "").strip()
                    return cleaned_fallback, references, {"status": "partial_success", "error": str(e)}
                except:
                    return f"ç”Ÿæˆå›ç­”å¤±è´¥: {str(e)}", references, {"status": "generation_error", "error": str(e)}
            
        except Exception as e:
            logger.exception(f"éæµå¼å¤„ç†ä¸¥é‡é”™è¯¯: {str(e)}")
            return f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}", [], {"status": "error", "error": str(e)}


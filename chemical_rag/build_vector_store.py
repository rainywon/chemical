import hashlib
import sys
import torch
from langchain.text_splitter import RecursiveCharacterTextSplitter  # å¯¼å…¥æ–‡æ¡£åˆ†å‰²å·¥å…·
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹
from langchain_community.vectorstores import FAISS  # å¯¼å…¥FAISSç”¨äºæ„å»ºå‘é‡æ•°æ®åº“
from langchain_community.document_loaders import UnstructuredPDFLoader  # æ–°å¢å¯¼å…¥
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
import os
import json
from pathlib import Path  # å¯¼å…¥Pathï¼Œç”¨äºè·¯å¾„å¤„ç†
from datetime import datetime  # å¯¼å…¥datetimeï¼Œç”¨äºè®°å½•æ—¶é—´æˆ³
from typing import List, Dict, Optional, Set, Tuple  # å¯¼å…¥ç±»å‹æç¤º
import logging  # å¯¼å…¥æ—¥å¿—æ¨¡å—ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¥å¿—
from concurrent.futures import ThreadPoolExecutor, as_completed  # å¯¼å…¥çº¿ç¨‹æ± æ¨¡å—ï¼Œæ”¯æŒå¹¶è¡ŒåŠ è½½PDFæ–‡ä»¶
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡æ¨¡å—ï¼Œç”¨äºæ˜¾ç¤ºåŠ è½½è¿›åº¦
from config import Config  # å¯¼å…¥é…ç½®ç±»ï¼Œç”¨äºåŠ è½½é…ç½®å‚æ•°
import shutil  # ç”¨äºæ–‡ä»¶æ“ä½œ
import pandas as pd  # å¯¼å…¥pandasç”¨äºåˆ›å»ºExcelæ–‡ä»¶
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—ï¼Œç”¨äºå¤„ç†æ–‡æœ¬

from pdf_cor_extractor.pdf_ocr_extractor import PDFProcessor


# é…ç½®æ—¥å¿—æ ¼å¼
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒæŒ‡å®šè¾“å‡ºåˆ°stdout
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)],  # æ˜ç¡®è¾“å‡ºåˆ°stdout
    force=True  # å…³é”®ï¼šå¼ºåˆ¶è¦†ç›–ç°æœ‰é…ç½®
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨
        Args:
            config (Config): é…ç½®ç±»ï¼ŒåŒ…å«å¿…è¦çš„é…ç½®
        """
        self.config = config
        


        
        # è®¾ç½®å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_dir = Path(config.vector_db_path)
        self.vector_backup_dir = self.vector_dir / "backups"
        
        # å°†æºæ–‡ä»¶ç›®å½•å®šä¹‰æ”¾åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­
        self.subfolders = ['emergency_document']  # 'æ ‡å‡†æ€§æ–‡ä»¶','æ³•å¾‹', 'è§„èŒƒæ€§æ–‡ä»¶'
        
        # æ£€æŸ¥æ–‡ä»¶åŒ¹é…æ¨¡å¼
        if not hasattr(config, 'files') or not config.files:
            # å¦‚æœconfigä¸­æ²¡æœ‰fileså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.config.files = ["data/**/*.pdf", "data/**/*.txt", "data/**/*.md", "data/**/*.docx"]
        
        # æ·»åŠ GPUä½¿ç”¨é…ç½®
        self.use_gpu_for_ocr = "cuda" in self.config.device
        
        # å·²å¤„ç†æ–‡ä»¶çŠ¶æ€
        self.failed_files_count = 0
        
        # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹
        self.print_detailed_chunks = getattr(config, 'print_detailed_chunks', False)
        # è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        self.max_chunk_preview_length = getattr(config, 'max_chunk_preview_length', 200)
        
        # è®¾ç½®ç¼“å­˜ç›®å½•ï¼ˆå·²æ³¨é‡Šï¼‰
        # self.cache_dir = Path(config.cache_dir)
        
        logger.info("åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨...")

    def _is_non_content_page(self, page_content: str, page_num: int) -> bool:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦ä¸ºéå†…å®¹é¡µé¢ï¼Œå¦‚å°é¢ã€ç›®å½•ã€ç›®æ¬¡ã€å‰è¨€ç­‰ï¼Œè¿™äº›é¡µé¢åœ¨åˆ†å—æ—¶åº”å½“è¢«è¿‡æ»¤æ‰
        
        Args:
            page_content: é¡µé¢æ–‡æœ¬å†…å®¹
            page_num: é¡µé¢ç¼–å·
            
        Returns:
            bool: å¦‚æœæ˜¯éå†…å®¹é¡µé¢è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µï¼Œå¾ˆå¯èƒ½æ˜¯å°é¢
        if page_num == 0 or page_num == 1:
            # å°é¢é¡µé€šå¸¸å¾ˆçŸ­ï¼Œæˆ–è€…åªåŒ…å«æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
            if len(page_content.strip()) < 200:
                return True
            
            # å°é¢é¡µé€šå¸¸åŒ…å«è¿™äº›å…³é”®è¯
            cover_keywords = ['å°é¢', 'ç‰ˆæƒ', 'ç‰ˆæƒæ‰€æœ‰', 'å‘å¸ƒ', 'ç¼–å†™', 'ç¼–è‘—', 
                             'è‘—ä½œæƒ', 'ä¿ç•™æ‰€æœ‰æƒåˆ©', 'ç‰ˆæƒå£°æ˜', 'ä¿®è®¢ç‰ˆ']
            for keyword in cover_keywords:
                if keyword in page_content:
                    return True
        
        # æ£€æµ‹ç›®å½•ã€ç›®æ¬¡é¡µé¢
        toc_keywords = ['ç›®å½•', 'ç›® å½•', 'ç›®æ¬¡', 'ç›® æ¬¡', 'ç« èŠ‚', 'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç¬¬ä¸‰ç« ', 'é™„å½•']
        
        # å¦‚æœé¡µé¢ä¸­åŒ…å«å¤šä¸ªç›®å½•å…³é”®è¯ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        keyword_count = sum(1 for keyword in toc_keywords if keyword in page_content)
        if keyword_count >= 1:
            return True
        
        # æ£€æµ‹å‰è¨€é¡µé¢
        preface_keywords = ['å‰è¨€', 'å‰ è¨€', 'åºè¨€', 'åº è¨€', 'å¼•è¨€', 'å¼• è¨€', 'ç»ªè®º']
        for keyword in preface_keywords:
            # å¦‚æœå‰è¨€å…³é”®è¯å‡ºç°åœ¨é¡µé¢å¼€å¤´éƒ¨åˆ†ï¼Œå¾ˆå¯èƒ½æ˜¯å‰è¨€é¡µ
            if keyword in page_content[:200] or f"\n{keyword}\n" in page_content:
                logger.info(f"æ£€æµ‹åˆ°å‰è¨€é¡µï¼Œå…³é”®è¯: {keyword}")
                return True
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å…¸å‹çš„ç›®å½•ç»“æ„ï¼ˆè¡Œé¦–æ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œè¡Œå°¾æ˜¯é¡µç ï¼‰
        lines = page_content.split('\n')
        pattern_count = 0
        for line in lines:
            line = line.strip()
            # åŒ¹é…ç±»ä¼¼ "ç¬¬Xç«  å†…å®¹..........10" çš„æ¨¡å¼
            if line and (line[0] == 'ç¬¬' or line.startswith('é™„å½•')) and line.strip()[-1].isdigit():
                pattern_count += 1
                
        # å¦‚æœæœ‰å¤šè¡Œç¬¦åˆç›®å½•ç‰¹å¾ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        if pattern_count >= 3:
            return True
        
        return False

    def _load_single_document(self, file_path: Path) -> Optional[List[Document]]:
        """å¤šçº¿ç¨‹åŠ è½½å•ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€DOCXã€DOCï¼‰"""
        try:
            file_extension = file_path.suffix.lower()
            docs = []

            if file_extension == ".pdf":
                try:
                    # æ£€æŸ¥PDFé¡µæ•°
                    import fitz
                    with fitz.open(str(file_path)) as doc:
                        page_count = doc.page_count
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFæ–‡ä»¶ '{file_path.name}' å…±æœ‰ {page_count} é¡µ")
                        
                    # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°åˆå§‹åŒ–å¤„ç†å™¨
                    processor = PDFProcessor(
                        file_path=str(file_path), 
                        lang='ch', 
                        use_gpu=self.use_gpu_for_ocr
                    )
                    
                    # æ ¹æ®é¡µæ•°é€‰æ‹©åˆé€‚çš„GPUå‚æ•°é…ç½®
                    if page_count > 30:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFé¡µæ•°è¾ƒå¤š({page_count}é¡µ)ï¼Œåº”ç”¨å¤§æ–‡æ¡£ä¼˜åŒ–é…ç½®")
                        processor.configure_gpu(**self.config.pdf_ocr_large_doc_params)
                    else:
                        # ä½¿ç”¨æ ‡å‡†å‚æ•°é…ç½®
                        processor.configure_gpu(**self.config.pdf_ocr_params)
                    
                    # å¤„ç†PDF
                    docs = processor.process()
                    
                    # è¿‡æ»¤æ‰éå†…å®¹é¡µé¢ï¼ˆå°é¢ã€ç›®å½•ã€å‰è¨€ç­‰ï¼‰
                    if docs:
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} é¡µï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}é¡µ{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} é¡µéå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœ
                    if docs and len(docs) < page_count * 0.5:
                        logger.warning(f"[æ–‡æ¡£åŠ è½½] è­¦å‘Š: åªè¯†åˆ«å‡º {len(docs)}/{page_count} é¡µï¼Œä½äº50%ï¼Œå¯èƒ½æœ‰é—®é¢˜")
                    elif docs:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸè¯†åˆ« {len(docs)}/{page_count} é¡µ")
                    
                    # å¤„ç†åæ¸…ç†å†…å­˜
                    import gc
                    gc.collect()
                    try:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except:
                        pass
                        
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†PDFæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    self.failed_files_count += 1
                    return None
                    
            elif file_extension in [".docx", ".doc"]:
                try:
                    # é¦–å…ˆå°è¯•å¯¼å…¥ä¾èµ–æ¨¡å—
                    try:
                        import docx2txt
                    except ImportError:
                        logger.error(f"ç¼ºå°‘å¤„ç†Wordæ–‡æ¡£æ‰€éœ€çš„ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install docx2txt")
                        # è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œï¼Œä»¥ä¾¿å¤„ç†å…¶ä»–æ–‡ä»¶ç±»å‹
                        self.failed_files_count += 1
                        return None
                        
                    from langchain_community.document_loaders import Docx2txtLoader
                    loader = Docx2txtLoader(str(file_path))
                    docs = loader.load()
                    
                    # å°è¯•è¿‡æ»¤Wordæ–‡æ¡£çš„éå†…å®¹é¡µé¢
                    if docs and len(docs) > 1:  # å¦‚æœWordæ–‡æ¡£è¢«åˆ†æˆäº†å¤šä¸ªé¡µé¢
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†DOCXæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    
                    # å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•
                    try:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£...")
                        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
                        loader = UnstructuredWordDocumentLoader(str(file_path))
                        docs = loader.load()
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£: {file_path.name}")
                        
                        # ä¹Ÿå°è¯•è¿‡æ»¤éå†…å®¹é¡µé¢
                        if docs and len(docs) > 1:
                            filtered_docs = []
                            filtered_count = 0
                            filtered_types = []
                            
                            for i, doc in enumerate(docs):
                                if not self._is_non_content_page(doc.page_content, i):
                                    filtered_docs.append(doc)
                                else:
                                    filtered_count += 1
                                    # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                    page_type = "éå†…å®¹é¡µé¢"
                                    if i == 0 or i == 1:
                                        page_type = "å°é¢"
                                    elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                        page_type = "ç›®å½•/ç›®æ¬¡"
                                    elif "å‰è¨€" in doc.page_content:
                                        page_type = "å‰è¨€"
                                        
                                    filtered_types.append(page_type)
                                    logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                    
                            if filtered_count > 0:
                                # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                                type_summary = {}
                                for t in filtered_types:
                                    if t not in type_summary:
                                        type_summary[t] = 0
                                    type_summary[t] += 1
                                    
                                type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                                logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                                docs = filtered_docs
                    except Exception as e2:
                        logger.error(f"[æ–‡æ¡£åŠ è½½] æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                        self.failed_files_count += 1
                        return None
            else:
                logger.warning(f"[æ–‡æ¡£åŠ è½½] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.name}")
                return None

            if docs:
                # ç»Ÿä¸€æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata["source"] = str(file_path)
                    doc.metadata["file_name"] = file_path.name
                return docs
            return None

        except Exception as e:
            logger.error(f"[æ–‡æ¡£åŠ è½½] åŠ è½½ {file_path} å¤±è´¥: {str(e)}")
            self.failed_files_count += 1
            return None

    def load_documents(self) -> List:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        logger.info("âŒ› å¼€å§‹åŠ è½½æ–‡æ¡£...")

        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        document_files = []
        for subfolder in self.subfolders:
            folder_path = self.config.data_dir / subfolder
            if folder_path.exists() and folder_path.is_dir():
                document_files.extend([f for f in folder_path.rglob("*") 
                                    if f.suffix.lower() in ['.pdf', '.docx', '.doc']])
            else:
                logger.warning(f"å­æ–‡ä»¶å¤¹ {subfolder} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {folder_path}")
                
        # è¿‡æ»¤å¹¶æ’åºæ–‡ä»¶ï¼ˆå…ˆå¤„ç†è¾ƒå°çš„æ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨æ˜¾å­˜ï¼‰
        document_files = sorted(document_files, key=lambda x: x.stat().st_size)
        logger.info(f"å‘ç° {len(document_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        results = []
        # é™åˆ¶çº¿ç¨‹æ± å¤§å°ä»¥é¿å…èµ„æºäº‰ç”¨
        with ThreadPoolExecutor(max_workers=1) as executor:
            futures = [executor.submit(self._load_single_document, file) for file in document_files]
            with tqdm(total=len(futures), desc="åŠ è½½æ–‡æ¡£", unit="files") as pbar:
                for future in as_completed(futures):
                    res = future.result()
                    if res:
                        results.extend(res)
                        pbar.update(1)
                        pbar.set_postfix_str(f"å·²åŠ è½½ {len(res)} é¡µ")
                    else:
                        pbar.update(1)
        
        # åœ¨å¤„ç†å®Œæˆåæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(results)} é¡µæ–‡æ¡£")
        logger.info(f"âŒ æœªæˆåŠŸåŠ è½½ {self.failed_files_count} ä¸ªæ–‡ä»¶")
        
        return results

    def process_files(self) -> List:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†æµç¨‹ï¼Œä½¿ç”¨ç« èŠ‚åˆ†å—æ–¹æ³•"""
        logger.info("å¼€å§‹æ–‡ä»¶å¤„ç†æµç¨‹")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        all_docs = self.load_documents()

        if not all_docs:
            logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶å†…å®¹")
            return []

        # é¦–å…ˆæŒ‰æ–‡ä»¶åˆå¹¶é¡µé¢å†…å®¹ï¼Œé¿å…è·¨é¡µåˆ†å—æ–­è£‚
        logger.info("åˆå¹¶æ–‡ä»¶é¡µé¢å†…å®¹ï¼Œå‡†å¤‡è¿›è¡Œæ•´ä½“åˆ†å—...")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ•´ç†æ–‡æ¡£
        file_docs = {}
        for doc in all_docs:
            source = doc.metadata.get("source", "")
            if source not in file_docs:
                file_docs[source] = []
            file_docs[source].append(doc)
        
        # å¯¹æ¯ä¸ªæ–‡ä»¶çš„é¡µé¢è¿›è¡Œæ’åºå’Œåˆå¹¶
        whole_docs = []
        for source, docs in file_docs.items():
            # æŒ‰é¡µç æ’åº
            sorted_docs = sorted(docs, key=lambda x: x.metadata.get("page", 0))
            
            # åˆå¹¶æ–‡ä»¶æ‰€æœ‰é¡µé¢çš„å†…å®¹
            full_content = "\n".join([doc.page_content for doc in sorted_docs])
            
            # åˆ›å»ºå®Œæ•´æ–‡æ¡£å¯¹è±¡
            file_doc = Document(
                page_content=full_content,
                metadata={
                    "source": source,
                    "file_name": sorted_docs[0].metadata.get("file_name", ""),
                    "page_count": len(sorted_docs),
                    "is_merged_doc": True  # æ ‡è®°ä¸ºåˆå¹¶åçš„å®Œæ•´æ–‡æ¡£
                }
            )
            whole_docs.append(file_doc)
            
        logger.info(f"å·²å°† {len(all_docs)} é¡µå†…å®¹åˆå¹¶ä¸º {len(whole_docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
        
        # ä½¿ç”¨æŒ‰ç« èŠ‚åˆ†å—æ–¹æ³•
        chunks = []
        
        with tqdm(total=len(whole_docs), desc="å¤„ç†æ–‡æ¡£ç« èŠ‚åˆ†å—") as pbar:
            for doc in whole_docs:
                metadata = doc.metadata.copy()
                # ç§»é™¤åˆ†å—åä¸å†é€‚ç”¨çš„å…ƒæ•°æ®
                if "is_merged_doc" in metadata:
                    del metadata["is_merged_doc"]
                
                # æŒ‰ç« èŠ‚åˆ†å—
                sections = self._split_by_section(doc.page_content)
                logger.info(f"æ­£åœ¨å¤„ç†{doc.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}")
                logger.info(f"sections: {len(sections)}")
                # å¦‚æœæ‰¾åˆ°ç« èŠ‚ï¼Œåˆ™ä½¿ç”¨ç« èŠ‚åˆ†å—
                if sections:
                    logger.info(f"æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ç« èŠ‚ç»“æ„è¿›è¡Œåˆ†å—")
                    for i, (title, content, section_meta) in enumerate(sections):
                        if not content.strip():  # è·³è¿‡ç©ºç« èŠ‚
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        
                        # åˆå¹¶å…ƒæ•°æ®
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata.update(section_meta)  # æ·»åŠ ç« èŠ‚å…ƒæ•°æ®
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(sections)
                        enhanced_metadata["chunk_type"] = "section"
                        
                        chunks.append(Document(
                            page_content=content,
                            metadata=enhanced_metadata
                        ))
                else:
                    # å¦‚æœæœªæ‰¾åˆ°ç« èŠ‚ç»“æ„ï¼Œåˆ™ç›´æ¥ä½¿ç”¨é€’å½’åˆ†å—
                    logger.warning(f"æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨é€’å½’åˆ†å—æ–¹æ³•")
                    
                    # é€’å½’æ–‡æœ¬åˆ†å‰²é…ç½®
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=self.config.chunk_size,
                        chunk_overlap=self.config.chunk_overlap,
                        separators=[
                            "\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""
                        ],
                        length_function=len,
                        add_start_index=True,
                        is_separator_regex=False
                    )
                    
                    # å¯¹å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
                    split_texts = text_splitter.split_text(doc.page_content)
                    
                    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                    for i, text in enumerate(split_texts):
                        if not text.strip():  # è·³è¿‡ç©ºæ–‡æœ¬å—
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(text.encode()).hexdigest()
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(split_texts)
                        enhanced_metadata["chunk_type"] = "fixed_size"
                        
                        chunks.append(Document(
                            page_content=text,
                            metadata=enhanced_metadata
                        ))
                
                pbar.update(1)
        
        # ç›´æ¥ä½¿ç”¨ç”Ÿæˆçš„æ–‡æœ¬å—ï¼Œä¸è¿›è¡Œåå¤„ç†
        logger.info(f"ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ‰“å°åˆ†å—ç»“æœæ¦‚è§ˆ
        self._print_chunks_summary(chunks)
        
        # # ä¿å­˜åˆ†å—åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹
        # self.save_chunks_to_file(chunks)

        # ä¿å­˜åˆ†å—åˆ°Excelæ–‡ä»¶
        self.save_chunks_to_excel(chunks)

        return chunks

    def _split_by_section(self, text: str) -> List[Tuple[str, str, Dict]]:
        """
        æ ¹æ®ç« èŠ‚æ ‡é¢˜å°†æ–‡æœ¬åˆ†å‰²æˆæœ‰ç»„ç»‡çš„æ®µè½
        
        Args:
            text: å®Œæ•´çš„æ–‡æ¡£æ–‡æœ¬
                
        Returns:
            List[Tuple[str, str, Dict]]: è¿”å›ç« èŠ‚æ ‡é¢˜ã€ç« èŠ‚å†…å®¹å’Œå…ƒæ•°æ®çš„å…ƒç»„åˆ—è¡¨
        """
        logger.info("å¼€å§‹æŒ‰ç« èŠ‚ç»“æ„è¿›è¡Œæ–‡æ¡£åˆ†å—...")
        import re
        
        # è¯†åˆ«å„ç§æ ‡é¢˜æ ¼å¼çš„æ­£åˆ™è¡¨è¾¾å¼
        patterns = [
            # æ ‡å‡†æ ¼å¼ï¼ˆä¸€çº§åˆ°å››çº§æ ‡é¢˜ï¼‰
            r'^\s*(\d+)\.?\s+([^\n]+)$',                  # "1. æ ‡é¢˜"
            r'^\s*(\d+\.\d+)\.?\s+([^\n]+)$',             # "1.1 æ ‡é¢˜"
            r'^\s*(\d+\.\d+\.\d+)\.?\s+([^\n]+)$',        # "1.1.1 æ ‡é¢˜"
            r'^\s*(\d+\.\d+\.\d+\.\d+)\.?\s+([^\n]+)$',   # "1.1.1.1 æ ‡é¢˜"
            # ä¸­æ–‡åºå·
            r'^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€.ï¼]\s+([^\n]+)$',     # "ä¸€ã€æ ‡é¢˜"
            r'^\s*[ï¼ˆ(]([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[)ï¼‰]\s+([^\n]+)$',  # "ï¼ˆä¸€ï¼‰æ ‡é¢˜"
            # æ— ç©ºæ ¼æ ¼å¼
            r'^\s*(\d+)\.([\S].*?)$',                     # "1.æ ‡é¢˜"
            r'^\s*(\d+\.\d+)([\S].*?)$',                  # "1.1æ ‡é¢˜"
            # é™„å½•æ ¼å¼
            r'^\s*(é™„å½•\s*[A-Za-z])[.ï¼ã€]?\s*([^\n]+)?$',  # "é™„å½•A æ ‡é¢˜"
            # äº‹æ•…æŠ¥å‘Šç‰¹æœ‰æ ¼å¼
            r'^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{2})\s*[ã€,:ï¼š.ï¼]\s*(.+)$',
            r'^\s*(\d+)\s*[ã€,:ï¼š.ï¼]\s*(.+)$',
            r'^(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?)[:ï¼š]?\s*(.*)$'
        ]
        
        # åˆå§‹åŒ–
        lines = text.split('\n')
        sections = []
        
        # ç”¨äºäº‹æ•…æŠ¥å‘Šç‰¹å¾æ£€æµ‹
        date_time_pattern = re.compile(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥(?:\s*[ä¸Šä¸‹]åˆ)?\s*\d{1,2}[æ—¶:](?:\d{1,2}åˆ†?)?)')
        accident_report_features = 0
        
        # æŒ‰ç…§ç« èŠ‚å±‚çº§ç»„ç»‡å†…å®¹ - æ›´ç®€å•çš„æ–¹æ³•
        all_sections = []  # å­˜å‚¨æ‰€æœ‰ç« èŠ‚ï¼ŒåŒ…æ‹¬ä¸€çº§å’Œå­ç« èŠ‚
        current_section = {"title": "", "content": [], "level": 0, "children": []}
        
        # é€è¡Œå¤„ç†æ–‡æœ¬
        line_num = 0
        while line_num < len(lines):
            line = lines[line_num].strip()
            line_num += 1
            
            # æ£€æµ‹äº‹æ•…æŠ¥å‘Šç‰¹å¾ï¼ˆæ—¥æœŸæ—¶é—´æ ¼å¼ï¼‰
            if date_time_pattern.search(line):
                accident_report_features += 1
            
            # åŒ¹é…æ ‡é¢˜
            is_heading = False
            heading_level = 0
            heading_num = ""
            heading_title = ""
            
            for i, pattern in enumerate(patterns):
                match = re.match(pattern, line)
                if match:
                    is_heading = True
                    
                    # æ ¹æ®æ¨¡å¼ç¡®å®šæ ‡é¢˜çº§åˆ«
                    if i < 4:  # æ ‡å‡†æ•°å­—æ ¼å¼ (1., 1.1., etc)
                        heading_level = i + 1
                    elif i < 6:  # ä¸­æ–‡åºå· (ä¸€ã€, (ä¸€))
                        heading_level = 1
                    elif i < 8:  # æ— ç©ºæ ¼æ ¼å¼ (1.æ ‡é¢˜, 1.1æ ‡é¢˜)
                        heading_level = 1 if "." not in match.group(1) else 2
                    elif i == 8:  # é™„å½•æ ¼å¼
                        heading_level = 1
                    elif i == 9:  # ä¸­æ–‡æ•°å­—æ ‡é¢˜ (ä¸€ã€äºŒã€ä¸‰)
                        heading_level = 1
                    elif i == 10:  # æ•°å­—åºå·æ ‡é¢˜ (1ã€2ã€3)
                        # æ£€æŸ¥æ˜¯å¦æ˜¯å¸¸è§çš„"1ã€2ã€3"è¿™æ ·çš„äº‹æ•…æŠ¥å‘Šå­é¡¹ç¼–å·
                        if re.match(r'^\s*[1-9](\d*)\s*[ã€,:ï¼š.ï¼]', line):
                            # æŸ¥çœ‹å‰é¢çš„æ ‡é¢˜æ˜¯å¦æœ‰ä¸­æ–‡ç¼–å·(ä¸€ã€äºŒã€ä¸‰ç­‰)æˆ–ç½—é©¬æ•°å­—
                            chinese_or_roman_header = False
                            for prev_section in all_sections:
                                prev_num = prev_section.get("num", "")
                                if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+$', prev_num) or \
                                   re.match(r'^[IVX]+$', prev_num):
                                    chinese_or_roman_header = True
                                    break
                            
                            if chinese_or_roman_header:
                                heading_level = 2  # å¦‚æœå‰é¢æœ‰ä¸­æ–‡æ ‡é¢˜ï¼Œè¿™é€šå¸¸æ˜¯äºŒçº§ç¼–å·
                            else:
                                heading_level = 1  # å¦åˆ™å¯èƒ½æ˜¯ä¸»è¦ç¼–å·
                        else:
                            heading_level = 1
                    else:  # å…¶ä»–äº‹æ•…æŠ¥å‘Šæ ¼å¼ (æ—¥æœŸç­‰)
                        heading_level = 2
                    
                    # å¢åŠ äº‹æ•…æŠ¥å‘Šç‰¹å¾è®¡æ•°(å¦‚æœæ˜¯äº‹æ•…æŠ¥å‘Šæ ¼å¼)
                    if i >= 9:
                        accident_report_features += 1
                    
                    heading_num = match.group(1)
                    heading_title = match.group(2) if len(match.groups()) > 1 and match.group(2) else heading_num
                    break
            
            # å¤„ç†æ ‡é¢˜å’Œå†…å®¹
            if is_heading:
                section_title = f"{heading_num} {heading_title}"
                
                # ä¿å­˜å½“å‰ç« èŠ‚
                if current_section["title"]:
                    content = "\n".join(current_section["content"])
                    metadata = {
                        "section_num": current_section.get("num", ""),
                        "section_title": current_section.get("text", ""),
                        "section_level": current_section["level"],
                        "section_type": "accident_report" if accident_report_features > 2 else "standard"
                    }
                    
                    # ä¿å­˜å½“å‰ç« èŠ‚
                    finalized_section = {
                        "title": current_section["title"],
                        "content": content,
                        "metadata": metadata,
                        "level": current_section["level"],
                        "num": current_section.get("num", ""),
                        "children": current_section.get("children", [])
                    }
                    all_sections.append(finalized_section)
                
                # åˆ›å»ºæ–°ç« èŠ‚
                current_section = {
                    "title": section_title,
                    "num": heading_num,
                    "text": heading_title,
                    "content": [line],  # åŒ…å«æ ‡é¢˜è¡Œ
                    "level": heading_level,
                    "children": []
                }
            else:
                # æ·»åŠ åˆ°å½“å‰ç« èŠ‚å†…å®¹
                if current_section["content"] or line:  # é¿å…æ·»åŠ ç©ºè¡Œåˆ°ç©ºç« èŠ‚
                    current_section["content"].append(line)
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current_section["title"]:
            content = "\n".join(current_section["content"])
            metadata = {
                "section_num": current_section.get("num", ""),
                "section_title": current_section.get("text", ""),
                "section_level": current_section["level"],
                "section_type": "accident_report" if accident_report_features > 2 else "standard"
            }
            finalized_section = {
                "title": current_section["title"],
                "content": content,
                "metadata": metadata,
                "level": current_section["level"],
                "num": current_section.get("num", ""),
                "children": []
            }
            all_sections.append(finalized_section)
        
        # æ„å»ºç« èŠ‚å±‚çº§å…³ç³» - ä»¥æ›´ç®€å•ã€æ›´å¯é çš„æ–¹å¼
        # æŒ‰é¡ºåºå¤„ç†ï¼Œå°†å­ç« èŠ‚å…³è”åˆ°æœ€è¿‘çš„ä¸»ç« èŠ‚
        section_hierarchy = []
        current_main = None
        
        for section in all_sections:
            if section["level"] == 1:
                # å¦‚æœå­˜åœ¨ä¹‹å‰çš„ä¸»ç« èŠ‚ï¼Œæ·»åŠ åˆ°ç»“æœ
                if current_main:
                    section_hierarchy.append(current_main)
                
                # åˆ›å»ºæ–°çš„ä¸»ç« èŠ‚
                current_main = section
                current_main["children"] = []
            elif section["level"] > 1 and current_main:
                # æ·»åŠ å­ç« èŠ‚åˆ°å½“å‰ä¸»ç« èŠ‚
                current_main["children"].append(section)
            else:
                # æ²¡æœ‰å…³è”çš„ä¸»ç« èŠ‚ï¼Œç›´æ¥æ·»åŠ 
                section_hierarchy.append(section)
        
        # æ·»åŠ æœ€åä¸€ä¸ªä¸»ç« èŠ‚
        if current_main and current_main not in section_hierarchy:
            section_hierarchy.append(current_main)
        
        # æœ€ç»ˆå¤„ç†ï¼šåˆå¹¶ä¸»ç« èŠ‚å’Œå­ç« èŠ‚çš„å†…å®¹ï¼Œç”Ÿæˆç»“æœ
        for section in section_hierarchy:
            if section.get("children") and len(section["children"]) > 0:
                # åˆå¹¶ä¸»ç« èŠ‚å’Œæ‰€æœ‰å­ç« èŠ‚å†…å®¹
                full_content = section["content"] + "\n\n"
                
                for child in section["children"]:
                    child_content = child.get("content", "")
                    if child_content:
                        full_content += child_content + "\n\n"
                
                # æ›´æ–°å…ƒæ•°æ®
                section["metadata"]["contains_subsections"] = True
                section["metadata"]["subsection_count"] = len(section["children"])
                
                # æ·»åŠ åˆ°æœ€ç»ˆç»“æœ
                sections.append((section["title"], full_content.strip(), section["metadata"]))
            else:
                # æ²¡æœ‰å­ç« èŠ‚çš„ç« èŠ‚ç›´æ¥æ·»åŠ 
                sections.append((section["title"], section["content"], section["metadata"]))
        
        # ç‰¹æ®Šæƒ…å†µå¤„ç†ï¼šæ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ç»“æ„çš„æ–‡æ¡£
        if not sections and text.strip():
            # å¦‚æœæ£€æµ‹åˆ°äº‹æ•…æŠ¥å‘Šç‰¹å¾ï¼Œä½¿ç”¨æ®µè½åˆ†å‰²ä½†åˆå¹¶æˆæ›´å¤§çš„å—
            if accident_report_features > 1:
                logger.info("æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼Œä½†å‘ç°äº‹æ•…æŠ¥å‘Šç‰¹å¾ï¼Œä½¿ç”¨æ®µè½åˆ†å‰²å¹¶åˆå¹¶ç›¸å…³æ®µè½...")
                paragraphs = []
                current_para = []
                
                # æŒ‰ç©ºè¡Œåˆ†å‰²æ®µè½
                for line in lines:
                    line = line.strip()
                    if line:
                        current_para.append(line)
                    elif current_para:  # ç©ºè¡Œä¸”å½“å‰æ®µè½æœ‰å†…å®¹
                        paragraphs.append("\n".join(current_para))
                        current_para = []
                
                # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
                if current_para:
                    paragraphs.append("\n".join(current_para))
                
                # æ¯3ä¸ªæ®µè½åˆå¹¶ä¸ºä¸€ç»„ï¼Œé¿å…è¿‡åº¦åˆ†å‰²
                merged_paragraphs = []
                for i in range(0, len(paragraphs), 3):
                    group = paragraphs[i:i+3]
                    merged_paragraphs.append("\n\n".join(group))
                
                # å°†åˆå¹¶åçš„æ®µè½ç»„è½¬æ¢ä¸ºç« èŠ‚
                for i, paragraph in enumerate(merged_paragraphs):
                    if len(paragraph) > 30:  # åªå¤„ç†è¾ƒé•¿çš„æ®µè½
                        # å°è¯•ä»æ®µè½ä¸­æå–æ ‡é¢˜
                        first_line = paragraph.split("\n")[0] if "\n" in paragraph else ""
                        title = first_line[:50] if len(first_line) > 10 else f"æ®µè½ç»„{i+1}"
                        
                        metadata = {
                            "section_num": f"PG{i+1}",
                            "section_title": title,
                            "section_level": 1,
                            "section_type": "accident_report_paragraph_group"
                        }
                        sections.append((f"PG{i+1} {title}", paragraph, metadata))
            else:
                # æ™®é€šæ–‡æ¡£ï¼šæ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç« èŠ‚
                logger.info("æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼Œå°†æ•´ä¸ªæ–‡æ¡£ä½œä¸ºä¸€ä¸ªç« èŠ‚...")
                first_line = text.strip().split('\n')[0][:50]
                metadata = {
                    "section_level": 0, 
                    "section_title": first_line,
                    "section_type": "no_section"
                }
                sections.append((first_line, text, metadata))
        
        # æ‰“å°è¯¦ç»†ä¿¡æ¯ä¾¿äºè°ƒè¯•
        if not sections:
            logger.warning("æœªèƒ½è¯†åˆ«ä»»ä½•ç« èŠ‚ï¼æ£€æŸ¥æ–‡æœ¬ç»“æ„æˆ–æ ‡é¢˜æ ¼å¼...")
        else:
            logger.info(f"æŒ‰ç« èŠ‚ç»“æ„åˆ†å—å®Œæˆï¼Œå…±æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚å—")
            
            # æ‰“å°ä¸»è¦ç« èŠ‚å’Œå­ç« èŠ‚å…³ç³»
            for i, (title, content, meta) in enumerate(sections):
                if meta.get("contains_subsections"):
                    logger.info(f"  â€¢ ç« èŠ‚ {i+1}: {title} (åŒ…å« {meta.get('subsection_count', 0)} ä¸ªå­ç« èŠ‚)")
                else:
                    logger.info(f"  â€¢ ç« èŠ‚ {i+1}: {title}")
        
        return sections

    def _ensure_complete_sentences(self, text: str) -> str:
        """ç¡®ä¿æ–‡æœ¬å—ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        
        Args:
            text: åŸå§‹æ–‡æœ¬å—
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å—ï¼Œç¡®ä¿ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        """
        if not text or len(text) < 10:  # æ–‡æœ¬è¿‡çŸ­åˆ™ç›´æ¥è¿”å›
            return text
            
        # ä¸­æ–‡å¥å­ç»“æŸæ ‡è®°
        sentence_end_marks = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n']
        # å¥å­å¼€å§‹çš„å¯èƒ½æ ‡è®°ï¼ˆä¸­æ–‡æ®µè½å¼€å¤´ã€ç« èŠ‚æ ‡é¢˜ç­‰ï¼‰
        sentence_start_patterns = ['\n', 'ç¬¬.{1,3}ç« ', 'ç¬¬.{1,3}èŠ‚']
        
        # å¤„ç†æ–‡æœ¬å—å¼€å¤´
        text_stripped = text.lstrip()
        # å¦‚æœä¸æ˜¯ä»¥å¥æœ«æ ‡ç‚¹å¼€å¤´ï¼Œä¹Ÿä¸æ˜¯ä»¥å¤§å†™å­—æ¯æˆ–æ•°å­—å¼€å¤´ï¼ˆå¯èƒ½æ˜¯æ–°æ®µè½ï¼‰ï¼Œåˆ™å¯èƒ½æ˜¯ä¸å®Œæ•´å¥å­
        is_incomplete_start = True
        
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­æˆ–æ®µè½å¼€å§‹çš„æ ‡è®°
        for pattern in sentence_start_patterns:
            if text.startswith(pattern) or text_stripped[0].isupper() or text_stripped[0].isdigit():
                is_incomplete_start = False
                break
        
        if is_incomplete_start:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­çš„å¼€å§‹
            for mark in sentence_end_marks:
                pos = text.find(mark)
                if pos > 0:
                    # æ‰¾åˆ°å¥æœ«æ ‡è®°åçš„å†…å®¹ä½œä¸ºèµ·ç‚¹
                    try:
                        # ç¡®ä¿å¥æœ«æ ‡è®°åè¿˜æœ‰å†…å®¹
                        if pos + 1 < len(text):
                            text = text[pos+1:].lstrip()
                            break
                    except:
                        # å‡ºé”™åˆ™ä¿æŒåŸæ ·
                        pass
        
        # å¤„ç†æ–‡æœ¬å—ç»“å°¾
        is_incomplete_end = True
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­ç»“æŸ
        for mark in sentence_end_marks:
            if text.endswith(mark):
                is_incomplete_end = False
                break
        
        if is_incomplete_end:
            # æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­çš„ç»“æŸä½ç½®
            last_pos = -1
            for mark in sentence_end_marks:
                pos = text.rfind(mark)
                if pos > last_pos:
                    last_pos = pos
                    
            if last_pos > 0:
                # æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­ç»“æŸ
                text = text[:last_pos+1]
        
        return text.strip()
    
    def _post_process_chunks(self, chunks: List[Document]) -> List[Document]:
        """å¯¹åˆ†å—åçš„æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œä¼˜åŒ–å—çš„è´¨é‡
        
        Args:
            chunks: åŸå§‹åˆ†å—åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            return []
            
        logger.info("å¯¹æ–‡æœ¬å—è¿›è¡Œåå¤„ç†ä¼˜åŒ–...")
        processed_chunks = []
        
        # æŒ‰æ–‡æ¡£æºåˆ†ç»„å¤„ç†
        doc_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            if source not in doc_chunks:
                doc_chunks[source] = []
            doc_chunks[source].append(chunk)
        
        total_merged = 0
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„å—
        for source, source_chunks in doc_chunks.items():
            # æŒ‰å—ç´¢å¼•æ’åº
            sorted_chunks = sorted(source_chunks, 
                                   key=lambda x: x.metadata.get("chunk_index", 0))
            
            # æ£€æŸ¥å’Œå¤„ç†ç›¸é‚»å—
            for i, chunk in enumerate(sorted_chunks):
                # åªå¯¹ç« èŠ‚ç±»å‹çš„å—åº”ç”¨å®Œæ•´å¥å­å¤„ç†
                if chunk.metadata.get("chunk_type") == "section":
                    chunk.page_content = self._ensure_complete_sentences(chunk.page_content)
                
                # è·³è¿‡ç©ºå—
                if not chunk.page_content.strip():
                    continue
            
                processed_chunks.append(chunk)
        
        logger.info(f"åå¤„ç†å®Œæˆï¼Œä¼˜åŒ–åçš„å—æ•°: {len(processed_chunks)}")
        return processed_chunks
        

    def _print_chunks_summary(self, chunks: List[Document]):
        """æ‰“å°æ–‡æœ¬åˆ†å—ç»“æœæ¦‚è§ˆ"""
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›æ˜¾ç¤º")
            return
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(chunks)
        avg_chunk_length = sum(len(chunk.page_content) for chunk in chunks) / total_chunks
        files_count = len(set(chunk.metadata.get("source", "") for chunk in chunks))
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š æ–‡æœ¬åˆ†å—å¤„ç†æ¦‚è§ˆ")
        logger.info("="*50)
        logger.info(f"ğŸ“„ æ€»å—æ•°: {total_chunks}")
        logger.info(f"ğŸ“Š å¹³å‡å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦")
        logger.info(f"ğŸ“‚ æ¶‰åŠæ–‡ä»¶æ•°: {files_count}")
        
        # æ–‡ä»¶çº§ç»Ÿè®¡
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        logger.info("\nğŸ“‚ æ–‡ä»¶çº§åˆ†å—ç»Ÿè®¡:")
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"  â€¢ {file_name}: {len(file_chunks_list)} å—")
        logger.info("="*50)

    def _print_detailed_chunks(self, chunks: List[Document]):
        """è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“‘ è¯¦ç»†æ–‡æœ¬å—å†…å®¹")
        logger.info("="*50)
        
        # å°†åˆ†å—æŒ‰æ–‡ä»¶åˆ†ç»„
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # ä¸ºäº†æ›´æœ‰ç»„ç»‡åœ°è¾“å‡ºï¼Œå…ˆæŒ‰æ–‡ä»¶è¾“å‡º
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"\nğŸ“„ æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)")
            
            # è¾“å‡ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªå—
            for i, chunk in enumerate(file_chunks_list[:3]):
                page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                chunk_size = len(chunk.page_content)
                
                # è·å–é¢„è§ˆå†…å®¹
                content_preview = chunk.page_content
                if len(content_preview) > self.max_chunk_preview_length:
                    content_preview = content_preview[:self.max_chunk_preview_length] + "..."
                
                # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿äºæ§åˆ¶å°æ˜¾ç¤º
                content_preview = content_preview.replace("\n", "\\n")
                
                logger.info(f"\n  å— {i+1}/{len(file_chunks_list[:3])} [ç¬¬{page_num}é¡µ, {chunk_size}å­—ç¬¦]:")
                logger.info(f"  {content_preview}")
            
            # å¦‚æœæ–‡ä»¶ä¸­çš„å—æ•°è¶…è¿‡3ä¸ªï¼Œæ˜¾ç¤ºçœç•¥ä¿¡æ¯
            if len(file_chunks_list) > 3:
                logger.info(f"  ... è¿˜æœ‰ {len(file_chunks_list) - 3} ä¸ªå—æœªæ˜¾ç¤º ...")
                
        # è¾“å‡ºä¿å­˜å®Œæ•´åˆ†å—å†…å®¹çš„æç¤º
        chunks_detail_file = self.cache_dir / "chunks_detail.txt"
        try:
            with open(chunks_detail_file, "w", encoding="utf-8") as f:
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                    
                    f.write(f"=== å— {i+1}/{len(chunks)} [{file_name} - ç¬¬{page_num}é¡µ] ===\n")
                    f.write(chunk.page_content)
                    f.write("\n\n")
            
            logger.info(f"\nâœ… æ‰€æœ‰æ–‡æœ¬å—çš„è¯¦ç»†å†…å®¹å·²ä¿å­˜è‡³: {chunks_detail_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        logger.info("="*50)

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹"""
        logger.info("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name=self.config.embedding_model_path,  # åµŒå…¥æ¨¡å‹çš„è·¯å¾„
            model_kwargs={"device": self.config.device},  # è®¾ç½®è®¾å¤‡ä¸ºCPUæˆ–GPU
            encode_kwargs={
                "batch_size": self.config.batch_size,  # æ‰¹å¤„ç†å¤§å°
                "normalize_embeddings": self.config.normalize_embeddings  # æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥
            },
        )

    def backup_vector_db(self):
        """å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“"""
        vector_db_path = Path(self.config.vector_db_path)
        if not vector_db_path.exists():
            return False
            
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = vector_db_path.parent / f"{vector_db_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            for item in vector_db_path.glob('*'):
                if item.is_file():
                    shutil.copy2(item, backup_dir)
                elif item.is_dir():
                    shutil.copytree(item, backup_dir / item.name)
                    
            logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²å¤‡ä»½è‡³ {backup_dir}")
            return True
        except Exception as e:
            logger.error(f"å¤‡ä»½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“"""
        logger.info("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")

        # åˆ›å»ºå¿…è¦ç›®å½•
        Path(self.config.vector_db_path).mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡æ¡£
        chunks = self.process_files()  # å¤„ç†æ–‡æ¡£å¹¶åˆ†å—
        
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥å¤„ç†ï¼Œè·³è¿‡å‘é‡å­˜å‚¨æ„å»º")
            return

        # ç”ŸæˆåµŒå…¥æ¨¡å‹
        embeddings = self.create_embeddings()

        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ç°æœ‰å‘é‡æ•°æ®åº“
        vector_db_path = Path(self.config.vector_db_path)
        if vector_db_path.exists() and any(vector_db_path.glob('*')):
            try:
                # å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“
                self.backup_vector_db()
                
                # åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
                logger.info("åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“...")
                existing_vector_store = FAISS.load_local(
                    str(vector_db_path),
                    embeddings,
                    allow_dangerous_deserialization=True,
                    distance_strategy=DistanceStrategy.COSINE
                )
                
                # å¢é‡æ›´æ–°
                logger.info("è¿›è¡Œå¢é‡æ›´æ–°...")
                existing_vector_store.add_documents(chunks)
                
                # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
                existing_vector_store.save_local(str(vector_db_path))
                logger.info(f"å‘é‡æ•°æ®åº“å·²æ›´æ–°å¹¶ä¿å­˜è‡³ {vector_db_path}")
                return
                
            except Exception as e:
                logger.error(f"å¢é‡æ›´æ–°å¤±è´¥: {str(e)}")
                logger.info("å°†åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“...")
        
        # å¦‚æœå‘é‡æ•°æ®åº“ä¸å­˜åœ¨æˆ–å¢é‡æ›´æ–°å¤±è´¥ï¼Œåˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
        logger.info("ç”Ÿæˆå‘é‡...")
        vector_store = FAISS.from_documents(
            chunks,
            embeddings,
            distance_strategy=DistanceStrategy.COSINE
        )

        # ä¿å­˜å‘é‡æ•°æ®åº“
        vector_store.save_local(str(vector_db_path))
        logger.info(f"æ–°çš„å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ {vector_db_path}")

    def save_chunks_to_file(self, chunks: List[Document]):
        """å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œä½†ä¸ä½œä¸ºç¼“å­˜å­˜å‚¨
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›ä¿å­˜")
            return
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œæ–¹ä¾¿ç›´æ¥æŸ¥çœ‹
        text_file = self.cache_dir / "chunks_text.txt"
        try:
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(f"æ–‡æ¡£åˆ†å—æ€»è§ˆ\n")
                f.write(f"=============\n")
                f.write(f"æ€»å—æ•°: {len(chunks)}\n")
                f.write(f"æ¶‰åŠæ–‡ä»¶æ•°: {len(set(chunk.metadata.get('source', '') for chunk in chunks))}\n\n")
                
                # æŒ‰æ–‡ä»¶åˆ†ç»„è¾“å‡º
                file_chunks = {}
                for chunk in chunks:
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    if source not in file_chunks:
                        file_chunks[source] = []
                    file_chunks[source].append(chunk)
                
                for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
                    file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
                    f.write(f"\n{'='*80}\n")
                    f.write(f"æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)\n")
                    f.write(f"{'='*80}\n\n")
                    
                    for i, chunk in enumerate(file_chunks_list):
                        # è·å–ç« èŠ‚ä¿¡æ¯
                        section_num = chunk.metadata.get("section_num", "")
                        section_title = chunk.metadata.get("section_title", "")
                        chunk_index = chunk.metadata.get("chunk_index", i)
                        total_chunks = chunk.metadata.get("total_chunks", len(file_chunks_list))
                        position = chunk.metadata.get("position", "")
                        chunk_type = chunk.metadata.get("chunk_type", "")
                        
                        # æ„å»ºå—æ ‡é¢˜
                        header = f"----- å— {chunk_index+1}/{total_chunks} "
                        if section_num and section_title:
                            header += f"[ç« èŠ‚: {section_num} {section_title}, "
                        header += f"ä½ç½®:{position}, {len(chunk.page_content)}å­—ç¬¦"
                        if chunk_type:
                            header += f", ç±»å‹:{chunk_type}"
                        header += "] -----\n"
                        
                        # å†™å…¥å—ä¿¡æ¯
                        f.write(header)
                        f.write(chunk.page_content)
                        f.write("\n\n")
            
            logger.info(f"âœ… æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å·²ä¿å­˜è‡³: {text_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®
        json_file = self.cache_dir / "chunks_detail.json"
        try:
            chunks_data = []
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "index": i,
                    "content": chunk.page_content,
                    "length": len(chunk.page_content),
                    "metadata": chunk.metadata
                }
                chunks_data.append(chunk_data)
                
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump({
                    "total_chunks": len(chunks),
                    "timestamp": datetime.now().isoformat(),
                    "chunks": chunks_data
                }, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜è‡³: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # ä¿å­˜CSVæ ¼å¼çš„æ‘˜è¦ä¿¡æ¯ï¼Œæ–¹ä¾¿å¯¼å…¥ç”µå­è¡¨æ ¼æŸ¥çœ‹
        csv_file = self.cache_dir / "chunks_summary.csv"
        try:
            with open(csv_file, "w", encoding="utf-8") as f:
                # å†™å…¥CSVå¤´
                f.write("ç´¢å¼•,æ–‡ä»¶å,ç« èŠ‚ç¼–å·,ç« èŠ‚æ ‡é¢˜,å—ç´¢å¼•,æ€»å—æ•°,ä½ç½®,å­—ç¬¦æ•°,å†…å®¹é¢„è§ˆ\n")
                
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    section_num = chunk.metadata.get("section_num", "")
                    section_title = chunk.metadata.get("section_title", "")
                    chunk_index = chunk.metadata.get("chunk_index", i)
                    total_chunks = chunk.metadata.get("total_chunks", 0)
                    position = chunk.metadata.get("position", "")
                    length = len(chunk.page_content)
                    
                    # å†…å®¹é¢„è§ˆï¼Œå»é™¤æ¢è¡Œç¬¦
                    preview = chunk.page_content[:100].replace("\n", " ").replace("\r", " ")
                    if len(chunk.page_content) > 100:
                        preview += "..."
                    preview = f'"{preview}"'  # ç”¨å¼•å·åŒ…å›´ï¼Œé¿å…CSVè§£æé”™è¯¯
                    
                    f.write(f"{i},{file_name},{section_num},{section_title},{chunk_index},{total_chunks},{position},{length},{preview}\n")
                
            logger.info(f"âœ… CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å·²ä¿å­˜è‡³: {csv_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å¤±è´¥: {str(e)}")
            
        # ä¿å­˜åˆ°Excelæ–‡ä»¶
        self.save_chunks_to_excel(chunks)

    def save_chunks_to_excel(self, chunks: List[Document]):
        """å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°Excelæ–‡ä»¶ä¸­
        
        æ¯ä¸ªæºæ–‡ä»¶ç”Ÿæˆä¸€ä¸ªExcelæ–‡ä»¶ï¼ŒåŒ…å«åŸæ–‡å†…å®¹å’Œå…¥åº“å†…å®¹ä¸¤åˆ—
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›ä¿å­˜åˆ°Excel")
            return
            
        # åˆ›å»ºè¾“å‡ºç›®å½•
        output_dir = Path(self.config.knowledge_base_path)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"å¼€å§‹å°†æ–‡æœ¬å—ä¿å­˜åˆ°Excelæ–‡ä»¶ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ–‡æœ¬å—
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„åˆ†å—
        for file_path, file_chunks_list in file_chunks.items():
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            excel_file = output_dir / f"{file_name}.xlsx"
            
            # åˆ›å»ºDataFrameå­˜å‚¨æ•°æ®
            data = []
            for chunk in file_chunks_list:
                # è·å–åŸå§‹æ–‡æœ¬
                raw_text = chunk.page_content
                
                # ä½¿ç”¨ä¼˜åŒ–åçš„ç« èŠ‚ç¼–å·ç§»é™¤ç®—æ³•
                cleaned_text = self._remove_chapter_numbering(raw_text)
                
                data.append({
                    "åŸæ–‡å†…å®¹": cleaned_text.strip(),
                    "å…¥åº“å†…å®¹": raw_text
                })
            
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            
            # ä¿å­˜åˆ°Excel
            try:
                df.to_excel(excel_file, index=False, engine='openpyxl')
                logger.info(f"âœ… å·²å°† {file_name} çš„ {len(data)} ä¸ªæ–‡æœ¬å—ä¿å­˜åˆ°: {excel_file}")
            except Exception as e:
                logger.error(f"âŒ ä¿å­˜ {file_name} çš„Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
        
        total_files = len(file_chunks)
        logger.info(f"âœ… å®Œæˆä¿å­˜ {total_files} ä¸ªæ–‡ä»¶çš„æ–‡æœ¬å—åˆ°Excelæ–‡ä»¶")

    def _remove_chapter_numbering(self, text):
        """ç§»é™¤æ–‡æœ¬å¼€å¤´çš„ç« èŠ‚ç¼–å·
        
        Args:
            text: åŸå§‹æ–‡æœ¬
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬ï¼Œç« èŠ‚ç¼–å·è¢«ç§»é™¤
        """
        if not text or len(text.strip()) < 2:
            return text
        
        # è·å–å‰12ä¸ªå­—ç¬¦ï¼Œå»é™¤ç©ºæ ¼ååˆ¤æ–­æ˜¯å¦å­˜åœ¨ç« èŠ‚ç¼–å·
        prefix = text[:12].replace(' ', '')
        
        # æ£€æŸ¥å‰ç¼€æ˜¯å¦åŒ…å«å…¸å‹çš„ç« èŠ‚ç¼–å·æ ¼å¼
        section_pattern = None
        
        # å¤šç§ç« èŠ‚ç¼–å·æ¨¡å¼
        patterns = [
            # æ ‡å‡†æ•°å­—æ ¼å¼
            r'^\.?\d+\.?\d+\.?\d+\.?\d+',  # å››çº§æ ‡é¢˜ 6.3.2.1
            r'^\.?\d+\.?\d+\.?\d+',         # ä¸‰çº§æ ‡é¢˜ 6.3.2
            r'^\.?\d+\.?\d+',               # äºŒçº§æ ‡é¢˜ 6.3
            r'^\.+\d+',                     # ç‚¹å·å¼€å¤´çš„æ•°å­— .2
            # ä¸­æ–‡åºå·
            r'^[ï¼ˆ(ï¼ˆ][ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[)ï¼‰ï¼‰]',  # ï¼ˆä¸€ï¼‰
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€.ï¼]',       # ä¸€ã€
        ]
        
        # å°è¯•åŒ¹é…å„ç§æ¨¡å¼
        for pattern in patterns:
            match = re.match(pattern, prefix)
            if match:
                section_pattern = match.group(0)
                break
        
        # å¦‚æœæ‰¾åˆ°ç« èŠ‚ç¼–å·ï¼Œåˆ™ç§»é™¤
        if section_pattern:
            # ç›´æ¥æˆªå–ç« èŠ‚ç¼–å·åçš„éƒ¨åˆ†
            if len(section_pattern) < len(text):
                return text[len(section_pattern):].lstrip()
        
        # å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç« èŠ‚ç¼–å·ï¼Œè¿”å›åŸæ–‡æœ¬
        return text

    def process_single_file(self, file_path: str) -> bool:
        """å¤„ç†å•ä¸ªæ–‡ä»¶å¹¶æ›´æ–°å‘é‡æ•°æ®åº“
        
        ç”¨äºå¢é‡æ›´æ–°å‘é‡æ•°æ®åº“ï¼Œå½“ä¸Šä¼ æ–°æ–‡ä»¶æ—¶ä½¿ç”¨
        
        Args:
            file_path: æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
            
        Returns:
            bool: å¤„ç†æ˜¯å¦æˆåŠŸ
        """
        try:
            logger.info(f"å¼€å§‹å¤„ç†æ–‡ä»¶ {file_path} å¹¶å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“")
            
            # è½¬æ¢ä¸ºPathå¯¹è±¡
            file_path_obj = Path(file_path)
            
            # ç¡®è®¤æ–‡ä»¶å­˜åœ¨
            if not file_path_obj.exists():
                logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                return False
                
            # ç¡®è®¤æ–‡ä»¶æ ¼å¼å—æ”¯æŒ
            if file_path_obj.suffix.lower() not in ['.pdf', '.docx', '.doc', '.xlsx', '.xls']:
                logger.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path_obj.suffix}")
                return False
                
            # åŠ è½½å•ä¸ªæ–‡æ¡£
            docs = self._load_single_document(file_path_obj)
            
            if not docs:
                logger.warning(f"æ–‡ä»¶ {file_path_obj.name} æ— æ³•åŠ è½½æˆ–æ²¡æœ‰å†…å®¹")
                return False
                
            logger.info(f"æˆåŠŸåŠ è½½æ–‡ä»¶ {file_path_obj.name}ï¼Œå…± {len(docs)} é¡µå†…å®¹")
            
            # åˆå¹¶æ–‡ä»¶çš„æ‰€æœ‰é¡µé¢
            full_content = "\n".join([doc.page_content for doc in docs])
            
            # åˆ›å»ºå®Œæ•´æ–‡æ¡£å¯¹è±¡
            whole_doc = Document(
                page_content=full_content,
                metadata={
                    "source": str(file_path_obj),
                    "file_name": file_path_obj.name,
                    "page_count": len(docs),
                    "is_merged_doc": True
                }
            )
            
            # æŒ‰ç« èŠ‚åˆ†å—
            chunks = []
            metadata = whole_doc.metadata.copy()
            
            # ç§»é™¤åˆ†å—åä¸å†é€‚ç”¨çš„å…ƒæ•°æ®
            if "is_merged_doc" in metadata:
                del metadata["is_merged_doc"]
            
            # æŒ‰ç« èŠ‚åˆ†å—
            sections = self._split_by_section(whole_doc.page_content)
            logger.info(f"æ–‡ä»¶ {file_path_obj.name} å…±æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚")
            
            # æŒ‰ç« èŠ‚å¤„ç†
            if sections:
                # ä½¿ç”¨ç« èŠ‚ç»“æ„åˆ†å—
                for i, (title, content, section_meta) in enumerate(sections):
                    if not content.strip():  # è·³è¿‡ç©ºç« èŠ‚
                        continue
                        
                    # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                    content_hash = hashlib.md5(content.encode()).hexdigest()
                    
                    # åˆå¹¶å…ƒæ•°æ®
                    enhanced_metadata = metadata.copy()
                    enhanced_metadata.update(section_meta)  # æ·»åŠ ç« èŠ‚å…ƒæ•°æ®
                    enhanced_metadata["content_hash"] = content_hash
                    enhanced_metadata["chunk_index"] = i
                    enhanced_metadata["total_chunks"] = len(sections)
                    enhanced_metadata["chunk_type"] = "section"
                    
                    chunks.append(Document(
                        page_content=content,
                        metadata=enhanced_metadata
                    ))
            else:
                # ä½¿ç”¨é€’å½’åˆ†å—
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=self.config.chunk_size,
                    chunk_overlap=self.config.chunk_overlap,
                    separators=["\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""],
                    length_function=len,
                    add_start_index=True,
                    is_separator_regex=False
                )
                
                # å¯¹å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
                split_texts = text_splitter.split_text(whole_doc.page_content)
                
                # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                for i, text in enumerate(split_texts):
                    if not text.strip():  # è·³è¿‡ç©ºæ–‡æœ¬å—
                        continue
                        
                    # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                    content_hash = hashlib.md5(text.encode()).hexdigest()
                    enhanced_metadata = metadata.copy()
                    enhanced_metadata["content_hash"] = content_hash
                    enhanced_metadata["chunk_index"] = i
                    enhanced_metadata["total_chunks"] = len(split_texts)
                    enhanced_metadata["chunk_type"] = "fixed_size"
                    
                    chunks.append(Document(
                        page_content=text,
                        metadata=enhanced_metadata
                    ))
            
            if not chunks:
                logger.warning(f"æ–‡ä»¶ {file_path_obj.name} æœªç”Ÿæˆä»»ä½•æ–‡æœ¬å—")
                return False
                
            logger.info(f"æ–‡ä»¶ {file_path_obj.name} ç”Ÿæˆäº† {len(chunks)} ä¸ªæ–‡æœ¬å—")
            self.save_chunks_to_excel(chunks)
            # æ£€æŸ¥å‘é‡æ•°æ®åº“æ˜¯å¦å­˜åœ¨
            vector_db_path = Path(self.config.vector_db_path)
            
            # åˆ›å»ºåµŒå…¥æ¨¡å‹
            embeddings = self.create_embeddings()
            
            # å¢é‡æ›´æ–°å‘é‡æ•°æ®åº“
            if vector_db_path.exists() and any(vector_db_path.glob("*")):
                try:
                    # åŠ è½½ç°æœ‰å‘é‡æ•°æ®åº“
                    vector_store = FAISS.load_local(
                        str(vector_db_path),
                        embeddings,
                        allow_dangerous_deserialization=True,
                        distance_strategy=DistanceStrategy.COSINE
                    )
                    
                    # ä¸ºæ–°æ–‡ä»¶åˆ›å»ºå‘é‡
                    logger.info(f"ä¸ºæ–‡ä»¶ {file_path_obj.name} ç”Ÿæˆå‘é‡å¹¶æ›´æ–°æ•°æ®åº“")
                    vector_store.add_documents(chunks)
                    
                    # ä¿å­˜æ›´æ–°åçš„å‘é‡æ•°æ®åº“
                    vector_store.save_local(str(vector_db_path))
                    logger.info(f"æˆåŠŸæ›´æ–°å‘é‡æ•°æ®åº“ï¼Œæ–°å¢ {len(chunks)} ä¸ªæ–‡æœ¬å—")
                    
                    return True
                except Exception as e:
                        logger.error(f"å‘é‡æ•°æ®åº“å¢é‡æ›´æ–°å¤±è´¥: {str(e)}")
                        # å¦‚æœå¢é‡æ›´æ–°å¤±è´¥ï¼Œè®°å½•é”™è¯¯ä½†ä»ç„¶å°è¯•é‡å»ºæ•´ä¸ªæ•°æ®åº“
                        logger.warning("å°è¯•é‡å»ºæ•´ä¸ªå‘é‡æ•°æ®åº“...")
            
            # å¦‚æœå‘é‡æ•°æ®åº“ä¸å­˜åœ¨æˆ–å¢é‡æ›´æ–°å¤±è´¥ï¼Œä»å½“å‰æ–‡ä»¶åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“
            try:
                # åˆ›å»ºå¿…è¦çš„ç›®å½•
                vector_db_path.mkdir(parents=True, exist_ok=True)
                
                # æ„å»ºå‘é‡å­˜å‚¨
                logger.info(f"ä»æ–‡ä»¶ {file_path_obj.name} åˆ›å»ºæ–°çš„å‘é‡æ•°æ®åº“")
                vector_store = FAISS.from_documents(
                    chunks,
                    embeddings,
                    distance_strategy=DistanceStrategy.COSINE
                )
                
                # ä¿å­˜å‘é‡æ•°æ®åº“
                vector_store.save_local(str(vector_db_path))
                logger.info(f"æˆåŠŸåˆ›å»ºå‘é‡æ•°æ®åº“ï¼ŒåŒ…å« {len(chunks)} ä¸ªæ–‡æœ¬å—")
                
                return True
            except Exception as e:
                logger.error(f"åˆ›å»ºå‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
                return False
                
        except Exception as e:
            logger.error(f"å¤„ç†æ–‡ä»¶ {file_path} æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            return False


if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        
        # æ·»åŠ : è§£æå‘½ä»¤è¡Œå‚æ•°ï¼Œå…è®¸ç”¨æˆ·æŒ‡å®šæ˜¯å¦æ‰“å°è¯¦ç»†åˆ†å—å†…å®¹
        import argparse
        parser = argparse.ArgumentParser(description='æ„å»ºåŒ–å·¥å®‰å…¨é¢†åŸŸå‘é‡æ•°æ®åº“')
        parser.add_argument('--detailed-chunks', action='store_true', 
                           help='æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹')
        parser.add_argument('--max-preview', type=int, default=510,
                           help='è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°')
        args = parser.parse_args()
        
        # æ›´æ–°é…ç½®
        if args.detailed_chunks:
            config.print_detailed_chunks = True
            config.max_chunk_preview_length = args.max_preview
            print(f"å°†è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ï¼Œæ¯å—æœ€å¤šæ˜¾ç¤º {args.max_preview} å­—ç¬¦")

        # æ„å»ºå‘é‡æ•°æ®åº“
        builder = VectorDBBuilder(config)
        builder.build_vector_store()

    except Exception as e:
        logger.exception("ç¨‹åºè¿è¡Œå‡ºé”™")  # è®°å½•ç¨‹åºå¼‚å¸¸
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ")  # ç¨‹åºç»“æŸæ—¥å¿—

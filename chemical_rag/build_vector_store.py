import hashlib
import sys
import torch
import pandas as pd  # å¯¼å…¥pandasï¼Œç”¨äºæ“ä½œExcelæ–‡ä»¶
import re  # å¯¼å…¥æ­£åˆ™è¡¨è¾¾å¼æ¨¡å—ï¼Œç”¨äºå¤„ç†æ–‡æœ¬
from langchain.text_splitter import RecursiveCharacterTextSplitter  # å¯¼å…¥æ–‡æ¡£åˆ†å‰²å·¥å…·
from langchain_community.vectorstores.utils import DistanceStrategy
from langchain_core.documents import Document
from langchain_huggingface import HuggingFaceEmbeddings  # å¯¼å…¥HuggingFaceåµŒå…¥æ¨¡å‹
from langchain_community.vectorstores import FAISS  # å¯¼å…¥FAISSç”¨äºæ„å»ºå‘é‡æ•°æ®åº“
from langchain_community.document_loaders import UnstructuredPDFLoader  # æ–°å¢å¯¼å…¥
from langchain_community.document_loaders import UnstructuredWordDocumentLoader
import os  # å¯¼å…¥osæ¨¡å—ï¼Œç”¨äºè·å–å·¥ä½œç›®å½•
import json
from pathlib import Path  # å¯¼å…¥Pathï¼Œç”¨äºè·¯å¾„å¤„ç†
from datetime import datetime  # å¯¼å…¥datetimeï¼Œç”¨äºè®°å½•æ—¶é—´æˆ³
from typing import List, Dict, Optional, Set, Tuple  # å¯¼å…¥ç±»å‹æç¤º
import logging  # å¯¼å…¥æ—¥å¿—æ¨¡å—ï¼Œç”¨äºè®°å½•è¿è¡Œæ—¥å¿—
from concurrent.futures import ThreadPoolExecutor, as_completed  # å¯¼å…¥çº¿ç¨‹æ± æ¨¡å—ï¼Œæ”¯æŒå¹¶è¡ŒåŠ è½½PDFæ–‡ä»¶
from tqdm import tqdm  # å¯¼å…¥è¿›åº¦æ¡æ¨¡å—ï¼Œç”¨äºæ˜¾ç¤ºåŠ è½½è¿›åº¦
from config import Config  # å¯¼å…¥é…ç½®ç±»ï¼Œç”¨äºåŠ è½½é…ç½®å‚æ•°
import shutil  # ç”¨äºæ–‡ä»¶æ“ä½œ

from pdf_cor_extractor.pdf_ocr_extractor import PDFProcessor


# é…ç½®æ—¥å¿—æ ¼å¼
# é…ç½®æ—¥å¿—æ ¼å¼ï¼ŒæŒ‡å®šè¾“å‡ºåˆ°stdout
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[logging.StreamHandler(stream=sys.stdout)],  # æ˜ç¡®è¾“å‡ºåˆ°stdout
    force=True  # å…³é”®ï¼šå¼ºåˆ¶è¦†ç›–ç°æœ‰é…ç½®
)
logger = logging.getLogger(__name__)


class VectorDBBuilder:
    def __init__(self, config: Config):
        """
        åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨
        Args:
            config (Config): é…ç½®ç±»ï¼ŒåŒ…å«å¿…è¦çš„é…ç½®
        """
        self.config = config
        
        # è®¾ç½®ç¼“å­˜ç›®å½•è·¯å¾„ï¼ˆä¿ç•™ç”¨äºå­˜å‚¨åˆ†å—åˆ†æç»“æœï¼‰
        self.cache_dir = Path(config.cache_dir)
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # è®¾ç½®å‘é‡æ•°æ®åº“è·¯å¾„
        self.vector_dir = Path(config.vector_db_path)
        self.vector_backup_dir = self.vector_dir / "backups"
        
        # å°†æºæ–‡ä»¶ç›®å½•å®šä¹‰æ”¾åœ¨åˆå§‹åŒ–æ–¹æ³•ä¸­
        self.subfolders = ['æ ‡å‡†']  # 'æ ‡å‡†æ€§æ–‡ä»¶','æ³•å¾‹', 'è§„èŒƒæ€§æ–‡ä»¶'
        
        # æ£€æŸ¥æ–‡ä»¶åŒ¹é…æ¨¡å¼
        if not hasattr(config, 'files') or not config.files:
            # å¦‚æœconfigä¸­æ²¡æœ‰fileså‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            self.config.files = ["data/**/*.pdf", "data/**/*.txt", "data/**/*.md", "data/**/*.docx"]
        
        # æ·»åŠ GPUä½¿ç”¨é…ç½®
        self.use_gpu_for_ocr = "cuda" in self.config.device
        
        # å·²å¤„ç†æ–‡ä»¶çŠ¶æ€
        self.failed_files_count = 0
        
        # æ˜¯å¦è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹
        self.print_detailed_chunks = getattr(config, 'print_detailed_chunks', False)
        # è¯¦ç»†è¾“å‡ºæ—¶æ¯ä¸ªæ–‡æœ¬å—æ˜¾ç¤ºçš„æœ€å¤§å­—ç¬¦æ•°
        self.max_chunk_preview_length = getattr(config, 'max_chunk_preview_length', 200)
        
        logger.info("åˆå§‹åŒ–å‘é‡æ•°æ®åº“æ„å»ºå™¨...")

    def _is_non_content_page(self, page_content: str, page_num: int) -> bool:
        """
        æ£€æµ‹é¡µé¢æ˜¯å¦ä¸ºéå†…å®¹é¡µé¢ï¼Œå¦‚å°é¢ã€ç›®å½•ã€ç›®æ¬¡ã€å‰è¨€ç­‰ï¼Œè¿™äº›é¡µé¢åœ¨åˆ†å—æ—¶åº”å½“è¢«è¿‡æ»¤æ‰
        
        Args:
            page_content: é¡µé¢æ–‡æœ¬å†…å®¹
            page_num: é¡µé¢ç¼–å·
            
        Returns:
            bool: å¦‚æœæ˜¯éå†…å®¹é¡µé¢è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        # å¦‚æœæ˜¯ç¬¬ä¸€é¡µï¼Œå¾ˆå¯èƒ½æ˜¯å°é¢
        if page_num == 0 or page_num == 1:
            # å°é¢é¡µé€šå¸¸å¾ˆçŸ­ï¼Œæˆ–è€…åªåŒ…å«æ ‡é¢˜ã€ä½œè€…ç­‰ä¿¡æ¯
            if len(page_content.strip()) < 200:
                return True
            
            # å°é¢é¡µé€šå¸¸åŒ…å«è¿™äº›å…³é”®è¯
            cover_keywords = ['å°é¢', 'ç‰ˆæƒ', 'ç‰ˆæƒæ‰€æœ‰', 'å‘å¸ƒ', 'ç¼–å†™', 'ç¼–è‘—', 
                             'è‘—ä½œæƒ', 'ä¿ç•™æ‰€æœ‰æƒåˆ©', 'ç‰ˆæƒå£°æ˜', 'ä¿®è®¢ç‰ˆ']
            for keyword in cover_keywords:
                if keyword in page_content:
                    return True
        
        # æ£€æµ‹ç›®å½•ã€ç›®æ¬¡é¡µé¢
        toc_keywords = ['ç›®å½•', 'ç›® å½•', 'ç›®æ¬¡', 'ç›® æ¬¡', 'ç« èŠ‚', 'ç¬¬ä¸€ç« ', 'ç¬¬äºŒç« ', 'ç¬¬ä¸‰ç« ', 'é™„å½•']
        
        # å¦‚æœé¡µé¢ä¸­åŒ…å«å¤šä¸ªç›®å½•å…³é”®è¯ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        keyword_count = sum(1 for keyword in toc_keywords if keyword in page_content)
        if keyword_count >= 1:
            return True
        
        # æ£€æµ‹å‰è¨€é¡µé¢
        preface_keywords = ['å‰è¨€', 'å‰ è¨€', 'åºè¨€', 'åº è¨€', 'å¼•è¨€', 'å¼• è¨€', 'ç»ªè®º']
        for keyword in preface_keywords:
            # å¦‚æœå‰è¨€å…³é”®è¯å‡ºç°åœ¨é¡µé¢å¼€å¤´éƒ¨åˆ†ï¼Œå¾ˆå¯èƒ½æ˜¯å‰è¨€é¡µ
            if keyword in page_content[:200] or f"\n{keyword}\n" in page_content:
                logger.info(f"æ£€æµ‹åˆ°å‰è¨€é¡µï¼Œå…³é”®è¯: {keyword}")
                return True
        
        # æ£€æŸ¥é¡µé¢æ˜¯å¦æœ‰å…¸å‹çš„ç›®å½•ç»“æ„ï¼ˆè¡Œé¦–æ˜¯ç« èŠ‚æ ‡é¢˜ï¼Œè¡Œå°¾æ˜¯é¡µç ï¼‰
        lines = page_content.split('\n')
        pattern_count = 0
        for line in lines:
            line = line.strip()
            # åŒ¹é…ç±»ä¼¼ "ç¬¬Xç«  å†…å®¹..........10" çš„æ¨¡å¼
            if line and (line[0] == 'ç¬¬' or line.startswith('é™„å½•')) and line.strip()[-1].isdigit():
                pattern_count += 1
                
        # å¦‚æœæœ‰å¤šè¡Œç¬¦åˆç›®å½•ç‰¹å¾ï¼Œå¯èƒ½æ˜¯ç›®å½•é¡µ
        if pattern_count >= 3:
            return True
        
        return False

    def _load_single_document(self, file_path: Path) -> Optional[List[Document]]:
        """å¤šçº¿ç¨‹åŠ è½½å•ä¸ªæ–‡æ¡£æ–‡ä»¶ï¼ˆæ”¯æŒ PDFã€DOCXã€DOCï¼‰"""
        try:
            file_extension = file_path.suffix.lower()
            docs = []

            if file_extension == ".pdf":
                try:
                    # æ£€æŸ¥PDFé¡µæ•°
                    import fitz
                    with fitz.open(str(file_path)) as doc:
                        page_count = doc.page_count
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFæ–‡ä»¶ '{file_path.name}' å…±æœ‰ {page_count} é¡µ")
                        
                    # ä½¿ç”¨é…ç½®ä¸­çš„å‚æ•°åˆå§‹åŒ–å¤„ç†å™¨
                    processor = PDFProcessor(
                        file_path=str(file_path), 
                        lang='ch', 
                        use_gpu=self.use_gpu_for_ocr
                    )
                    
                    # æ ¹æ®é¡µæ•°é€‰æ‹©åˆé€‚çš„GPUå‚æ•°é…ç½®
                    if page_count > 30:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] PDFé¡µæ•°è¾ƒå¤š({page_count}é¡µ)ï¼Œåº”ç”¨å¤§æ–‡æ¡£ä¼˜åŒ–é…ç½®")
                        processor.configure_gpu(**self.config.pdf_ocr_large_doc_params)
                    else:
                        # ä½¿ç”¨æ ‡å‡†å‚æ•°é…ç½®
                        processor.configure_gpu(**self.config.pdf_ocr_params)
                    
                    # å¤„ç†PDF
                    docs = processor.process()
                    
                    # è¿‡æ»¤æ‰éå†…å®¹é¡µé¢ï¼ˆå°é¢ã€ç›®å½•ã€å‰è¨€ç­‰ï¼‰
                    if docs:
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} é¡µï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}é¡µ{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} é¡µéå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                    
                    # æ£€æŸ¥å¤„ç†ç»“æœ
                    if docs and len(docs) < page_count * 0.5:
                        logger.warning(f"[æ–‡æ¡£åŠ è½½] è­¦å‘Š: åªè¯†åˆ«å‡º {len(docs)}/{page_count} é¡µï¼Œä½äº50%ï¼Œå¯èƒ½æœ‰é—®é¢˜")
                    elif docs:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸè¯†åˆ« {len(docs)}/{page_count} é¡µ")
                    
                    # å¤„ç†åæ¸…ç†å†…å­˜
                    import gc
                    gc.collect()
                    try:
                        import torch
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except:
                        pass
                        
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†PDFæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    self.failed_files_count += 1
                    return None
                    
            elif file_extension in [".docx", ".doc"]:
                try:
                    # é¦–å…ˆå°è¯•å¯¼å…¥ä¾èµ–æ¨¡å—
                    try:
                        import docx2txt
                    except ImportError:
                        logger.error(f"ç¼ºå°‘å¤„ç†Wordæ–‡æ¡£æ‰€éœ€çš„ä¾èµ–åŒ…ï¼Œè¯·è¿è¡Œ: pip install docx2txt")
                        # è®°å½•é”™è¯¯ä½†ç»§ç»­æ‰§è¡Œï¼Œä»¥ä¾¿å¤„ç†å…¶ä»–æ–‡ä»¶ç±»å‹
                        self.failed_files_count += 1
                        return None
                        
                    from langchain_community.document_loaders import Docx2txtLoader
                    loader = Docx2txtLoader(str(file_path))
                    docs = loader.load()
                    
                    # å°è¯•è¿‡æ»¤Wordæ–‡æ¡£çš„éå†…å®¹é¡µé¢
                    if docs and len(docs) > 1:  # å¦‚æœWordæ–‡æ¡£è¢«åˆ†æˆäº†å¤šä¸ªé¡µé¢
                        filtered_docs = []
                        filtered_count = 0
                        filtered_types = []
                        
                        for i, doc in enumerate(docs):
                            if not self._is_non_content_page(doc.page_content, i):
                                filtered_docs.append(doc)
                            else:
                                filtered_count += 1
                                # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                page_type = "éå†…å®¹é¡µé¢"
                                if i == 0:
                                    page_type = "å°é¢"
                                elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                    page_type = "ç›®å½•/ç›®æ¬¡"
                                elif "å‰è¨€" in doc.page_content:
                                    page_type = "å‰è¨€"
                                    
                                filtered_types.append(page_type)
                                logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                
                        if filtered_count > 0:
                            # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                            type_summary = {}
                            for t in filtered_types:
                                if t not in type_summary:
                                    type_summary[t] = 0
                                type_summary[t] += 1
                                
                            type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                            logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                            docs = filtered_docs
                except Exception as e:
                    logger.error(f"[æ–‡æ¡£åŠ è½½] å¤„ç†DOCXæ–‡ä»¶ '{file_path.name}' å¤±è´¥: {str(e)}")
                    
                    # å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•
                    try:
                        logger.info(f"[æ–‡æ¡£åŠ è½½] å°è¯•ä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£...")
                        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
                        loader = UnstructuredWordDocumentLoader(str(file_path))
                        docs = loader.load()
                        logger.info(f"[æ–‡æ¡£åŠ è½½] æˆåŠŸä½¿ç”¨æ›¿ä»£æ–¹æ³•åŠ è½½Wordæ–‡æ¡£: {file_path.name}")
                        
                        # ä¹Ÿå°è¯•è¿‡æ»¤éå†…å®¹é¡µé¢
                        if docs and len(docs) > 1:
                            filtered_docs = []
                            filtered_count = 0
                            filtered_types = []
                            
                            for i, doc in enumerate(docs):
                                if not self._is_non_content_page(doc.page_content, i):
                                    filtered_docs.append(doc)
                                else:
                                    filtered_count += 1
                                    # å°è¯•åˆ¤æ–­é¡µé¢ç±»å‹
                                    page_type = "éå†…å®¹é¡µé¢"
                                    if i == 0 or i == 1:
                                        page_type = "å°é¢"
                                    elif "ç›®å½•" in doc.page_content or "ç›®æ¬¡" in doc.page_content:
                                        page_type = "ç›®å½•/ç›®æ¬¡"
                                    elif "å‰è¨€" in doc.page_content:
                                        page_type = "å‰è¨€"
                                        
                                    filtered_types.append(page_type)
                                    logger.info(f"[æ–‡æ¡£åŠ è½½] è¿‡æ»¤æ‰ '{file_path.name}' çš„ç¬¬ {i+1} éƒ¨åˆ†ï¼ˆç–‘ä¼¼{page_type}ï¼‰")
                                    
                            if filtered_count > 0:
                                # æ±‡æ€»è¿‡æ»¤æƒ…å†µ
                                type_summary = {}
                                for t in filtered_types:
                                    if t not in type_summary:
                                        type_summary[t] = 0
                                    type_summary[t] += 1
                                    
                                type_str = ", ".join([f"{k}{v}é¡µ" for k, v in type_summary.items()])
                                logger.info(f"[æ–‡æ¡£åŠ è½½] ä» '{file_path.name}' ä¸­è¿‡æ»¤æ‰ {filtered_count} éƒ¨åˆ†éå†…å®¹é¡µé¢ï¼ˆ{type_str}ï¼‰")
                                docs = filtered_docs
                    except Exception as e2:
                        logger.error(f"[æ–‡æ¡£åŠ è½½] æ›¿ä»£æ–¹æ³•ä¹Ÿå¤±è´¥: {str(e2)}")
                        self.failed_files_count += 1
                        return None
            else:
                logger.warning(f"[æ–‡æ¡£åŠ è½½] ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {file_path.name}")
                return None

            if docs:
                # ç»Ÿä¸€æ·»åŠ å…ƒæ•°æ®
                for doc in docs:
                    doc.metadata["source"] = str(file_path)
                    doc.metadata["file_name"] = file_path.name
                return docs
            return None

        except Exception as e:
            logger.error(f"[æ–‡æ¡£åŠ è½½] åŠ è½½ {file_path} å¤±è´¥: {str(e)}")
            self.failed_files_count += 1
            return None

    def load_documents(self) -> List:
        """åŠ è½½æ‰€æœ‰æ–‡æ¡£"""
        logger.info("âŒ› å¼€å§‹åŠ è½½æ–‡æ¡£...")

        # è·å–æ‰€æœ‰æ–‡æ¡£æ–‡ä»¶
        document_files = []
        for subfolder in self.subfolders:
            folder_path = self.config.data_dir / subfolder
            if folder_path.exists() and folder_path.is_dir():
                document_files.extend([f for f in folder_path.rglob("*") 
                                    if f.suffix.lower() in ['.pdf', '.docx', '.doc']])
            else:
                logger.warning(f"å­æ–‡ä»¶å¤¹ {subfolder} ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•: {folder_path}")
                
        # è¿‡æ»¤å¹¶æ’åºæ–‡ä»¶ï¼ˆå…ˆå¤„ç†è¾ƒå°çš„æ–‡ä»¶ï¼Œé¿å…å¤§æ–‡ä»¶å ç”¨æ˜¾å­˜ï¼‰
        document_files = sorted(document_files, key=lambda x: x.stat().st_size)
        logger.info(f"å‘ç° {len(document_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
        
        results = []
        # é™åˆ¶çº¿ç¨‹æ± å¤§å°ä»¥é¿å…èµ„æºäº‰ç”¨
        with ThreadPoolExecutor(max_workers=1) as executor:
            futures = [executor.submit(self._load_single_document, file) for file in document_files]
            with tqdm(total=len(futures), desc="åŠ è½½æ–‡æ¡£", unit="files") as pbar:
                for future in as_completed(futures):
                    res = future.result()
                    if res:
                        results.extend(res)
                        pbar.update(1)
                        pbar.set_postfix_str(f"å·²åŠ è½½ {len(res)} é¡µ")
                    else:
                        pbar.update(1)
        
        # åœ¨å¤„ç†å®Œæˆåæ¸…ç†GPUç¼“å­˜
        try:
            import torch
            if torch.cuda.is_available():
                torch.cuda.empty_cache()
        except:
            pass
        
        logger.info(f"âœ… æˆåŠŸåŠ è½½ {len(results)} é¡µæ–‡æ¡£")
        logger.info(f"âŒ æœªæˆåŠŸåŠ è½½ {self.failed_files_count} ä¸ªæ–‡ä»¶")
        
        return results

    def process_files(self) -> List:
        """ä¼˜åŒ–çš„æ–‡ä»¶å¤„ç†æµç¨‹ï¼Œä½¿ç”¨ç« èŠ‚åˆ†å—æ–¹æ³•"""
        logger.info("å¼€å§‹æ–‡ä»¶å¤„ç†æµç¨‹")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£
        all_docs = self.load_documents()

        if not all_docs:
            logger.warning("æ²¡æœ‰å¯å¤„ç†çš„æ–‡ä»¶å†…å®¹")
            return []

        # é¦–å…ˆæŒ‰æ–‡ä»¶åˆå¹¶é¡µé¢å†…å®¹ï¼Œé¿å…è·¨é¡µåˆ†å—æ–­è£‚
        logger.info("åˆå¹¶æ–‡ä»¶é¡µé¢å†…å®¹ï¼Œå‡†å¤‡è¿›è¡Œæ•´ä½“åˆ†å—...")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ•´ç†æ–‡æ¡£
        file_docs = {}
        for doc in all_docs:
            source = doc.metadata.get("source", "")
            if source not in file_docs:
                file_docs[source] = []
            file_docs[source].append(doc)
        
        # å¯¹æ¯ä¸ªæ–‡ä»¶çš„é¡µé¢è¿›è¡Œæ’åºå’Œåˆå¹¶
        whole_docs = []
        for source, docs in file_docs.items():
            # æŒ‰é¡µç æ’åº
            sorted_docs = sorted(docs, key=lambda x: x.metadata.get("page", 0))
            
            # åˆå¹¶æ–‡ä»¶æ‰€æœ‰é¡µé¢çš„å†…å®¹
            full_content = "\n".join([doc.page_content for doc in sorted_docs])
            
            # åˆ›å»ºå®Œæ•´æ–‡æ¡£å¯¹è±¡
            file_doc = Document(
                page_content=full_content,
                metadata={
                    "source": source,
                    "file_name": sorted_docs[0].metadata.get("file_name", ""),
                    "page_count": len(sorted_docs),
                    "is_merged_doc": True  # æ ‡è®°ä¸ºåˆå¹¶åçš„å®Œæ•´æ–‡æ¡£
                }
            )
            whole_docs.append(file_doc)
            
        logger.info(f"å·²å°† {len(all_docs)} é¡µå†…å®¹åˆå¹¶ä¸º {len(whole_docs)} ä¸ªå®Œæ•´æ–‡æ¡£")
        
        # ä½¿ç”¨æŒ‰ç« èŠ‚åˆ†å—æ–¹æ³•
        chunks = []
        
        with tqdm(total=len(whole_docs), desc="å¤„ç†æ–‡æ¡£ç« èŠ‚åˆ†å—") as pbar:
            for doc in whole_docs:
                metadata = doc.metadata.copy()
                # ç§»é™¤åˆ†å—åä¸å†é€‚ç”¨çš„å…ƒæ•°æ®
                if "is_merged_doc" in metadata:
                    del metadata["is_merged_doc"]
                
                # æŒ‰ç« èŠ‚åˆ†å—
                sections = self._split_by_section(doc.page_content)
                logger.info(f"æ­£åœ¨å¤„ç†{doc.metadata.get('file_name', 'æœªçŸ¥æ–‡ä»¶')}")
                logger.info(f"sections: {len(sections)}")
                # å¦‚æœæ‰¾åˆ°ç« èŠ‚ï¼Œåˆ™ä½¿ç”¨ç« èŠ‚åˆ†å—
                if sections:
                    logger.info(f"æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚ï¼Œä½¿ç”¨ç« èŠ‚ç»“æ„è¿›è¡Œåˆ†å—")
                    for i, (title, content, section_meta) in enumerate(sections):
                        if not content.strip():  # è·³è¿‡ç©ºç« èŠ‚
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(content.encode()).hexdigest()
                        
                        # åˆå¹¶å…ƒæ•°æ®
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata.update(section_meta)  # æ·»åŠ ç« èŠ‚å…ƒæ•°æ®
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(sections)
                        enhanced_metadata["chunk_type"] = "section"
                        
                        chunks.append(Document(
                            page_content=content,
                            metadata=enhanced_metadata
                        ))
                else:
                    # å¦‚æœæœªæ‰¾åˆ°ç« èŠ‚ç»“æ„ï¼Œåˆ™ç›´æ¥ä½¿ç”¨é€’å½’åˆ†å—
                    logger.warning(f"æœªæ£€æµ‹åˆ°ç« èŠ‚ç»“æ„ï¼Œç›´æ¥ä½¿ç”¨é€’å½’åˆ†å—æ–¹æ³•")
                    
                    # é€’å½’æ–‡æœ¬åˆ†å‰²é…ç½®
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=self.config.chunk_size,
                        chunk_overlap=self.config.chunk_overlap,
                        separators=[
                            "\n\n", "\n", "ã€‚", "ï¼›", "ï¼", "ï¼Ÿ", "ï¼Œ", " ", ""
                        ],
                        length_function=len,
                        add_start_index=True,
                        is_separator_regex=False
                    )
                    
                    # å¯¹å®Œæ•´æ–‡æ¡£è¿›è¡Œåˆ†å—
                    split_texts = text_splitter.split_text(doc.page_content)
                    
                    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                    for i, text in enumerate(split_texts):
                        if not text.strip():  # è·³è¿‡ç©ºæ–‡æœ¬å—
                            continue
                            
                        # ç”Ÿæˆå†…å®¹å“ˆå¸Œ
                        content_hash = hashlib.md5(text.encode()).hexdigest()
                        enhanced_metadata = metadata.copy()
                        enhanced_metadata["content_hash"] = content_hash
                        enhanced_metadata["chunk_index"] = i
                        enhanced_metadata["total_chunks"] = len(split_texts)
                        enhanced_metadata["chunk_type"] = "fixed_size"
                        
                        chunks.append(Document(
                            page_content=text,
                            metadata=enhanced_metadata
                        ))
                
                pbar.update(1)
        
        # ç›´æ¥ä½¿ç”¨ç”Ÿæˆçš„æ–‡æœ¬å—ï¼Œä¸è¿›è¡Œåå¤„ç†
        logger.info(f"ç”Ÿæˆ {len(chunks)} ä¸ªæ–‡æœ¬å—")
        
        # æ‰“å°åˆ†å—ç»“æœæ¦‚è§ˆ
        self._print_chunks_summary(chunks)
        
        # ä¿å­˜åˆ†å—åˆ°æ–‡ä»¶ï¼Œæ–¹ä¾¿ç”¨æˆ·æŸ¥çœ‹
        self.save_chunks_to_file(chunks)

        return chunks

    def _split_by_section(self, text: str) -> List[Tuple[str, str, Dict]]:
        """
        æ ¹æ®ç« èŠ‚æ ‡é¢˜ï¼ˆå¦‚1ã€1.1ã€1.1.1æ ¼å¼ï¼‰å°†æ–‡æœ¬åˆ†å‰²æˆæ®µè½ï¼Œ
        åŒæ—¶å°†ä¸€çº§æ ‡é¢˜ä¸å…¶ç›´æ¥ä¸‹å±å†…å®¹åˆå¹¶ä¸ºä¸€ä¸ªå—
        
        Args:
            text: å®Œæ•´çš„æ–‡æ¡£æ–‡æœ¬
            
        Returns:
            List[Tuple[str, str, Dict]]: è¿”å›ç« èŠ‚æ ‡é¢˜ã€ç« èŠ‚å†…å®¹å’Œå…ƒæ•°æ®çš„å…ƒç»„åˆ—è¡¨
        """
        logger.info("å¼€å§‹æŒ‰ç« èŠ‚ç»“æ„è¿›è¡Œæ–‡æ¡£åˆ†å—...")
        import re
        
        # åˆå¹¶æ‰€æœ‰æ¨¡å¼
        patterns = {
            # æ ‡å‡†æ ¼å¼
            'standard': [
                # ç¬¬ä¸€çº§åˆ°ç¬¬å››çº§æ ‡é¢˜
                r'^\s*(\d+)\.?\s+([^\n]+)$',
                r'^\s*(\d+\.\d+)\.?\s+([^\n]+)$',
                r'^\s*(\d+\.\d+\.\d+)\.?\s+([^\n]+)$',
                r'^\s*(\d+\.\d+\.\d+\.\d+)\.?\s+([^\n]+)$',
                # ä¸­æ–‡åºå·å’Œæ‹¬å·åºå·æ ‡é¢˜
                r'^\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€.ï¼]\s+([^\n]+)$',
                r'^\s*[ï¼ˆ(]([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[)ï¼‰]\s+([^\n]+)$',
                # é™„å½•æ ¼å¼
                r'^\s*(é™„å½•\s*[A-Za-z])[.ï¼ã€]?\s*([^\n]+)?$'
            ],
            # æ— ç©ºæ ¼æ ¼å¼
            'no_space': [
                r'^\s*(\d+)\.([\S].*?)$',
                r'^\s*(\d+\.\d+)([\S].*?)$',
                r'^\s*(\d+\.\d+\.\d+)([\S].*?)$',
                r'^\s*(\d+\.\d+\.\d+\.\d+)([\S].*?)$'
            ],
            # ç‹¬ç«‹ç« èŠ‚ç¼–å·
            'standalone': [
                r'^\s*(\d+)\s*$',
                r'^\s*(\d+)\.?\s*$',
                r'^\s*(\d+\.\d+)\.?\s*$',
                r'^\s*(\d+\.\d+\.\d+)\.?\s*$',
                r'^\s*(\d+\.\d+\.\d+\.\d+)\.?\s*$'
            ],
            # äº‹æ•…æŠ¥å‘Šç‰¹æœ‰æ ¼å¼
            'accident_report': [
                # äº‹æ•…æŠ¥å‘Šä¸­çš„ä¸­æ–‡ç¼–å·ï¼ˆä¸€ã€äºŒã€ä¸‰ã€å››ã€äº”ç­‰ï¼‰
                r'^\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{2})\s*[ã€,:ï¼š.ï¼]\s*(.+)$',
                # äº‹æ•…æŠ¥å‘Šä¸­çš„"1."ã€"2."æ ¼å¼
                r'^\s*(\d+)\s*[ã€,:ï¼š.ï¼]\s*(.+)$',
                # æ—¥æœŸæ ¼å¼æ ‡é¢˜ï¼ˆå¦‚"2014å¹´1æœˆ9æ—¥äº‹æ•…æƒ…å†µ"ï¼‰
                r'^(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥.*?)[:ï¼š]?\s*(.*)$',
                # "ï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰"æ ¼å¼
                r'^\s*[ï¼ˆ(]\s*(ä¸€|äºŒ|ä¸‰|å››|äº”|å…­|ä¸ƒ|å…«|ä¹|å|[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{2})\s*[)ï¼‰]\s*[ã€,:ï¼š.ï¼]?\s*(.+)$'
            ]
        }
        
        # åˆ†å‰²æ–‡æœ¬ä¸ºè¡Œ
        lines = text.split('\n')
        
        # åˆå§‹åŒ–å˜é‡
        sections = []
        current = {
            "num": "",
            "title": "",
            "content": [],
            "level": 0,
            "children": []  # å­˜å‚¨å­ç« èŠ‚
        }
        found_first_section = False
        
        # ç”¨äºæ—¥æœŸæ—¶é—´æ£€æµ‹çš„æ­£åˆ™è¡¨è¾¾å¼
        date_time_pattern = re.compile(r'(\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥(?:\s*[ä¸Šä¸‹]åˆ)?\s*\d{1,2}[æ—¶:](?:\d{1,2}åˆ†?)?)')
        
        # äº‹æ•…æŠ¥å‘Šç‰¹å¾çš„æ£€æµ‹è®¡æ•°
        accident_report_features = 0
        
        # ä¸´æ—¶å­˜å‚¨åŒºï¼Œç”¨äºåˆå¹¶ä¸€çº§æ ‡é¢˜ä¸å…¶å­é¡¹
        pending_main_sections = []
        current_main_section = None
        last_section_level = 0
        
        line_num = 0
        while line_num < len(lines):
            line = lines[line_num].strip()
            line_num += 1
            
            # æ£€æµ‹æ˜¯å¦åŒ…å«æ—¥æœŸæ—¶é—´ï¼ˆäº‹æ•…æŠ¥å‘Šç‰¹å¾ï¼‰
            if date_time_pattern.search(line):
                accident_report_features += 1
            
            # å¤„ç†ç©ºè¡Œ
            if not line:
                if current["content"]:
                    current["content"].append("")
                continue
            
            # è¯†åˆ«æ ‡é¢˜å˜é‡
            match_info = {
                "is_header": False,
                "level": 0,
                "num": "",
                "title": "",
                "next_line_used": False,
                "pattern_type": ""
            }
            
            # æ£€æŸ¥å„ç§æ ‡é¢˜æ¨¡å¼
            for pattern_type, pattern_list in patterns.items():
                if match_info["is_header"]:
                    break
                    
                for i, pattern in enumerate(pattern_list):
                    match = re.match(pattern, line)
                    if not match:
                        continue
                        
                    match_info["is_header"] = True
                    match_info["level"] = i + 1
                    match_info["num"] = match.group(1)
                    match_info["pattern_type"] = pattern_type
                    
                    # å¤„ç†æ ‡é¢˜æ–‡æœ¬
                    if len(match.groups()) > 1 and match.group(2):
                        match_info["title"] = match.group(2).strip()
                    else:
                        match_info["title"] = match_info["num"]
                    
                    # äº‹æ•…æŠ¥å‘Šç‰¹æœ‰çš„å¤„ç†
                    if pattern_type == 'accident_report':
                        # å¢åŠ äº‹æ•…æŠ¥å‘Šç‰¹å¾è®¡æ•°
                        accident_report_features += 1
                        # ä¸ºä¸­æ–‡æ•°å­—è®¾ç½®åˆé€‚çš„çº§åˆ«
                        if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]{1,2}$', match_info["num"]):
                            match_info["level"] = 1  # ä¸€çº§æ ‡é¢˜ï¼ˆä¸€ã€äºŒã€ä¸‰...ï¼‰
                        elif re.match(r'^\d+$', match_info["num"]):
                            match_info["level"] = 2  # äºŒçº§æ ‡é¢˜ï¼ˆ1ã€2ã€3...ï¼‰
                    
                    # å¯¹äºç‹¬ç«‹ç¼–å·æ ¼å¼ï¼Œæ£€æŸ¥ä¸‹ä¸€è¡Œæ˜¯å¦ä¸ºæ ‡é¢˜å†…å®¹
                    if pattern_type == 'standalone':
                        if line_num < len(lines):
                            # åªå‘å‰æŸ¥çœ‹æœ€å¤š3è¡Œ
                            for look_ahead in range(line_num, min(line_num + 3, len(lines))):
                                next_line = lines[look_ahead].strip()
                                if not next_line:
                                    continue
                                
                                # ç¡®è®¤ä¸‹ä¸€è¡Œä¸æ˜¯æ ‡é¢˜
                                is_next_header = False
                                for ptype in patterns.values():
                                    for p in ptype:
                                        if re.match(p, next_line):
                                            is_next_header = True
                                            break
                                    if is_next_header:
                                        break
                                
                                if not is_next_header and len(next_line) < 50:
                                    match_info["title"] = next_line
                                    match_info["next_line_used"] = True
                                    line_num = look_ahead + 1
                                    break
                    
                    break
            
            # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç¬¬ä¸€ä¸ªä¸€çº§æ ‡é¢˜
            if match_info["is_header"] and not found_first_section and match_info["level"] == 1:
                found_first_section = True
            
            # å¤„ç†ç« èŠ‚æ ‡é¢˜è¡Œ
            if match_info["is_header"] and (found_first_section or "é™„å½•" in match_info["num"] or accident_report_features > 2):
                # å½“è¯†åˆ«åˆ°æ ‡é¢˜æ—¶ï¼Œä¿å­˜å½“å‰çš„ç« èŠ‚
                if current["num"] and current["content"]:
                    content = "\n".join(current["content"])
                    metadata = {
                        "section_num": current["num"],
                        "section_title": current["title"],
                        "section_level": current["level"],
                        "section_type": "accident_report" if accident_report_features > 2 else "standard"
                    }
                    
                    # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€çº§æ ‡é¢˜
                    is_main_heading = current["level"] == 1
                    
                    # å¦‚æœæ˜¯ä¸€çº§æ ‡é¢˜ï¼Œå­˜å‚¨åˆ°ä¸»ç« èŠ‚å¯¹è±¡ä¸­
                    if is_main_heading:
                        if current_main_section:
                            # å°†å½“å‰ä¸»ç« èŠ‚æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨
                            pending_main_sections.append(current_main_section)
                        
                        # åˆ›å»ºæ–°çš„ä¸»ç« èŠ‚å¯¹è±¡
                        current_main_section = {
                            "title": current["num"] + " " + current["title"],
                            "content": content,
                            "metadata": metadata,
                            "children": []  # å­ç« èŠ‚åˆ—è¡¨
                        }
                    # å¦‚æœæ˜¯å­æ ‡é¢˜ä¸”å­˜åœ¨å½“å‰ä¸»ç« èŠ‚ï¼Œåˆ™æ·»åŠ åˆ°å­ç« èŠ‚åˆ—è¡¨ä¸­
                    elif current_main_section:
                        current_main_section["children"].append({
                            "title": current["num"] + " " + current["title"],
                            "content": content,
                            "metadata": metadata
                        })
                    # å¦‚æœæ˜¯å­æ ‡é¢˜ä½†æ²¡æœ‰ä¸»ç« èŠ‚ï¼ˆä¾‹å¦‚æ–‡æ¡£å¼€å§‹å°±æ˜¯å­ç« èŠ‚ï¼‰ï¼Œåˆ™ç›´æ¥æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨
                    else:
                        sections.append((current["num"] + " " + current["title"], content, metadata))
                
                # åˆ›å»ºæ–°ç« èŠ‚
                current = {
                    "num": match_info["num"],
                    "title": match_info["title"],
                    "level": match_info["level"],
                    "content": [f"{match_info['num']} {match_info['title']}"] if not match_info["next_line_used"] else [f"{match_info['num']} {match_info['title']}"]
                }
                
                # è®°å½•ä¸Šä¸€ä¸ªç« èŠ‚çš„çº§åˆ«
                last_section_level = match_info["level"]
            else:
                # å¸¸è§„å†…å®¹è¡Œ
                if current["content"] or not found_first_section:
                    current["content"].append(line)
                # å¦‚æœæ˜¯é¦–æ®µå†…å®¹ä¸”è¿˜æ²¡æœ‰ç« èŠ‚ï¼Œåˆ™åˆ›å»ºä¸€ä¸ª"æ¦‚è¿°"ç« èŠ‚
                elif not current["content"] and not sections and len(line) > 20:
                    current = {
                        "num": "æ¦‚è¿°",
                        "title": "",
                        "level": 0,
                        "content": [line]
                    }
        
        # å¤„ç†æœ€åä¸€ä¸ªç« èŠ‚
        if current["num"] and current["content"]:
            content = "\n".join(current["content"])
            metadata = {
                "section_num": current["num"],
                "section_title": current["title"],
                "section_level": current["level"],
                "section_type": "accident_report" if accident_report_features > 2 else "standard"
            }
            
            # åˆ¤æ–­æ˜¯å¦æ˜¯ä¸€çº§æ ‡é¢˜
            is_main_heading = current["level"] == 1
            
            if is_main_heading:
                if current_main_section:
                    pending_main_sections.append(current_main_section)
                
                current_main_section = {
                    "title": current["num"] + " " + current["title"],
                    "content": content,
                    "metadata": metadata,
                    "children": []
                }
            elif current_main_section:
                current_main_section["children"].append({
                    "title": current["num"] + " " + current["title"],
                    "content": content,
                    "metadata": metadata
                })
            else:
                sections.append((current["num"] + " " + current["title"], content, metadata))
        
        # æ·»åŠ æœ€åä¸€ä¸ªä¸»ç« èŠ‚
        if current_main_section:
            pending_main_sections.append(current_main_section)
        
        # å¤„ç†æ‰€æœ‰å¾…å¤„ç†çš„ä¸»ç« èŠ‚åŠå…¶å­ç« èŠ‚ï¼Œåˆå¹¶å†…å®¹
        for main_section in pending_main_sections:
            if main_section["children"]:
                # åˆå¹¶ä¸»ç« èŠ‚å†…å®¹å’Œæ‰€æœ‰å­ç« èŠ‚å†…å®¹
                combined_content = main_section["content"] + "\n\n"
                
                for child in main_section["children"]:
                    combined_content += child["content"] + "\n\n"
                
                # ä¿ç•™ä¸»ç« èŠ‚çš„å…ƒæ•°æ®ï¼Œä½†æ·»åŠ åŒ…å«å­ç« èŠ‚ä¿¡æ¯
                main_section["metadata"]["contains_subsections"] = True
                main_section["metadata"]["subsection_count"] = len(main_section["children"])
                
                # å°†åˆå¹¶åçš„ç« èŠ‚æ·»åŠ åˆ°è¾“å‡ºåˆ—è¡¨
                sections.append((main_section["title"], combined_content.strip(), main_section["metadata"]))
            else:
                # å¦‚æœæ²¡æœ‰å­ç« èŠ‚ï¼Œåˆ™ç›´æ¥æ·»åŠ ä¸»ç« èŠ‚
                sections.append((main_section["title"], main_section["content"], main_section["metadata"]))
        
        # å¦‚æœå…¨æ–‡æ²¡æœ‰è¯†åˆ«åˆ°ç« èŠ‚ï¼Œä½†æœ‰å†…å®¹ä¸”åŒ…å«äº‹æ•…æŠ¥å‘Šç‰¹å¾
        if not sections and accident_report_features > 1:
            # ä½¿ç”¨ç®€å•æ®µè½åˆ†å‰²ï¼ˆé€šè¿‡è¿ç»­ç©ºè¡Œï¼‰
            paragraphs = []
            current_para = []
            
            for line in lines:
                line = line.strip()
                if line:
                    current_para.append(line)
                elif current_para:  # ç©ºè¡Œä¸”å½“å‰æ®µè½æœ‰å†…å®¹
                    paragraphs.append("\n".join(current_para))
                    current_para = []
            
            # æ·»åŠ æœ€åä¸€ä¸ªæ®µè½
            if current_para:
                paragraphs.append("\n".join(current_para))
            
            # å°†æ®µè½è½¬æ¢ä¸ºç« èŠ‚
            for i, paragraph in enumerate(paragraphs):
                if len(paragraph) > 30:  # åªå¤„ç†è¾ƒé•¿çš„æ®µè½
                    # å°è¯•ä»æ®µè½ä¸­æå–æ ‡é¢˜
                    first_line = paragraph.split("\n")[0] if "\n" in paragraph else ""
                    title = first_line[:50] if len(first_line) > 10 else f"æ®µè½{i+1}"
                    
                    metadata = {
                        "section_num": f"P{i+1}",
                        "section_title": title,
                        "section_level": 1,
                        "section_type": "accident_report_paragraph"
                    }
                    sections.append((f"P{i+1} {title}", paragraph, metadata))
        
        # è¾“å‡ºç»“æœæ‘˜è¦
        logger.info(f"æŒ‰ç« èŠ‚ç»“æ„åˆ†å—å®Œæˆï¼Œå…±æ‰¾åˆ° {len(sections)} ä¸ªç« èŠ‚")
        if sections:
            for i, (title, _, meta) in enumerate(sections[:min(5, len(sections))]):
                section_type = meta.get("section_type", "standard")
                contains_subsections = meta.get("contains_subsections", False)
                subsection_str = f"ï¼ŒåŒ…å« {meta.get('subsection_count', 0)} ä¸ªå­ç« èŠ‚" if contains_subsections else ""
                logger.info(f"  â€¢ ç« èŠ‚ {i+1}: {title} (çº§åˆ«: {meta['section_level']}, ç±»å‹: {section_type}{subsection_str})")
            
            if len(sections) > 5:
                logger.info(f"  ... ä»¥åŠ {len(sections)-5} ä¸ªå…¶ä»–ç« èŠ‚")
        
        return sections

    def _ensure_complete_sentences(self, text: str) -> str:
        """ç¡®ä¿æ–‡æœ¬å—ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        
        Args:
            text: åŸå§‹æ–‡æœ¬å—
            
        Returns:
            å¤„ç†åçš„æ–‡æœ¬å—ï¼Œç¡®ä¿ä»¥å®Œæ•´å¥å­å¼€å§‹å’Œç»“æŸ
        """
        if not text or len(text) < 10:  # æ–‡æœ¬è¿‡çŸ­åˆ™ç›´æ¥è¿”å›
            return text
            
        # ä¸­æ–‡å¥å­ç»“æŸæ ‡è®°
        sentence_end_marks = ['ã€‚', 'ï¼', 'ï¼Ÿ', 'ï¼›', '\n']
        # å¥å­å¼€å§‹çš„å¯èƒ½æ ‡è®°ï¼ˆä¸­æ–‡æ®µè½å¼€å¤´ã€ç« èŠ‚æ ‡é¢˜ç­‰ï¼‰
        sentence_start_patterns = ['\n', 'ç¬¬.{1,3}ç« ', 'ç¬¬.{1,3}èŠ‚']
        
        # å¤„ç†æ–‡æœ¬å—å¼€å¤´
        text_stripped = text.lstrip()
        # å¦‚æœä¸æ˜¯ä»¥å¥æœ«æ ‡ç‚¹å¼€å¤´ï¼Œä¹Ÿä¸æ˜¯ä»¥å¤§å†™å­—æ¯æˆ–æ•°å­—å¼€å¤´ï¼ˆå¯èƒ½æ˜¯æ–°æ®µè½ï¼‰ï¼Œåˆ™å¯èƒ½æ˜¯ä¸å®Œæ•´å¥å­
        is_incomplete_start = True
        
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­æˆ–æ®µè½å¼€å§‹çš„æ ‡è®°
        for pattern in sentence_start_patterns:
            if text.startswith(pattern) or text_stripped[0].isupper() or text_stripped[0].isdigit():
                is_incomplete_start = False
                break
        
        if is_incomplete_start:
            # æŸ¥æ‰¾ç¬¬ä¸€ä¸ªå®Œæ•´å¥å­çš„å¼€å§‹
            for mark in sentence_end_marks:
                pos = text.find(mark)
                if pos > 0:
                    # æ‰¾åˆ°å¥æœ«æ ‡è®°åçš„å†…å®¹ä½œä¸ºèµ·ç‚¹
                    try:
                        # ç¡®ä¿å¥æœ«æ ‡è®°åè¿˜æœ‰å†…å®¹
                        if pos + 1 < len(text):
                            text = text[pos+1:].lstrip()
                            break
                    except:
                        # å‡ºé”™åˆ™ä¿æŒåŸæ ·
                        pass
        
        # å¤„ç†æ–‡æœ¬å—ç»“å°¾
        is_incomplete_end = True
        # æ£€æŸ¥æ˜¯å¦ä»¥å®Œæ•´å¥å­ç»“æŸ
        for mark in sentence_end_marks:
            if text.endswith(mark):
                is_incomplete_end = False
                break
        
        if is_incomplete_end:
            # æ‰¾æœ€åä¸€ä¸ªå®Œæ•´å¥å­çš„ç»“æŸä½ç½®
            last_pos = -1
            for mark in sentence_end_marks:
                pos = text.rfind(mark)
                if pos > last_pos:
                    last_pos = pos
                    
            if last_pos > 0:
                # æˆªå–åˆ°æœ€åä¸€ä¸ªå®Œæ•´å¥å­ç»“æŸ
                text = text[:last_pos+1]
        
        return text.strip()
    
    def _post_process_chunks(self, chunks: List[Document]) -> List[Document]:
        """å¯¹åˆ†å—åçš„æ–‡æœ¬è¿›è¡Œåå¤„ç†ï¼Œä¼˜åŒ–å—çš„è´¨é‡
        
        Args:
            chunks: åŸå§‹åˆ†å—åˆ—è¡¨
            
        Returns:
            å¤„ç†åçš„åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            return []
            
        logger.info("å¯¹æ–‡æœ¬å—è¿›è¡Œåå¤„ç†ä¼˜åŒ–...")
        processed_chunks = []
        
        # æŒ‰æ–‡æ¡£æºåˆ†ç»„å¤„ç†
        doc_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            if source not in doc_chunks:
                doc_chunks[source] = []
            doc_chunks[source].append(chunk)
        
        total_merged = 0
        
        # å¤„ç†æ¯ä¸ªæ–‡æ¡£çš„å—
        for source, source_chunks in doc_chunks.items():
            # æŒ‰å—ç´¢å¼•æ’åº
            sorted_chunks = sorted(source_chunks, 
                                   key=lambda x: x.metadata.get("chunk_index", 0))
            
            # æ£€æŸ¥å’Œå¤„ç†ç›¸é‚»å—
            for i, chunk in enumerate(sorted_chunks):
                # åªå¯¹ç« èŠ‚ç±»å‹çš„å—åº”ç”¨å®Œæ•´å¥å­å¤„ç†
                if chunk.metadata.get("chunk_type") == "section":
                    chunk.page_content = self._ensure_complete_sentences(chunk.page_content)
                
                # è·³è¿‡ç©ºå—
                if not chunk.page_content.strip():
                    continue
            
                processed_chunks.append(chunk)
        
        logger.info(f"åå¤„ç†å®Œæˆï¼Œä¼˜åŒ–åçš„å—æ•°: {len(processed_chunks)}")
        return processed_chunks
        

    def _print_chunks_summary(self, chunks: List[Document]):
        """æ‰“å°æ–‡æœ¬åˆ†å—ç»“æœæ¦‚è§ˆ"""
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›æ˜¾ç¤º")
            return
            
        # ç»Ÿè®¡ä¿¡æ¯
        total_chunks = len(chunks)
        avg_chunk_length = sum(len(chunk.page_content) for chunk in chunks) / total_chunks
        files_count = len(set(chunk.metadata.get("source", "") for chunk in chunks))
        
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š æ–‡æœ¬åˆ†å—å¤„ç†æ¦‚è§ˆ")
        logger.info("="*50)
        logger.info(f"ğŸ“„ æ€»å—æ•°: {total_chunks}")
        logger.info(f"ğŸ“Š å¹³å‡å—é•¿åº¦: {avg_chunk_length:.1f} å­—ç¬¦")
        logger.info(f"ğŸ“‚ æ¶‰åŠæ–‡ä»¶æ•°: {files_count}")
        
        # æ–‡ä»¶çº§ç»Ÿè®¡
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        logger.info("\nğŸ“‚ æ–‡ä»¶çº§åˆ†å—ç»Ÿè®¡:")
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"  â€¢ {file_name}: {len(file_chunks_list)} å—")
        
        
        # è¾“å‡ºè¯¦ç»†åˆ†å—å†…å®¹ (å¦‚æœå¼€å¯)
        if self.print_detailed_chunks:
            self._print_detailed_chunks(chunks)
            
        logger.info("="*50)

    def _print_detailed_chunks(self, chunks: List[Document]):
        """è¾“å‡ºè¯¦ç»†çš„åˆ†å—å†…å®¹"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“‘ è¯¦ç»†æ–‡æœ¬å—å†…å®¹")
        logger.info("="*50)
        
        # å°†åˆ†å—æŒ‰æ–‡ä»¶åˆ†ç»„
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # ä¸ºäº†æ›´æœ‰ç»„ç»‡åœ°è¾“å‡ºï¼Œå…ˆæŒ‰æ–‡ä»¶è¾“å‡º
        for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
            file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
            logger.info(f"\nğŸ“„ æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)")
            
            # è¾“å‡ºè¯¥æ–‡ä»¶çš„å‰3ä¸ªå—
            for i, chunk in enumerate(file_chunks_list[:3]):
                page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                chunk_size = len(chunk.page_content)
                
                # è·å–é¢„è§ˆå†…å®¹
                content_preview = chunk.page_content
                if len(content_preview) > self.max_chunk_preview_length:
                    content_preview = content_preview[:self.max_chunk_preview_length] + "..."
                
                # æ›¿æ¢æ¢è¡Œç¬¦ä»¥ä¾¿äºæ§åˆ¶å°æ˜¾ç¤º
                content_preview = content_preview.replace("\n", "\\n")
                
                logger.info(f"\n  å— {i+1}/{len(file_chunks_list[:3])} [ç¬¬{page_num}é¡µ, {chunk_size}å­—ç¬¦]:")
                logger.info(f"  {content_preview}")
            
            # å¦‚æœæ–‡ä»¶ä¸­çš„å—æ•°è¶…è¿‡3ä¸ªï¼Œæ˜¾ç¤ºçœç•¥ä¿¡æ¯
            if len(file_chunks_list) > 3:
                logger.info(f"  ... è¿˜æœ‰ {len(file_chunks_list) - 3} ä¸ªå—æœªæ˜¾ç¤º ...")
                
        # è¾“å‡ºä¿å­˜å®Œæ•´åˆ†å—å†…å®¹çš„æç¤º
        chunks_detail_file = self.cache_dir / "chunks_detail.txt"
        try:
            with open(chunks_detail_file, "w", encoding="utf-8") as f:
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    page_num = chunk.metadata.get("page", "æœªçŸ¥é¡µç ")
                    
                    f.write(f"=== å— {i+1}/{len(chunks)} [{file_name} - ç¬¬{page_num}é¡µ] ===\n")
                    f.write(chunk.page_content)
                    f.write("\n\n")
            
            logger.info(f"\nâœ… æ‰€æœ‰æ–‡æœ¬å—çš„è¯¦ç»†å†…å®¹å·²ä¿å­˜è‡³: {chunks_detail_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜è¯¦ç»†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        logger.info("="*50)

    def create_embeddings(self) -> HuggingFaceEmbeddings:
        """åˆ›å»ºåµŒå…¥æ¨¡å‹å®ä¾‹"""
        logger.info("åˆå§‹åŒ–åµŒå…¥æ¨¡å‹...")
        return HuggingFaceEmbeddings(
            model_name=self.config.embedding_model_path,  # åµŒå…¥æ¨¡å‹çš„è·¯å¾„
            model_kwargs={"device": self.config.device},  # è®¾ç½®è®¾å¤‡ä¸ºCPUæˆ–GPU
            encode_kwargs={
                "batch_size": self.config.batch_size,  # æ‰¹å¤„ç†å¤§å°
                "normalize_embeddings": self.config.normalize_embeddings  # æ˜¯å¦å½’ä¸€åŒ–åµŒå…¥
            },
        )

    def backup_vector_db(self):
        """å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“"""
        vector_db_path = Path(self.config.vector_db_path)
        if not vector_db_path.exists():
            return False
            
        try:
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = vector_db_path.parent / f"{vector_db_path.name}_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            backup_dir.mkdir(parents=True, exist_ok=True)
            
            # å¤åˆ¶æ‰€æœ‰æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            for item in vector_db_path.glob('*'):
                if item.is_file():
                    shutil.copy2(item, backup_dir)
                elif item.is_dir():
                    shutil.copytree(item, backup_dir / item.name)
                    
            logger.info(f"âœ… å‘é‡æ•°æ®åº“å·²å¤‡ä»½è‡³ {backup_dir}")
            return True
        except Exception as e:
            logger.error(f"å¤‡ä»½å‘é‡æ•°æ®åº“å¤±è´¥: {str(e)}")
            return False

    def build_vector_store(self):
        """æ„å»ºå‘é‡æ•°æ®åº“
        
        Returns:
            List[Document]: å¤„ç†åçš„æ–‡æ¡£å—åˆ—è¡¨
        """
        logger.info("å¼€å§‹æ„å»ºå‘é‡æ•°æ®åº“")

        # åˆ›å»ºå¿…è¦ç›®å½•
        Path(self.config.vector_db_path).mkdir(parents=True, exist_ok=True)

        # å¤„ç†æ–‡æ¡£
        chunks = self.process_files()  # å¤„ç†æ–‡æ¡£å¹¶åˆ†å—
        
        if not chunks:
            logger.warning("æ²¡æœ‰æ–‡æ¡£å—å¯ä»¥å¤„ç†ï¼Œè·³è¿‡å‘é‡å­˜å‚¨æ„å»º")
            return []

        # å¤‡ä»½ç°æœ‰å‘é‡æ•°æ®åº“
        if Path(self.config.vector_db_path).exists() and any(Path(self.config.vector_db_path).glob('*')):
            self.backup_vector_db()

        # ç”ŸæˆåµŒå…¥æ¨¡å‹
        embeddings = self.create_embeddings()

        # æ„å»ºå‘é‡å­˜å‚¨
        logger.info("ç”Ÿæˆå‘é‡...")
        # æ„å»ºå‘é‡å­˜å‚¨æ—¶æ˜¾å¼æŒ‡å®š
        vector_store = FAISS.from_documents(
            chunks,
            embeddings,
            distance_strategy=DistanceStrategy.COSINE  # æ˜ç¡®æŒ‡å®šä½™å¼¦ç›¸ä¼¼åº¦
        )

        # ä¿å­˜å‘é‡æ•°æ®åº“
        vector_store.save_local(str(self.config.vector_db_path))  # ä¿å­˜å‘é‡å­˜å‚¨åˆ°æŒ‡å®šè·¯å¾„
        logger.info(f"å‘é‡æ•°æ®åº“å·²ä¿å­˜è‡³ {self.config.vector_db_path}")  # è¾“å‡ºä¿å­˜è·¯å¾„
        
        # è¿”å›å¤„ç†åçš„æ–‡æ¡£å—
        return chunks

    def save_chunks_to_file(self, chunks: List[Document]):
        """å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°æ–‡ä»¶ä¸­ï¼Œæ”¯æŒå¤šç§æ ¼å¼ï¼Œä½†ä¸ä½œä¸ºç¼“å­˜å­˜å‚¨
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
        """
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›ä¿å­˜")
            return
        
        # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # ä¿å­˜ä¸ºçº¯æ–‡æœ¬æ ¼å¼ï¼Œæ–¹ä¾¿ç›´æ¥æŸ¥çœ‹
        text_file = self.cache_dir / "chunks_text.txt"
        try:
            with open(text_file, "w", encoding="utf-8") as f:
                f.write(f"æ–‡æ¡£åˆ†å—æ€»è§ˆ\n")
                f.write(f"=============\n")
                f.write(f"æ€»å—æ•°: {len(chunks)}\n")
                f.write(f"æ¶‰åŠæ–‡ä»¶æ•°: {len(set(chunk.metadata.get('source', '') for chunk in chunks))}\n\n")
                
                # æŒ‰æ–‡ä»¶åˆ†ç»„è¾“å‡º
                file_chunks = {}
                for chunk in chunks:
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    if source not in file_chunks:
                        file_chunks[source] = []
                    file_chunks[source].append(chunk)
                
                for file_path, file_chunks_list in sorted(file_chunks.items(), key=lambda x: len(x[1]), reverse=True):
                    file_name = Path(file_path).name if isinstance(file_path, str) else "æœªçŸ¥æ–‡ä»¶"
                    f.write(f"\n{'='*80}\n")
                    f.write(f"æ–‡ä»¶: {file_name} (å…±{len(file_chunks_list)}å—)\n")
                    f.write(f"{'='*80}\n\n")
                    
                    for i, chunk in enumerate(file_chunks_list):
                        # è·å–ç« èŠ‚ä¿¡æ¯
                        section_num = chunk.metadata.get("section_num", "")
                        section_title = chunk.metadata.get("section_title", "")
                        chunk_index = chunk.metadata.get("chunk_index", i)
                        total_chunks = chunk.metadata.get("total_chunks", len(file_chunks_list))
                        position = chunk.metadata.get("position", "")
                        chunk_type = chunk.metadata.get("chunk_type", "")
                        
                        # æ„å»ºå—æ ‡é¢˜
                        header = f"----- å— {chunk_index+1}/{total_chunks} "
                        if section_num and section_title:
                            header += f"[ç« èŠ‚: {section_num} {section_title}, "
                        header += f"ä½ç½®:{position}, {len(chunk.page_content)}å­—ç¬¦"
                        if chunk_type:
                            header += f", ç±»å‹:{chunk_type}"
                        header += "] -----\n"
                        
                        # å†™å…¥å—ä¿¡æ¯
                        f.write(header)
                        f.write(chunk.page_content)
                        f.write("\n\n")
            
            logger.info(f"âœ… æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å·²ä¿å­˜è‡³: {text_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜æ–‡æœ¬æ ¼å¼çš„åˆ†å—å†…å®¹å¤±è´¥: {str(e)}")
        
        # ä¿å­˜ä¸ºJSONæ ¼å¼ï¼ŒåŒ…å«å®Œæ•´çš„å…ƒæ•°æ®
        json_file = self.cache_dir / "chunks_detail.json"
        try:
            chunks_data = []
            for i, chunk in enumerate(chunks):
                chunk_data = {
                    "index": i,
                    "content": chunk.page_content,
                    "length": len(chunk.page_content),
                    "metadata": chunk.metadata
                }
                chunks_data.append(chunk_data)
                
            with open(json_file, "w", encoding="utf-8") as f:
                json.dump({
                    "total_chunks": len(chunks),
                    "timestamp": datetime.now().isoformat(),
                    "chunks": chunks_data
                }, f, ensure_ascii=False, indent=2)
                
            logger.info(f"âœ… JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å·²ä¿å­˜è‡³: {json_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜JSONæ ¼å¼çš„åˆ†å—è¯¦ç»†ä¿¡æ¯å¤±è´¥: {str(e)}")
        
        # ä¿å­˜CSVæ ¼å¼çš„æ‘˜è¦ä¿¡æ¯ï¼Œæ–¹ä¾¿å¯¼å…¥ç”µå­è¡¨æ ¼æŸ¥çœ‹
        csv_file = self.cache_dir / "chunks_summary.csv"
        try:
            with open(csv_file, "w", encoding="utf-8") as f:
                # å†™å…¥CSVå¤´
                f.write("ç´¢å¼•,æ–‡ä»¶å,ç« èŠ‚ç¼–å·,ç« èŠ‚æ ‡é¢˜,å—ç´¢å¼•,æ€»å—æ•°,ä½ç½®,å­—ç¬¦æ•°,å†…å®¹é¢„è§ˆ\n")
                
                for i, chunk in enumerate(chunks):
                    source = chunk.metadata.get("source", "æœªçŸ¥æ¥æº")
                    file_name = Path(source).name if isinstance(source, str) else "æœªçŸ¥æ–‡ä»¶"
                    section_num = chunk.metadata.get("section_num", "")
                    section_title = chunk.metadata.get("section_title", "")
                    chunk_index = chunk.metadata.get("chunk_index", i)
                    total_chunks = chunk.metadata.get("total_chunks", 0)
                    position = chunk.metadata.get("position", "")
                    length = len(chunk.page_content)
                    
                    # å†…å®¹é¢„è§ˆï¼Œå»é™¤æ¢è¡Œç¬¦
                    preview = chunk.page_content[:100].replace("\n", " ").replace("\r", " ")
                    if len(chunk.page_content) > 100:
                        preview += "..."
                    preview = f'"{preview}"'  # ç”¨å¼•å·åŒ…å›´ï¼Œé¿å…CSVè§£æé”™è¯¯
                    
                    f.write(f"{i},{file_name},{section_num},{section_title},{chunk_index},{total_chunks},{position},{length},{preview}\n")
                
            logger.info(f"âœ… CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å·²ä¿å­˜è‡³: {csv_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜CSVæ ¼å¼çš„åˆ†å—æ‘˜è¦å¤±è´¥: {str(e)}")

    def save_chunks_to_excel(self, chunks: List[Document], output_dir: str = None):
        """å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°Excelæ–‡ä»¶ä¸­ï¼ŒæŒ‰ç…§æºæ–‡ä»¶åˆ†ç»„
        
        Args:
            chunks: æ–‡æ¡£åˆ†å—åˆ—è¡¨
            output_dir: è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸ºC:/Users/Administrator/Desktop/chunks
        """
        if not chunks:
            logger.info("æ²¡æœ‰æ–‡æœ¬å—å¯ä¾›ä¿å­˜")
            return
        
        # è®¾ç½®è¾“å‡ºç›®å½•ï¼Œå¦‚æœæœªæŒ‡å®šåˆ™ä½¿ç”¨æ¡Œé¢è·¯å¾„
        if output_dir is None:
            output_dir = r"C:\Users\Administrator\Desktop\chunks"
            
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"å¼€å§‹å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°Excelæ–‡ä»¶ï¼Œè¾“å‡ºç›®å½•: {output_dir}")
        
        # æŒ‰æ–‡ä»¶åˆ†ç»„æ•´ç†æ–‡æ¡£å—
        file_chunks = {}
        for chunk in chunks:
            source = chunk.metadata.get("source", "")
            if source not in file_chunks:
                file_chunks[source] = []
            file_chunks[source].append(chunk)
        
        # æœ¯è¯­åˆ¤æ–­æ­£åˆ™è¡¨è¾¾å¼
        term_patterns = [
            r'^(?:\d+\.?)+\s*æœ¯è¯­(?:å’Œå®šä¹‰)?$',  # åŒ¹é…"3.æœ¯è¯­"ã€"3.1 æœ¯è¯­å’Œå®šä¹‰"ç­‰
            r'^ç¬¬\s*\d+\s*ç« \s*æœ¯è¯­(?:å’Œå®šä¹‰)?$',  # åŒ¹é…"ç¬¬3ç«  æœ¯è¯­å’Œå®šä¹‰"
            r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+ã€\s*æœ¯è¯­(?:å’Œå®šä¹‰)?$'  # åŒ¹é…"ä¸‰ã€æœ¯è¯­å’Œå®šä¹‰"
        ]
        
        # å¤„ç†æ¯ä¸ªæ–‡ä»¶çš„å—
        total_files = len(file_chunks)
        processed_files = 0
        
        with tqdm(total=total_files, desc="ä¿å­˜Excelæ–‡ä»¶") as pbar:
            for source, source_chunks in file_chunks.items():
                try:
                    # è·å–æ–‡ä»¶åï¼Œç”¨ä½œExcelæ–‡ä»¶å
                    file_path = Path(source)
                    file_name = file_path.stem  # ä¸å¸¦æ‰©å±•åçš„æ–‡ä»¶å
                    excel_filename = f"{file_name}.xlsx"
                    excel_path = output_path / excel_filename
                    
                    # åˆ›å»ºæ•°æ®ç»“æ„ä»¥å­˜å‚¨Excelæ•°æ®
                    excel_data = []
                    
                    # æŒ‰å—ç´¢å¼•æ’åº
                    sorted_chunks = sorted(source_chunks, 
                                           key=lambda x: x.metadata.get("chunk_index", 0))
                    
                    # å¤„ç†æ¯ä¸ªæ–‡æœ¬å—
                    for chunk in sorted_chunks:
                        content = chunk.page_content
                        
                        # è·å–ç« èŠ‚ç¼–å·å’Œæ ‡é¢˜
                        section_num = chunk.metadata.get("section_num", "")
                        section_title = chunk.metadata.get("section_title", "")
                        
                        # åŸæ–‡å†…å®¹ï¼šåˆ é™¤ç« èŠ‚ç¼–å·
                        # å¦‚æœå†…å®¹ä»¥ç« èŠ‚ç¼–å·å¼€å¤´ï¼Œåˆ™åˆ é™¤
                        original_content = content
                        if section_num:
                            # å°è¯•åˆ é™¤å¼€å¤´çš„ç« èŠ‚ç¼–å·å’Œæ ‡é¢˜
                            header_pattern = f"^{re.escape(section_num)}\\s*{re.escape(section_title)}\\s*"
                            original_content = re.sub(header_pattern, "", original_content, count=1)
                        
                        # å…¥åº“å†…å®¹ï¼šç›´æ¥ä½¿ç”¨åŸå§‹content
                        db_content = content
                        
                        # åˆ¤æ–­æ˜¯å¦æ˜¯æœ¯è¯­å—
                        is_term = False
                        
                        # æ£€æŸ¥å—çš„æ ‡é¢˜æ˜¯å¦ç¬¦åˆæœ¯è¯­ç« èŠ‚çš„ç‰¹å¾
                        if section_title and section_num:
                            full_title = f"{section_num} {section_title}"
                            for pattern in term_patterns:
                                if re.search(pattern, full_title, re.IGNORECASE):
                                    is_term = True
                                    break
                        
                        # æ£€æŸ¥å—å†…å®¹æ˜¯å¦åŒ…å«æœ¯è¯­å®šä¹‰çš„ç‰¹å¾
                        if not is_term:
                            # æœ¯è¯­å®šä¹‰é€šå¸¸é‡‡ç”¨"æœ¯è¯­å å®šä¹‰"çš„æ ¼å¼
                            definition_patterns = [
                                r'\d+\.\d+\s+[\u4e00-\u9fa5a-zA-Z]+\s+[\u4e00-\u9fa5]',  # 3.1 æœ¯è¯­ å®šä¹‰
                                r'[\u4e00-\u9fa5a-zA-Z]+\s+[\u4e00-\u9fa5]'  # æœ¯è¯­ å®šä¹‰
                            ]
                            
                            definition_count = 0
                            content_lines = content.split('\n')
                            for line in content_lines:
                                for pattern in definition_patterns:
                                    if re.search(pattern, line):
                                        definition_count += 1
                                        break
                            
                            # å¦‚æœæœ‰å¤šè¡Œç¬¦åˆæœ¯è¯­å®šä¹‰æ¨¡å¼ï¼Œåˆ™è®¤ä¸ºæ˜¯æœ¯è¯­å—
                            if definition_count >= 3:  # è‡³å°‘æœ‰3ä¸ªæœ¯è¯­å®šä¹‰
                                is_term = True
                        
                        # å°†æ–‡æœ¬å—æ·»åŠ åˆ°Excelæ•°æ®ä¸­
                        excel_data.append({
                            "åŸæ–‡å†…å®¹": original_content.strip(),
                            "å…¥åº“å†…å®¹": db_content.strip(),
                            "æœ¯è¯­": "æ˜¯" if is_term else "å¦"
                        })
                    
                    # åˆ›å»ºDataFrameå¹¶ä¿å­˜ä¸ºExcel
                    if excel_data:
                        df = pd.DataFrame(excel_data)
                        df.to_excel(excel_path, index=False, engine='openpyxl')
                        logger.info(f"âœ… å·²å°† '{file_name}' çš„ {len(excel_data)} ä¸ªæ–‡æœ¬å—ä¿å­˜è‡³Excelæ–‡ä»¶: {excel_path}")
                    else:
                        logger.warning(f"âš ï¸ '{file_name}' æ²¡æœ‰å¯ä¿å­˜çš„æ–‡æœ¬å—")
                
                except Exception as e:
                    logger.error(f"âŒ ä¿å­˜ '{file_name}' çš„Excelæ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
                
                processed_files += 1
                pbar.update(1)
                pbar.set_postfix_str(f"å·²å¤„ç† {processed_files}/{total_files} ä¸ªæ–‡ä»¶")
        
        logger.info(f"âœ… æ‰€æœ‰æ–‡æ¡£åˆ†å—å·²ä¿å­˜åˆ°Excelæ–‡ä»¶ï¼Œå…±å¤„ç† {processed_files} ä¸ªæ–‡ä»¶")


if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–é…ç½®
        config = Config()
        
        # æ‰“å°å·¥ä½œç›®å½•ä¿¡æ¯
        logger.info(f"å·¥ä½œç›®å½•: {os.getcwd()}")
        logger.info(f"æ•°æ®ç›®å½•: {config.data_dir}")
        logger.info(f"ç¼“å­˜ç›®å½•: {config.cache_dir}")
        logger.info(f"å‘é‡æ•°æ®åº“ç›®å½•: {config.vector_db_path}")
        
        # Excelè¾“å‡ºç›®å½•
        excel_output_dir = r"C:\Users\Administrator\Desktop\chunks"
        logger.info(f"Excelæ–‡ä»¶å°†ä¿å­˜è‡³: {excel_output_dir}")
        
        # æ„å»ºå‘é‡æ•°æ®åº“
        builder = VectorDBBuilder(config)
        
        # å¤„ç†æ–‡æ¡£å¹¶è·å–åˆ†å—
        logger.info("å¼€å§‹å¤„ç†æ–‡æ¡£å¹¶å¯¼å‡ºExcel...")
        chunks = builder.process_files()
        
        # å°†æ–‡æ¡£åˆ†å—ä¿å­˜åˆ°Excelæ–‡ä»¶
        builder.save_chunks_to_excel(chunks, excel_output_dir)
        
        logger.info("æ‰€æœ‰å¤„ç†å®Œæˆ")

    except Exception as e:
        logger.exception("ç¨‹åºè¿è¡Œå‡ºé”™")  # è®°å½•ç¨‹åºå¼‚å¸¸
    finally:
        logger.info("ç¨‹åºè¿è¡Œç»“æŸ")  # ç¨‹åºç»“æŸæ—¥å¿—
